<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Standards [SEEKING FEEDBACK] | rOpenSci Statistical Software Peer Review</title>
  <meta name="description" content="Chapter 5 Standards [SEEKING FEEDBACK] | rOpenSci Statistical Software Peer Review" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Standards [SEEKING FEEDBACK] | rOpenSci Statistical Software Peer Review" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://ropensci-blog-guidance.netlify.com/" />
  
  
  <meta name="github-repo" content="ropenscilabs/statistical-software-review-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Standards [SEEKING FEEDBACK] | rOpenSci Statistical Software Peer Review" />
  
  
  

<meta name="author" content="Mark Padgham and Noam Ross" />


<meta name="date" content="2020-10-13" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="favicon/apple-touch-icon.png" />
  <link rel="shortcut icon" href="favicon/favicon.ico" type="image/x-icon" />
<link rel="prev" href="scope.html"/>
<link rel="next" href="assessment.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/htmlwidgets-1.5.2/htmlwidgets.js"></script>
<link href="libs/vis-4.20.1/vis.css" rel="stylesheet" />
<script src="libs/vis-4.20.1/vis.min.js"></script>
<script src="libs/visNetwork-binding-2.0.9/visNetwork.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://github.com/ropenscilabs/statistical-software-review-book"><i class="fa fa-github"></i> Statistical Software Peer Review</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Welcome</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#contributors"><i class="fa fa-check"></i><b>1.1</b> Contributors</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#content"><i class="fa fa-check"></i><b>1.2</b> Content</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#issue-authors"><i class="fa fa-check"></i><b>1.3</b> Issue Authors</a></li>
</ul></li>
<li class="part"><span><b>I Introduction</b></span></li>
<li class="chapter" data-level="2" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>2</b> Project Overview</a><ul>
<li class="chapter" data-level="2.1" data-path="overview.html"><a href="overview.html#project-aims"><i class="fa fa-check"></i><b>2.1</b> Project Aims</a></li>
<li class="chapter" data-level="2.2" data-path="overview.html"><a href="overview.html#related-projects-and-initiatives"><i class="fa fa-check"></i><b>2.2</b> Related projects and initiatives</a></li>
<li class="chapter" data-level="2.3" data-path="overview.html"><a href="overview.html#outline-of-this-document"><i class="fa fa-check"></i><b>2.3</b> Outline of this document</a><ul>
<li class="chapter" data-level="2.3.1" data-path="overview.html"><a href="overview.html#scope-of-statistical-software-review"><i class="fa fa-check"></i><b>2.3.1</b> Scope of Statistical Software Review</a></li>
<li class="chapter" data-level="2.3.2" data-path="overview.html"><a href="overview.html#standards-for-statistical-software"><i class="fa fa-check"></i><b>2.3.2</b> Standards for Statistical Software</a></li>
<li class="chapter" data-level="2.3.3" data-path="overview.html"><a href="overview.html#software-assessment"><i class="fa fa-check"></i><b>2.3.3</b> Software Assessment</a></li>
<li class="chapter" data-level="2.3.4" data-path="overview.html"><a href="overview.html#statistical-software-peer-review-process"><i class="fa fa-check"></i><b>2.3.4</b> Statistical Software Peer Review Process</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="overview.html"><a href="overview.html#community"><i class="fa fa-check"></i><b>2.4</b> Community</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="reading.html"><a href="reading.html"><i class="fa fa-check"></i><b>3</b> Some Light Reading: An Annotated Bibliography</a><ul>
<li class="chapter" data-level="3.1" data-path="reading.html"><a href="reading.html#books"><i class="fa fa-check"></i><b>3.1</b> Books</a></li>
<li class="chapter" data-level="3.2" data-path="reading.html"><a href="reading.html#journal-articles"><i class="fa fa-check"></i><b>3.2</b> Journal Articles</a></li>
<li class="chapter" data-level="3.3" data-path="reading.html"><a href="reading.html#technical-reports"><i class="fa fa-check"></i><b>3.3</b> Technical Reports</a></li>
<li class="chapter" data-level="3.4" data-path="reading.html"><a href="reading.html#computer-programs"><i class="fa fa-check"></i><b>3.4</b> Computer Programs</a><ul>
<li class="chapter" data-level="3.4.1" data-path="reading.html"><a href="reading.html#computer-programs-testing"><i class="fa fa-check"></i><b>3.4.1</b> Computer Programs (Testing)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="reading.html"><a href="reading.html#web-pages"><i class="fa fa-check"></i><b>3.5</b> Web Pages</a></li>
<li class="chapter" data-level="3.6" data-path="reading.html"><a href="reading.html#contributing-to-the-biblopgraphy"><i class="fa fa-check"></i><b>3.6</b> Contributing to the bibliography</a></li>
</ul></li>
<li class="part"><span><b>II Scope and Standards</b></span></li>
<li class="chapter" data-level="4" data-path="scope.html"><a href="scope.html"><i class="fa fa-check"></i><b>4</b> Scope</a><ul>
<li class="chapter" data-level="4.1" data-path="scope.html"><a href="scope.html#software-types"><i class="fa fa-check"></i><b>4.1</b> Software types</a><ul>
<li class="chapter" data-level="4.1.1" data-path="scope.html"><a href="scope.html#languages"><i class="fa fa-check"></i><b>4.1.1</b> Languages</a></li>
<li class="chapter" data-level="4.1.2" data-path="scope.html"><a href="scope.html#structure"><i class="fa fa-check"></i><b>4.1.2</b> Structure</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="scope.html"><a href="scope.html#scope-categories-bg"><i class="fa fa-check"></i><b>4.2</b> Statistical Categories – Background</a><ul>
<li class="chapter" data-level="4.2.1" data-path="scope.html"><a href="scope.html#empirical-derivation-of-categories"><i class="fa fa-check"></i><b>4.2.1</b> Empirical Derivation of Categories</a></li>
<li class="chapter" data-level="4.2.2" data-path="scope.html"><a href="scope.html#examples-of-statistical-software"><i class="fa fa-check"></i><b>4.2.2</b> Examples of Statistical Software</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="scope.html"><a href="scope.html#scope-categories"><i class="fa fa-check"></i><b>4.3</b> Statistical Categories</a><ul>
<li class="chapter" data-level="4.3.1" data-path="scope.html"><a href="scope.html#scope-category-bayesian"><i class="fa fa-check"></i><b>4.3.1</b> Bayesian and Monte Carlo Routines</a></li>
<li class="chapter" data-level="4.3.2" data-path="scope.html"><a href="scope.html#scope-category-unsupervised"><i class="fa fa-check"></i><b>4.3.2</b> Dimensionality Reduction, Clustering, and Unsupervised Learning</a></li>
<li class="chapter" data-level="4.3.3" data-path="scope.html"><a href="scope.html#scope-category-ML"><i class="fa fa-check"></i><b>4.3.3</b> Machine Learning</a></li>
<li class="chapter" data-level="4.3.4" data-path="scope.html"><a href="scope.html#scope-category-supervised"><i class="fa fa-check"></i><b>4.3.4</b> Regression and Supervised Learning</a></li>
<li class="chapter" data-level="4.3.5" data-path="scope.html"><a href="scope.html#scope-category-distributions"><i class="fa fa-check"></i><b>4.3.5</b> Probability Distributions</a></li>
<li class="chapter" data-level="4.3.6" data-path="scope.html"><a href="scope.html#scope-category-wrapper"><i class="fa fa-check"></i><b>4.3.6</b> Wrapper Packages</a></li>
<li class="chapter" data-level="4.3.7" data-path="scope.html"><a href="scope.html#scope-category-networks"><i class="fa fa-check"></i><b>4.3.7</b> Networks</a></li>
<li class="chapter" data-level="4.3.8" data-path="scope.html"><a href="scope.html#scope-category-EDA"><i class="fa fa-check"></i><b>4.3.8</b> Exploratory Data Analysis (EDA) and Summary Statistics</a></li>
<li class="chapter" data-level="4.3.9" data-path="scope.html"><a href="scope.html#scope-category-workflow"><i class="fa fa-check"></i><b>4.3.9</b> Workflow Support</a></li>
<li class="chapter" data-level="4.3.10" data-path="scope.html"><a href="scope.html#scope-category-spatial"><i class="fa fa-check"></i><b>4.3.10</b> Spatial Analyses</a></li>
<li class="chapter" data-level="4.3.11" data-path="scope.html"><a href="scope.html#scope-category-time"><i class="fa fa-check"></i><b>4.3.11</b> Time Series Analyses</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="scope.html"><a href="scope.html#out-of-scope-categories"><i class="fa fa-check"></i><b>4.4</b> Out Of Scope Categories</a><ul>
<li class="chapter" data-level="4.4.1" data-path="scope.html"><a href="scope.html#visualisation"><i class="fa fa-check"></i><b>4.4.1</b> Visualisation</a></li>
<li class="chapter" data-level="4.4.2" data-path="scope.html"><a href="scope.html#education"><i class="fa fa-check"></i><b>4.4.2</b> Education</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="scope.html"><a href="scope.html#proposals"><i class="fa fa-check"></i><b>4.5</b> Proposals</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="standards.html"><a href="standards.html"><i class="fa fa-check"></i><b>5</b> <span style="color:red;">Standards [SEEKING FEEDBACK]<span></a><ul>
<li class="chapter" data-level="5.1" data-path="standards.html"><a href="standards.html#other-standards"><i class="fa fa-check"></i><b>5.1</b> Other Standards</a></li>
<li class="chapter" data-level="5.2" data-path="standards.html"><a href="standards.html#general-standards-for-statistical-software"><i class="fa fa-check"></i><b>5.2</b> General Standards for Statistical Software</a><ul>
<li class="chapter" data-level="5.2.1" data-path="standards.html"><a href="standards.html#documentation"><i class="fa fa-check"></i><b>5.2.1</b> Documentation</a></li>
<li class="chapter" data-level="5.2.2" data-path="standards.html"><a href="standards.html#input-structures"><i class="fa fa-check"></i><b>5.2.2</b> Input Structures</a><ul>
<li class="chapter" data-level="5.2.2.1" data-path="standards.html"><a href="standards.html#uni-variate-vector-input"><i class="fa fa-check"></i><b>5.2.2.1</b> Uni-variate (Vector) Input</a></li>
<li class="chapter" data-level="5.2.2.2" data-path="standards.html"><a href="standards.html#tabular-input"><i class="fa fa-check"></i><b>5.2.2.2</b> Tabular Input</a></li>
<li class="chapter" data-level="5.2.2.3" data-path="standards.html"><a href="standards.html#missing-or-undefined-values"><i class="fa fa-check"></i><b>5.2.2.3</b> Missing or Undefined Values</a></li>
</ul></li>
<li class="chapter" data-level="5.2.3" data-path="standards.html"><a href="standards.html#output-structures"><i class="fa fa-check"></i><b>5.2.3</b> Output Structures</a></li>
<li class="chapter" data-level="5.2.4" data-path="standards.html"><a href="standards.html#testing"><i class="fa fa-check"></i><b>5.2.4</b> Testing</a><ul>
<li class="chapter" data-level="5.2.4.1" data-path="standards.html"><a href="standards.html#extended-tests"><i class="fa fa-check"></i><b>5.2.4.1</b> Extended tests</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="standards.html"><a href="standards.html#bayesian-and-monte-carlo-software"><i class="fa fa-check"></i><b>5.3</b> Bayesian and Monte Carlo Software</a><ul>
<li class="chapter" data-level="5.3.1" data-path="standards.html"><a href="standards.html#documentation-of-inputs"><i class="fa fa-check"></i><b>5.3.1</b> Documentation of Inputs</a></li>
<li class="chapter" data-level="5.3.2" data-path="standards.html"><a href="standards.html#input-data-structures-and-validation"><i class="fa fa-check"></i><b>5.3.2</b> Input Data Structures and Validation</a><ul>
<li class="chapter" data-level="5.3.2.1" data-path="standards.html"><a href="standards.html#input-data"><i class="fa fa-check"></i><b>5.3.2.1</b> Input Data</a></li>
<li class="chapter" data-level="5.3.2.2" data-path="standards.html"><a href="standards.html#prior-distributions-model-specifications-and-hyperparameters"><i class="fa fa-check"></i><b>5.3.2.2</b> Prior Distributions, Model Specifications, and Hyperparameters</a></li>
<li class="chapter" data-level="5.3.2.3" data-path="standards.html"><a href="standards.html#computational-parameters"><i class="fa fa-check"></i><b>5.3.2.3</b> Computational Parameters</a></li>
<li class="chapter" data-level="5.3.2.4" data-path="standards.html"><a href="standards.html#seed-parameters"><i class="fa fa-check"></i><b>5.3.2.4</b> Seed Parameters</a></li>
<li class="chapter" data-level="5.3.2.5" data-path="standards.html"><a href="standards.html#output-verbosity"><i class="fa fa-check"></i><b>5.3.2.5</b> Output Verbosity</a></li>
</ul></li>
<li class="chapter" data-level="5.3.3" data-path="standards.html"><a href="standards.html#pre-processing-and-data-transformation"><i class="fa fa-check"></i><b>5.3.3</b> Pre-processing and Data Transformation</a><ul>
<li class="chapter" data-level="5.3.3.1" data-path="standards.html"><a href="standards.html#missing-values"><i class="fa fa-check"></i><b>5.3.3.1</b> Missing Values</a></li>
<li class="chapter" data-level="5.3.3.2" data-path="standards.html"><a href="standards.html#perfect-collinearity"><i class="fa fa-check"></i><b>5.3.3.2</b> Perfect Collinearity</a></li>
</ul></li>
<li class="chapter" data-level="5.3.4" data-path="standards.html"><a href="standards.html#analytic-algorithms"><i class="fa fa-check"></i><b>5.3.4</b> Analytic Algorithms</a></li>
<li class="chapter" data-level="5.3.5" data-path="standards.html"><a href="standards.html#return-values"><i class="fa fa-check"></i><b>5.3.5</b> Return Values</a></li>
<li class="chapter" data-level="5.3.6" data-path="standards.html"><a href="standards.html#additional-functionality"><i class="fa fa-check"></i><b>5.3.6</b> Additional Functionality</a></li>
<li class="chapter" data-level="5.3.7" data-path="standards.html"><a href="standards.html#tests"><i class="fa fa-check"></i><b>5.3.7</b> Tests</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="standards.html"><a href="standards.html#regression-and-supervised-learning"><i class="fa fa-check"></i><b>5.4</b> Regression and Supervised Learning</a><ul>
<li class="chapter" data-level="5.4.1" data-path="standards.html"><a href="standards.html#input-data-structures-and-validation-1"><i class="fa fa-check"></i><b>5.4.1</b> Input data structures and validation</a></li>
<li class="chapter" data-level="5.4.2" data-path="standards.html"><a href="standards.html#pre-processing-and-variable-transformation"><i class="fa fa-check"></i><b>5.4.2</b> Pre-processing and Variable Transformation</a></li>
<li class="chapter" data-level="5.4.3" data-path="standards.html"><a href="standards.html#algorithms"><i class="fa fa-check"></i><b>5.4.3</b> Algorithms</a></li>
<li class="chapter" data-level="5.4.4" data-path="standards.html"><a href="standards.html#return-results"><i class="fa fa-check"></i><b>5.4.4</b> Return Results</a><ul>
<li class="chapter" data-level="5.4.4.1" data-path="standards.html"><a href="standards.html#accessor-methods"><i class="fa fa-check"></i><b>5.4.4.1</b> Accessor Methods</a></li>
<li class="chapter" data-level="5.4.4.2" data-path="standards.html"><a href="standards.html#prediction-extrapolation-and-forecasting"><i class="fa fa-check"></i><b>5.4.4.2</b> Prediction, Extrapolation, and Forecasting</a></li>
<li class="chapter" data-level="5.4.4.3" data-path="standards.html"><a href="standards.html#reporting-return-results"><i class="fa fa-check"></i><b>5.4.4.3</b> Reporting Return Results</a></li>
</ul></li>
<li class="chapter" data-level="5.4.5" data-path="standards.html"><a href="standards.html#documentation-1"><i class="fa fa-check"></i><b>5.4.5</b> Documentation</a></li>
<li class="chapter" data-level="5.4.6" data-path="standards.html"><a href="standards.html#visualization"><i class="fa fa-check"></i><b>5.4.6</b> Visualization</a></li>
<li class="chapter" data-level="5.4.7" data-path="standards.html"><a href="standards.html#testing-1"><i class="fa fa-check"></i><b>5.4.7</b> Testing</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="standards.html"><a href="standards.html#dimensionality-reduction-clustering-and-unsupervised-learning"><i class="fa fa-check"></i><b>5.5</b> Dimensionality Reduction, Clustering, and Unsupervised Learning</a><ul>
<li class="chapter" data-level="5.5.1" data-path="standards.html"><a href="standards.html#input-data-structures-and-validation-2"><i class="fa fa-check"></i><b>5.5.1</b> Input Data Structures and Validation</a></li>
<li class="chapter" data-level="5.5.2" data-path="standards.html"><a href="standards.html#pre-processing-and-variable-transformation-1"><i class="fa fa-check"></i><b>5.5.2</b> Pre-processing and Variable Transformation</a></li>
<li class="chapter" data-level="5.5.3" data-path="standards.html"><a href="standards.html#algorithms-1"><i class="fa fa-check"></i><b>5.5.3</b> Algorithms</a><ul>
<li class="chapter" data-level="5.5.3.1" data-path="standards.html"><a href="standards.html#labelling"><i class="fa fa-check"></i><b>5.5.3.1</b> Labelling</a></li>
<li class="chapter" data-level="5.5.3.2" data-path="standards.html"><a href="standards.html#prediction"><i class="fa fa-check"></i><b>5.5.3.2</b> Prediction</a></li>
<li class="chapter" data-level="5.5.3.3" data-path="standards.html"><a href="standards.html#group-distributions-and-associated-statistics"><i class="fa fa-check"></i><b>5.5.3.3</b> Group Distributions and Associated Statistics</a></li>
</ul></li>
<li class="chapter" data-level="5.5.4" data-path="standards.html"><a href="standards.html#return-results-1"><i class="fa fa-check"></i><b>5.5.4</b> Return Results</a><ul>
<li class="chapter" data-level="5.5.4.1" data-path="standards.html"><a href="standards.html#reporting-return-results-1"><i class="fa fa-check"></i><b>5.5.4.1</b> Reporting Return Results</a></li>
</ul></li>
<li class="chapter" data-level="5.5.5" data-path="standards.html"><a href="standards.html#documentation-2"><i class="fa fa-check"></i><b>5.5.5</b> Documentation</a></li>
<li class="chapter" data-level="5.5.6" data-path="standards.html"><a href="standards.html#visualization-1"><i class="fa fa-check"></i><b>5.5.6</b> Visualization</a></li>
<li class="chapter" data-level="5.5.7" data-path="standards.html"><a href="standards.html#testing-2"><i class="fa fa-check"></i><b>5.5.7</b> Testing</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="standards.html"><a href="standards.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>5.6</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="5.6.1" data-path="standards.html"><a href="standards.html#documentation-standards"><i class="fa fa-check"></i><b>5.6.1</b> Documentation Standards</a></li>
<li class="chapter" data-level="5.6.2" data-path="standards.html"><a href="standards.html#input-data-1"><i class="fa fa-check"></i><b>5.6.2</b> Input Data</a><ul>
<li class="chapter" data-level="5.6.2.1" data-path="standards.html"><a href="standards.html#index-columns"><i class="fa fa-check"></i><b>5.6.2.1</b> Index Columns</a></li>
<li class="chapter" data-level="5.6.2.2" data-path="standards.html"><a href="standards.html#multi-tabular-input"><i class="fa fa-check"></i><b>5.6.2.2</b> Multi-tabular input</a></li>
<li class="chapter" data-level="5.6.2.3" data-path="standards.html"><a href="standards.html#classes-and-sub-classes"><i class="fa fa-check"></i><b>5.6.2.3</b> Classes and Sub-Classes</a></li>
</ul></li>
<li class="chapter" data-level="5.6.3" data-path="standards.html"><a href="standards.html#analytic-algorithms-1"><i class="fa fa-check"></i><b>5.6.3</b> Analytic Algorithms</a></li>
<li class="chapter" data-level="5.6.4" data-path="standards.html"><a href="standards.html#return-results-output-data"><i class="fa fa-check"></i><b>5.6.4</b> Return Results / Output Data</a></li>
<li class="chapter" data-level="5.6.5" data-path="standards.html"><a href="standards.html#visualization-and-summary-output"><i class="fa fa-check"></i><b>5.6.5</b> Visualization and Summary Output</a><ul>
<li class="chapter" data-level="5.6.5.1" data-path="standards.html"><a href="standards.html#summary-and-screen-based-output"><i class="fa fa-check"></i><b>5.6.5.1</b> Summary and Screen-based Output</a></li>
<li class="chapter" data-level="5.6.5.2" data-path="standards.html"><a href="standards.html#general-standards-for-visualization-static-and-dynamic"><i class="fa fa-check"></i><b>5.6.5.2</b> General Standards for Visualization (Static and Dynamic)</a></li>
<li class="chapter" data-level="5.6.5.3" data-path="standards.html"><a href="standards.html#dynamic-visualization"><i class="fa fa-check"></i><b>5.6.5.3</b> Dynamic Visualization</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="standards.html"><a href="standards.html#time-series-software"><i class="fa fa-check"></i><b>5.7</b> Time Series Software</a><ul>
<li class="chapter" data-level="5.7.1" data-path="standards.html"><a href="standards.html#input-data-structures-and-validation-3"><i class="fa fa-check"></i><b>5.7.1</b> Input data structures and validation</a><ul>
<li class="chapter" data-level="5.7.1.1" data-path="standards.html"><a href="standards.html#time-intervals-and-relative-time"><i class="fa fa-check"></i><b>5.7.1.1</b> Time Intervals and Relative Time</a></li>
</ul></li>
<li class="chapter" data-level="5.7.2" data-path="standards.html"><a href="standards.html#pre-processing-and-variable-transformation-2"><i class="fa fa-check"></i><b>5.7.2</b> Pre-processing and Variable Transformation</a><ul>
<li class="chapter" data-level="5.7.2.1" data-path="standards.html"><a href="standards.html#missing-data"><i class="fa fa-check"></i><b>5.7.2.1</b> Missing Data</a></li>
<li class="chapter" data-level="5.7.2.2" data-path="standards.html"><a href="standards.html#stationarity"><i class="fa fa-check"></i><b>5.7.2.2</b> Stationarity</a></li>
<li class="chapter" data-level="5.7.2.3" data-path="standards.html"><a href="standards.html#covariance-matrices"><i class="fa fa-check"></i><b>5.7.2.3</b> Covariance Matrices</a></li>
</ul></li>
<li class="chapter" data-level="5.7.3" data-path="standards.html"><a href="standards.html#analytic-algorithms-2"><i class="fa fa-check"></i><b>5.7.3</b> Analytic Algorithms</a><ul>
<li class="chapter" data-level="5.7.3.1" data-path="standards.html"><a href="standards.html#forecasting"><i class="fa fa-check"></i><b>5.7.3.1</b> Forecasting</a></li>
</ul></li>
<li class="chapter" data-level="5.7.4" data-path="standards.html"><a href="standards.html#return-results-2"><i class="fa fa-check"></i><b>5.7.4</b> Return Results</a><ul>
<li class="chapter" data-level="5.7.4.1" data-path="standards.html"><a href="standards.html#data-transformation"><i class="fa fa-check"></i><b>5.7.4.1</b> Data Transformation</a></li>
<li class="chapter" data-level="5.7.4.2" data-path="standards.html"><a href="standards.html#forecasting-1"><i class="fa fa-check"></i><b>5.7.4.2</b> Forecasting</a></li>
</ul></li>
<li class="chapter" data-level="5.7.5" data-path="standards.html"><a href="standards.html#visualization-2"><i class="fa fa-check"></i><b>5.7.5</b> Visualization</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="standards.html"><a href="standards.html#machine-learning-software"><i class="fa fa-check"></i><b>5.8</b> Machine Learning Software</a><ul>
<li class="chapter" data-level="5.8.1" data-path="standards.html"><a href="standards.html#input-data-specification"><i class="fa fa-check"></i><b>5.8.1</b> Input Data Specification</a><ul>
<li class="chapter" data-level="5.8.1.1" data-path="standards.html"><a href="standards.html#missing-values-1"><i class="fa fa-check"></i><b>5.8.1.1</b> Missing Values</a></li>
</ul></li>
<li class="chapter" data-level="5.8.2" data-path="standards.html"><a href="standards.html#pre-processing"><i class="fa fa-check"></i><b>5.8.2</b> Pre-processing</a></li>
<li class="chapter" data-level="5.8.3" data-path="standards.html"><a href="standards.html#model-and-algorithm-specification"><i class="fa fa-check"></i><b>5.8.3</b> Model and Algorithm Specification</a><ul>
<li class="chapter" data-level="5.8.3.1" data-path="standards.html"><a href="standards.html#control-parameters"><i class="fa fa-check"></i><b>5.8.3.1</b> Control Parameters</a></li>
<li class="chapter" data-level="5.8.3.2" data-path="standards.html"><a href="standards.html#cpu-and-gpu-processing"><i class="fa fa-check"></i><b>5.8.3.2</b> CPU and GPU processing</a></li>
</ul></li>
<li class="chapter" data-level="5.8.4" data-path="standards.html"><a href="standards.html#model-training"><i class="fa fa-check"></i><b>5.8.4</b> Model Training</a><ul>
<li class="chapter" data-level="5.8.4.1" data-path="standards.html"><a href="standards.html#batch-processing"><i class="fa fa-check"></i><b>5.8.4.1</b> Batch Processing</a></li>
<li class="chapter" data-level="5.8.4.2" data-path="standards.html"><a href="standards.html#re-sampling"><i class="fa fa-check"></i><b>5.8.4.2</b> Re-sampling</a></li>
</ul></li>
<li class="chapter" data-level="5.8.5" data-path="standards.html"><a href="standards.html#model-output-and-performance"><i class="fa fa-check"></i><b>5.8.5</b> Model Output and Performance</a><ul>
<li class="chapter" data-level="5.8.5.1" data-path="standards.html"><a href="standards.html#model-output"><i class="fa fa-check"></i><b>5.8.5.1</b> Model Output</a></li>
<li class="chapter" data-level="5.8.5.2" data-path="standards.html"><a href="standards.html#model-performance"><i class="fa fa-check"></i><b>5.8.5.2</b> Model Performance</a></li>
</ul></li>
<li class="chapter" data-level="5.8.6" data-path="standards.html"><a href="standards.html#documentation-3"><i class="fa fa-check"></i><b>5.8.6</b> Documentation</a></li>
<li class="chapter" data-level="5.8.7" data-path="standards.html"><a href="standards.html#testing-3"><i class="fa fa-check"></i><b>5.8.7</b> Testing</a><ul>
<li class="chapter" data-level="5.8.7.1" data-path="standards.html"><a href="standards.html#input-data-2"><i class="fa fa-check"></i><b>5.8.7.1</b> Input Data</a></li>
<li class="chapter" data-level="5.8.7.2" data-path="standards.html"><a href="standards.html#model-classes"><i class="fa fa-check"></i><b>5.8.7.2</b> Model Classes</a></li>
<li class="chapter" data-level="5.8.7.3" data-path="standards.html"><a href="standards.html#model-training-1"><i class="fa fa-check"></i><b>5.8.7.3</b> Model Training</a></li>
<li class="chapter" data-level="5.8.7.4" data-path="standards.html"><a href="standards.html#model-performance-1"><i class="fa fa-check"></i><b>5.8.7.4</b> Model Performance</a></li>
</ul></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="assessment.html"><a href="assessment.html"><i class="fa fa-check"></i><b>6</b> Assessment</a><ul>
<li class="chapter" data-level="6.1" data-path="assessment.html"><a href="assessment.html#general-software-metrics"><i class="fa fa-check"></i><b>6.1</b> General Software Metrics</a></li>
<li class="chapter" data-level="6.2" data-path="assessment.html"><a href="assessment.html#metrics-specific-to-statistical-software"><i class="fa fa-check"></i><b>6.2</b> Metrics specific to statistical software</a></li>
<li class="chapter" data-level="6.3" data-path="assessment.html"><a href="assessment.html#diagnostics-and-reporting"><i class="fa fa-check"></i><b>6.3</b> Diagnostics and Reporting</a></li>
<li class="chapter" data-level="6.4" data-path="assessment.html"><a href="assessment.html#proposals-and-aims"><i class="fa fa-check"></i><b>6.4</b> Proposals and Aims</a></li>
</ul></li>
<li class="part"><span><b>III Software Review Process and Software Assessment</b></span></li>
<li class="chapter" data-level="7" data-path="lifeycle.html"><a href="lifeycle.html"><i class="fa fa-check"></i><b>7</b> <span style="color:red;">Software Review and Life Cycle Models [Seeking Feedback]</span></a><ul>
<li class="chapter" data-level="7.1" data-path="lifeycle.html"><a href="lifeycle.html#other-systems-for-software-and-peer-review"><i class="fa fa-check"></i><b>7.1</b> Other systems for software and peer review</a><ul>
<li class="chapter" data-level="7.1.1" data-path="lifeycle.html"><a href="lifeycle.html#ropensci"><i class="fa fa-check"></i><b>7.1.1</b> rOpenSci</a></li>
<li class="chapter" data-level="7.1.2" data-path="lifeycle.html"><a href="lifeycle.html#the-journal-of-open-source-software"><i class="fa fa-check"></i><b>7.1.2</b> The Journal of Open Source Software</a></li>
<li class="chapter" data-level="7.1.3" data-path="lifeycle.html"><a href="lifeycle.html#academic-journal-reviews"><i class="fa fa-check"></i><b>7.1.3</b> Academic Journal Reviews</a><ul>
<li class="chapter" data-level="7.1.3.1" data-path="lifeycle.html"><a href="lifeycle.html#primary-and-secondary-editors"><i class="fa fa-check"></i><b>7.1.3.1</b> Primary and Secondary Editors</a></li>
<li class="chapter" data-level="7.1.3.2" data-path="lifeycle.html"><a href="lifeycle.html#invited-and-mentored-submissions"><i class="fa fa-check"></i><b>7.1.3.2</b> Invited and Mentored Submissions</a></li>
</ul></li>
<li class="chapter" data-level="7.1.4" data-path="lifeycle.html"><a href="lifeycle.html#the-debian-system"><i class="fa fa-check"></i><b>7.1.4</b> The Debian System</a></li>
<li class="chapter" data-level="7.1.5" data-path="lifeycle.html"><a href="lifeycle.html#other-potential-models"><i class="fa fa-check"></i><b>7.1.5</b> Other Potential Models</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="lifeycle.html"><a href="lifeycle.html#software-life-cycle-considerations"><i class="fa fa-check"></i><b>7.2</b> Software Life Cycle Considerations</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="process.html"><a href="process.html"><i class="fa fa-check"></i><b>8</b> <span style="color:red;">The Review Process [SEEKING FEEDBACK]</span></a><ul>
<li class="chapter" data-level="8.1" data-path="process.html"><a href="process.html#self-eval"><i class="fa fa-check"></i><b>8.1</b> Self-Evaluation of Software Prior to Submission</a></li>
<li class="chapter" data-level="8.2" data-path="process.html"><a href="process.html#presub-comm"><i class="fa fa-check"></i><b>8.2</b> Pre-Submission Communication</a></li>
<li class="chapter" data-level="8.3" data-path="process.html"><a href="process.html#reviewers-selection"><i class="fa fa-check"></i><b>8.3</b> Reviewers / Selection</a><ul>
<li class="chapter" data-level="8.3.1" data-path="process.html"><a href="process.html#database-of-potential-reviewers"><i class="fa fa-check"></i><b>8.3.1</b> Database of Potential Reviewers</a></li>
<li class="chapter" data-level="8.3.2" data-path="process.html"><a href="process.html#automating-the-identification-of-potential-reviewers"><i class="fa fa-check"></i><b>8.3.2</b> Automating the Identification of Potential Reviewers</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="process.html"><a href="process.html#submission-phase"><i class="fa fa-check"></i><b>8.4</b> Submission</a></li>
<li class="chapter" data-level="8.5" data-path="process.html"><a href="process.html#initial-screening"><i class="fa fa-check"></i><b>8.5</b> Initial Screening</a></li>
<li class="chapter" data-level="8.6" data-path="process.html"><a href="process.html#review-process"><i class="fa fa-check"></i><b>8.6</b> Review Process</a><ul>
<li class="chapter" data-level="8.6.1" data-path="process.html"><a href="process.html#review-templates"><i class="fa fa-check"></i><b>8.6.1</b> Review Templates</a></li>
<li class="chapter" data-level="8.6.2" data-path="process.html"><a href="process.html#review-comments"><i class="fa fa-check"></i><b>8.6.2</b> Review Comments</a></li>
<li class="chapter" data-level="8.6.3" data-path="process.html"><a href="process.html#category-specific-aspects-of-reviews"><i class="fa fa-check"></i><b>8.6.3</b> Category-Specific Aspects of Reviews</a></li>
<li class="chapter" data-level="8.6.4" data-path="process.html"><a href="process.html#reviewer-recommendations"><i class="fa fa-check"></i><b>8.6.4</b> Reviewer Recommendations</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="process.html"><a href="process.html#review-acceptance"><i class="fa fa-check"></i><b>8.7</b> Acceptance / Scoring / Badging</a></li>
<li class="chapter" data-level="8.8" data-path="process.html"><a href="process.html#post-acceptance-dissemination-publication-etc."><i class="fa fa-check"></i><b>8.8</b> Post-acceptance Dissemination, Publication, etc.</a></li>
<li class="chapter" data-level="8.9" data-path="process.html"><a href="process.html#ongoing-maintenance"><i class="fa fa-check"></i><b>8.9</b> Ongoing Maintenance</a></li>
<li class="chapter" data-level="8.10" data-path="process.html"><a href="process.html#structured-review-beyond-acceptance"><i class="fa fa-check"></i><b>8.10</b> Structured Review beyond Acceptance</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="appendices.html"><a href="appendices.html"><i class="fa fa-check"></i><b>A</b> Appendices</a><ul>
<li class="chapter" data-level="A.1" data-path="appendices.html"><a href="appendices.html#python"><i class="fa fa-check"></i><b>A.1</b> Notes on Scope and the Python Statistical Ecosystem</a></li>
<li class="chapter" data-level="A.2" data-path="appendices.html"><a href="appendices.html#appendix-keywords"><i class="fa fa-check"></i><b>A.2</b> Analysis of statistical software keywords</a></li>
<li class="chapter" data-level="A.3" data-path="appendices.html"><a href="appendices.html#other-software-standards"><i class="fa fa-check"></i><b>A.3</b> Other Software Standards</a></li>
<li class="chapter" data-level="A.4" data-path="appendices.html"><a href="appendices.html#bibliography"><i class="fa fa-check"></i><b>A.4</b> Bibliography</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Made with bookdown</a></li>
#download: [pdf]

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">rOpenSci Statistical Software Peer Review</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="standards" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> <span style="color:red;">Standards [SEEKING FEEDBACK]<span></h1>
<p>This Chapter is divided between:</p>
<ul>
<li><p>“<em>General Standards</em>” which may be applied to all software considered within
this project, irrespective of how it may be categorized under the times of
categories of statistical software listed above; and</p></li>
<li><p>“<em>Specific Standards</em>” which apply to different degrees to statistical
software depending on the software category.</p></li>
</ul>
<p>It is likely that standards developed under the first category may subsequently
be deemed to be genuinely <em>Statistical Standards</em> yet which are applicable
across all categories, and it may also be likely that the development of
category-specific standards reveals aspects which are common across all
categories, and which may subsequently be deemed general standards. We
accordingly anticipate a degree of fluidity between these two broad categories.</p>
<p>There is also a necessary relationship between the Standards described here,
and processes of Assessment described below in <a href="assessment.html#assessment">Chapter 8</a>. We
consider the latter to describe concrete <em>and generally quantitative</em> aspects
of <em>post hoc</em> software assessment, while the present Standards provides guides
and benchmarks against which to <em>prospectively</em> compare software during
development. As this entire document is intended to serve as the defining
reference for our Standards, that term may in turn be interpreted to reflect
this entire document, with the current section explicitly describing aspects of
Standards not covered elsewhere.</p>
<p>As described above, we anticipate the ongoing development of this current
document to employ a versioning system, with software reviewed and hosted under
the system mandated to flag the latest version of these standards to which it
complies.</p>
<div id="other-standards" class="section level2">
<h2><span class="header-section-number">5.1</span> Other Standards</h2>
<p>Among the noteworthy instances of software standards which might be adapted for
our purposes, and in addition to entries in our <a href="reading.html#reading"><em>Annotated
Bibliography</em></a>, the following are particularly relevant:</p>
<ol style="list-style-type: decimal">
<li>The <a href="https://bestpractices.coreinfrastructure.org/en">Core Infrastructure Initiative’s Best Practices
Badge</a>, which is granted to
software meeting an extensive list of
<a href="https://github.com/coreinfrastructure/best-practices-badge/blob/master/doc/criteria.md">criteria</a>.
This list of criteria provides a singularly useful reference for software
standards.</li>
<li>The <a href="https://www.software.ac.uk/">Software Sustainability Institute</a>’s
<a href="https://www.software.ac.uk/resources/guides-everything/software-evaluation-guide"><em>Software Evaluation
Guide</em></a>,
in particular their guide to <a href="http://software.ac.uk/sites/default/files/SSI-SoftwareEvaluationCriteria.pdf"><em>Criteria-based software
evaluation</em></a>,
which considers two primary categories of <em>Usability</em> and <em>Sustainability
and Maintainability</em>, each of which is divided into numerous sub-categories.
The guide identifies numerous concrete criteria for each sub-category,
explicitly detailed below in order to provide an example of the kind of
standards that might be adapted and developed for application to the present
project.</li>
<li>The <a href="https://transparentstats.github.io/guidelines/"><em>Transparent Statistics
Guidelines</em></a>, by the “HCI
(Human Computer Interaction) Working Group”. While currently only in its
beginning phases, that document aims to provide concrete guidance on
“transparent statistical communication.” If its development continues, it is
likely to provide useful guidelines on best practices for how statistical
software produces and reports results.</li>
<li>The more technical considerations of the <a href="https://www.omg.org/index.htm">Object Management
Group</a>’s <a href="https://www.omg.org/spec/ASCMM/"><em>Automated Source Code CISQ
Maintainability Measure</em></a> (where CISQ
refers to the <a href="https://www.it-cisq.org/"><em>Consortium for IT Software
Quality</em></a>). This guide describes a number of
measures which can be automatically extracted and used to quantify the
maintainability of source code. None of these measures are not already
considered in one or both of the preceding two documents, but the
identification of measures particularly amenable to automated assessment
provides a particularly useful reference.</li>
</ol>
<p>There is also rOpenSci’s guide on <a href="https://devguide.ropensci.org/">package development, maintenance, and peer
review</a>, which provides standards of this type
for R packages, primarily within its first chapter. Another notable example is
the <a href="https://principles.tidyverse.org/">tidyverse design guide</a>, and the
section on <a href="https://tidymodels.github.io/model-implementation-principles/">Conventions for R Modeling
Packages</a> which
provides guidance for model-fitting APIs.</p>
<p>Specific standards for neural network algorithms have also been developed as
part of a <a href="http://www.inmodelia.com/gsoc2019.html">google 2019 Summer Of Code
project</a>, resulting in a dedicated
R package, <a href="https://akshajverma.com/NNbenchmarkWeb/index.html"><code>NNbenchmark</code></a>,
and accompanying results—their so-called
<a href="https://akshajverma.com/NNbenchmarkWeb/notebooks.html">“notebooks”</a>—of
applying their benchmarks to a suite of neural network packages.</p>
<!-- Edit the .Rmd not the .md file -->
</div>
<div id="general-standards-for-statistical-software" class="section level2">
<h2><span class="header-section-number">5.2</span> General Standards for Statistical Software</h2>
<p>These standards refer to <strong>Data Types</strong> as the fundamental types defined by the
R language itself between the following:</p>
<ul>
<li>Continuous (numeric)</li>
<li>Integer</li>
<li>String / character</li>
<li>Date/Time</li>
<li>Factor</li>
<li>Ordered Factor</li>
</ul>
<p>The standards also refer to <strong>tabular data</strong>, intended to connote any
rectangular data form, including but not limited to <code>matrix</code>, two-dimensional
<code>array</code>, <code>data.frame</code>, and any extensions thereof such as <code>tibble</code>.</p>
<div id="documentation" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Documentation</h3>
<p>Standards will include requirements for form and completeness of documentation.
As with interface, several sources already provide starting points for
reasonable documentation. Some documentation requirements will be specific to
the statistical context. For instance, it is likely we will have requirements
for referencing appropriate literature or references for theoretical support of
implementations. Another area of importance is correctness and clarity of
definitions of statistical quantities produced by the software, e.g., the
definition of null hypotheses or confidence intervals. Data included in
software – that used in examples or tests – will also have documentation
requirements. It is worth noting that the
<a href="https://roxygen2.r-lib.org/"><code>roxygen</code></a> system for documenting R packages is
readily extensible, as exemplified through the <a href="https://github.com/mikldk/roxytest"><code>roxytest</code>
package</a> for specifying tests <em>in-line</em>.</p>
<p>The following standards describe several forms of what might be considered
“Supplementary Material”. While there are many places within an R package where
such material may be included, common locations include vignettes, or in
additional directories (such as <code>data-raw</code>) listed in <code>.Rbuildignore</code> to
prevent inclusion within installed packages.</p>
<p>Where software supports a publication, all claims made in the publication with
regard to software performance (for example, claims of algorithmic scaling or
efficiency; or claims of accuracy), the following standard applies:</p>
<ul>
<li><strong>G1.0</strong> <em>Software should include all code necessary to reproduce results which
form the basis of performance claims made in associated publications.</em></li>
</ul>
<p>Where claims regarding aspects of software performance are made with respect to
other extant R packages, the following standard applies:</p>
<ul>
<li><strong>G1.1</strong> <em>Software should include code necessary to compare performance claims
with alternative implementations in other R packages.</em></li>
</ul>
</div>
<div id="input-structures" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Input Structures</h3>
<p>This section considers general standards for <em>Input Structures</em>. These
standards may often effectively be addressed through implementing class
structures, although this is not a general requirement. Developers are
nevertheless encouraged to examine the guide to <a href="https://vctrs.r-lib.org/articles/s3-vector.html#casting-and-coercion">S3
vectors</a>
in the <a href="https://vctrs.r-lib.org"><code>vctrs</code> package</a> as an example of the kind of
assurances and validation checks that are possible with regard to input data.
Systems like those demonstrated in that vignette provide a very effective way
to ensure that software remains robust to diverse and unexpected classes and
types of input data.</p>
<div id="uni-variate-vector-input" class="section level4">
<h4><span class="header-section-number">5.2.2.1</span> Uni-variate (Vector) Input</h4>
<p>It is important to note for univariate data that single values in R are vectors
with a length of one, and that <code>1</code> is of exactly the same <em>data type</em> as <code>1:n</code>.
Given this, inputs expected to be univariate should:</p>
<ul>
<li><strong>G2.0</strong> <em>Provide explicit secondary documentation of any expectations on lengths
of inputs (generally implying identifying whether an input is expected to be
single- or multi-valued)</em></li>
<li><strong>G2.1</strong> <em>Provide explicit secondary documentation of expectations on data types
of all vector inputs (see the above list).</em></li>
<li><strong>G2.2</strong> <em>Appropriately prohibit or restrict submission of multivariate input to
parameters expected to be univariate.</em></li>
<li><strong>G2.3</strong> <em>For univariate character input:</em>
<ul>
<li><strong>G2.3a</strong> <em>Use <code>match.arg()</code> or equivalent where applicable to only permit expected values.</em></li>
<li><strong>G2.3b</strong> <em>Either: use <code>tolower()</code> or equivalent to ensure input of character parameters is not case dependent; or explicitly document that parameters are strictly case-sensitive.</em></li>
</ul></li>
<li><strong>G2.4</strong> <em>Provide appropriate mechanisms to convert between different data types, potentially including:</em>
<ul>
<li><strong>G2.4a</strong> <em>explicit conversion to <code>integer</code> via <code>as.integer()</code></em></li>
<li><strong>G2.4b</strong> <em>explicit conversion to continuous via <code>as.numeric()</code></em></li>
<li><strong>G2.4c</strong> <em>explicit conversion to character via <code>as.character()</code> (and not <code>paste</code> or <code>paste0</code>)</em></li>
<li><strong>G2.4d</strong> <em>explicit conversion to factor via <code>as.factor()</code></em></li>
<li><strong>G2.4e</strong> <em>explicit conversion from factor via <code>as...()</code> functions</em></li>
</ul></li>
<li><strong>G2.5</strong> <em>Where inputs are expected to be of <code>factor</code> type, secondary
documentation should explicitly state whether these should be <code>ordered</code> or
not, and those inputs should provide appropriate error or other routines to
ensure inputs follow these expectations.</em></li>
</ul>
</div>
<div id="tabular-input" class="section level4">
<h4><span class="header-section-number">5.2.2.2</span> Tabular Input</h4>
<p>This sub-section concerns input in “tabular data” forms, implying the two
primary distinctions within R itself between <code>array</code> or <code>matrix</code>
representations, and <code>data.frame</code> and associated representations. Among
important differences between these two forms are that <code>array</code>/<code>matrix</code> classes
are restricted to storing data of a single uniform type (for example, all
<code>integer</code> or all <code>character</code> values), whereas <code>data.frame</code> as associated
representations store each column as a list item, allowing different columns to
hold values of different types. Further noting that
a <code>matrix</code> may, <a href="https://developer.r-project.org/Blog/public/2019/11/09/when-you-think-class.-think-again/index.html">as of R version
4.0</a>,
be considered as a strictly two-dimensional array, tabular inputs for the
purposes of these standards are considered to imply data represented in one or
more of the following forms:</p>
<p>Given this, tabular inputs may be in one or or more of the following forms:</p>
<ul>
<li><code>matrix</code> form when referring to specifically two-dimensional data of one
uniform type</li>
<li><code>array</code> form as a more general expression, or when referring to data that are
not necessarily or strictly two-dimensional</li>
<li><code>data.frame</code></li>
<li>Extensions such as
<ul>
<li><a href="https://tibble.tidyverse.org"><code>tibble</code></a></li>
<li><a href="https://rdatatable.gitlab.io/data.table"><code>data.table</code></a></li>
<li>domain-specific classes such as
<a href="https://tsibble.tidyverts.org"><code>tsibble</code></a> for time series, or
<a href="https://r-spatial.github.io/sf/"><code>sf</code></a> for spatial data.</li>
</ul></li>
</ul>
<p>The term “<code>data.frame</code> and associated forms” is assumed to refer to data
represented in either the <code>base::data.frame</code> format, and/or any of the classes
listed in the final of the above points.</p>
<p>General Standards applicable to software which is intended to accept any one or
more of these tabular inputs are then that:</p>
<ul>
<li><strong>G2.6</strong> <em>Software should accept as input as many of the above standard tabular
forms as possible, including extension to domain-specific forms.</em></li>
<li><strong>G2.7</strong> <em>Software should provide appropriate conversion routines as part of initial
pre-processing to ensure that all other sub-functions of a package receive
inputs of a single defined class or type.</em></li>
<li><strong>G2.8</strong> <em>Software should issue diagnostic messages for type conversion in which
information is lost (such as conversion of variables from factor to
character; standardisation of variable names; or removal of meta-data such as
those associated with <a href="https://r-spatial.github.io/sf/"><code>sf</code>-format</a> data) or
added (such as insertion of variable or column names where none were
provided).</em></li>
</ul>
<p>The next standard concerns the following inconsistencies between three common
tabular classes in regard the column extraction operator, <code>[</code>.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="standards.html#cb7-1"></a><span class="kw">class</span> (x) <span class="co"># x is any kind of `data.frame` object</span></span>
<span id="cb7-2"><a href="standards.html#cb7-2"></a><span class="co">#&gt; [1] &quot;data.frame&quot;</span></span>
<span id="cb7-3"><a href="standards.html#cb7-3"></a><span class="kw">class</span> (x [, <span class="dv">1</span>])</span>
<span id="cb7-4"><a href="standards.html#cb7-4"></a><span class="co">#&gt; [1] &quot;integer&quot;</span></span>
<span id="cb7-5"><a href="standards.html#cb7-5"></a><span class="kw">class</span> (x [, <span class="dv">1</span>, <span class="dt">drop =</span> <span class="ot">TRUE</span>]) <span class="co"># default</span></span>
<span id="cb7-6"><a href="standards.html#cb7-6"></a><span class="co">#&gt; [1] &quot;integer&quot;</span></span>
<span id="cb7-7"><a href="standards.html#cb7-7"></a><span class="kw">class</span> (x [, <span class="dv">1</span>, <span class="dt">drop =</span> <span class="ot">FALSE</span>])</span>
<span id="cb7-8"><a href="standards.html#cb7-8"></a><span class="co">#&gt; [1] &quot;data.frame&quot;</span></span>
<span id="cb7-9"><a href="standards.html#cb7-9"></a></span>
<span id="cb7-10"><a href="standards.html#cb7-10"></a>x &lt;-<span class="st"> </span>tibble<span class="op">::</span><span class="kw">tibble</span> (x)</span>
<span id="cb7-11"><a href="standards.html#cb7-11"></a><span class="kw">class</span> (x [, <span class="dv">1</span>])</span>
<span id="cb7-12"><a href="standards.html#cb7-12"></a><span class="co">#&gt; [1] &quot;tbl_df&quot;     &quot;tbl&quot;        &quot;data.frame&quot;</span></span>
<span id="cb7-13"><a href="standards.html#cb7-13"></a><span class="kw">class</span> (x [, <span class="dv">1</span>, <span class="dt">drop =</span> <span class="ot">TRUE</span>])</span>
<span id="cb7-14"><a href="standards.html#cb7-14"></a><span class="co">#&gt; [1] &quot;integer&quot;</span></span>
<span id="cb7-15"><a href="standards.html#cb7-15"></a><span class="kw">class</span> (x [, <span class="dv">1</span>, <span class="dt">drop =</span> <span class="ot">FALSE</span>]) <span class="co"># default</span></span>
<span id="cb7-16"><a href="standards.html#cb7-16"></a><span class="co">#&gt; [1] &quot;tbl_df&quot;     &quot;tbl&quot;        &quot;data.frame&quot;</span></span>
<span id="cb7-17"><a href="standards.html#cb7-17"></a></span>
<span id="cb7-18"><a href="standards.html#cb7-18"></a>x &lt;-<span class="st"> </span>data.table<span class="op">::</span><span class="kw">data.table</span> (x)</span>
<span id="cb7-19"><a href="standards.html#cb7-19"></a><span class="kw">class</span> (x [, <span class="dv">1</span>])</span>
<span id="cb7-20"><a href="standards.html#cb7-20"></a><span class="co">#&gt; [1] &quot;data.table&quot; &quot;data.frame&quot;</span></span>
<span id="cb7-21"><a href="standards.html#cb7-21"></a><span class="kw">class</span> (x [, <span class="dv">1</span>, <span class="dt">drop =</span> <span class="ot">TRUE</span>]) <span class="co"># no effect</span></span>
<span id="cb7-22"><a href="standards.html#cb7-22"></a><span class="co">#&gt; [1] &quot;data.table&quot; &quot;data.frame&quot;</span></span>
<span id="cb7-23"><a href="standards.html#cb7-23"></a><span class="kw">class</span> (x [, <span class="dv">1</span>, <span class="dt">drop =</span> <span class="ot">FALSE</span>]) <span class="co"># default</span></span>
<span id="cb7-24"><a href="standards.html#cb7-24"></a><span class="co">#&gt; [1] &quot;data.table&quot; &quot;data.frame&quot;</span></span></code></pre></div>
<ul>
<li>Extracting a single column from a <code>data.frame</code> returns a <code>vector</code> by default,
and a <code>data.frame</code> if <code>drop = FALSE</code>.</li>
<li>Extracting a single column from a <code>tibble</code> returns a single-column <code>tibble</code>
by default, and a <code>vector</code> is <code>drop = TRUE</code>.</li>
<li>Extracting a single column from a <code>data.table</code> always returns a <code>data.table</code>,
and the <code>drop</code> argument has no effect.</li>
</ul>
<p>Given such inconsistencies,</p>
<ul>
<li><strong>G2.9</strong> <em>Software should ensure that extraction or filtering of single columns
from tabular inputs should not presume any particular default behaviour, and
should ensure all column-extraction operations behave consistently regardless
of the class of tabular data used as input.</em></li>
</ul>
<p>Adherence to the above standard G2.6 will ensure that any implicitly or
explicitly assumed default behaviour will yield consistent results regardless
of input classes.</p>
</div>
<div id="missing-or-undefined-values" class="section level4">
<h4><span class="header-section-number">5.2.2.3</span> Missing or Undefined Values</h4>
<ul>
<li><strong>G2.10</strong> <em>Statistical Software should implement appropriate checks for missing
data as part of initial pre-processing prior to passing data to analytic
algorithms.</em></li>
<li><strong>G2.11</strong> <em>Where possible, all functions should provide options for users to
specify how to handle missing (<code>NA</code>) data, with options minimally including:</em>
<ul>
<li><strong>G2.11a</strong> <em>error on missing data</em></li>
<li><strong>G2.11b</strong> <em>ignore missing data with default warnings or messages issued</em></li>
<li><strong>G2.11c</strong> <em>replace missing data with appropriately imputed values</em></li>
</ul></li>
<li><strong>G2.12</strong> <em>Functions should never assume non-missingness, and should never pass
data with potential missing values to any base routines with default <code>na.rm = FALSE</code>-type parameters (such as
<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/mean.html"><code>mean()</code></a>,
<a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/sd.html"><code>sd()</code></a> or
<a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/cor.html"><code>cor()</code></a>).</em></li>
<li><strong>G2.13</strong> <em>All functions should also provide options to handle undefined values
(e.g., <code>NaN</code>, <code>Inf</code> and <code>-Inf</code>), including potentially ignoring or removing
such values.</em></li>
</ul>
</div>
</div>
<div id="output-structures" class="section level3">
<h3><span class="header-section-number">5.2.3</span> Output Structures</h3>
<ul>
<li><strong>G3.0</strong> <em>Statistical Software which enables outputs to be written to local files
should parse parameters specifying file names to ensure appropriate file
suffixes are automatically generated where not provided.</em></li>
</ul>
</div>
<div id="testing" class="section level3">
<h3><span class="header-section-number">5.2.4</span> Testing</h3>
<p>All packages should follow rOpenSci standards on
<a href="https://devguide.ropensci.org/building.html#testing">testing</a> and <a href="https://devguide.ropensci.org/ci.html">continuous
integration</a>, including aiming for high
test coverage. Extant R packages which may be useful for testing include
<a href="https://testthat.r-lib.org"><code>testthat</code></a>,
<a href="https://github.com/markvanderloo/tinytest"><code>tinytest</code></a>,
<a href="https://github.com/mikldk/roxytest"><code>roxytest</code></a>, and
<a href="https://github.com/LudvigOlsen/xpectr"><code>xpectr</code></a>.</p>
<ul>
<li><strong>G4.0</strong> <em>Where applicable or practicable, tests should use standard data sets
with known properties (for example, the <a href="https://www.itl.nist.gov/div898/strd/">NIST Standard Reference
Datasets</a>, or data sets provided by
other widely-used R packages).</em></li>
<li><strong>G4.1</strong> <em>Data sets created within, and used to test, a package should be
exported (or otherwise made generally available) so that users can confirm
tests and run examples.</em></li>
</ul>
<p>For testing <em>statistical algorithms</em>, tests should include tests of the
following types:</p>
<ul>
<li><strong>G4.2</strong> <strong>Correctness tests</strong> <em>to test that statistical algorithms produce
expected results to some fixed test data sets (potentially through
comparisons using binding frameworks such as
<a href="https://github.com/lbraglia/RStata">RStata</a>).</em>
<ul>
<li><strong>G4.2a</strong> <em>For new methods, it can be difficult to separate out correctness of the method from the correctness of the implementation, as there may not be reference for comparison. In this case, testing may be implemented against simple, trivial cases or against multiple implementations such as an initial R implementation compared with results from a C/C++ implementation.</em></li>
<li><strong>G4.2b</strong> <em>For new implementations of existing methods, correctness tests should include tests against previous implementations. Such testing may explicitly call those implementations in testing, preferably from fixed-versions of other software, or use stored outputs from those where that is not possible.</em></li>
<li><strong>G4.2c</strong> <em>Where applicable, stored values may be drawn from published paper outputs when applicable and where code from original implementations is not available</em></li>
</ul></li>
<li><strong>G4.3</strong> <em>Correctness tests should be run with a fixed random seed</em></li>
<li><strong>G4.4</strong> <strong>Parameter recovery tests</strong> <em>to test that the implementation produce
expected results given data with known properties. For instance, a linear
regression algorithm should return expected coefficient values for a
simulated data set generated from a linear model.</em>
<ul>
<li><strong>G4.4a</strong> <em>Parameter recovery tests should generally be expected to succeed within a defined tolerance rather than recovering exact values.</em></li>
<li><strong>G4.4b</strong> <em>Parameter recovery tests should be run with multiple random seeds when either data simulation or the algorithm contains a random component. (When long-running, such tests may be part of an extended, rather than regular, test suite; see G4.8-4.10, below).</em></li>
</ul></li>
<li><strong>G4.5</strong> <strong>Algorithm performance tests</strong> <em>to test that implementation performs
as expected as properties of data change. For instance, a test may show that
parameters approach correct estimates within tolerance as data size
increases, or that convergence times decrease for higher convergence
thresholds.</em></li>
<li><strong>G4.6</strong> <strong>Edge condition tests</strong> <em>to test that these conditions produce
expected behaviour such as clear warnings or errors when confronted with data
with extreme properties including but not limited to:</em>
<ul>
<li><strong>G4.6a</strong> <em>Zero-length data</em></li>
<li><strong>G4.6b</strong> <em>Data of unsupported types (e.g., character or complex numbers in for functions designed only for numeric data)</em></li>
<li><strong>G4.6c</strong> <em>Data with all-<code>NA</code> fields or columns or all identical fields or columns</em></li>
<li><strong>G4.6d</strong> <em>Data outside the scope of the algorithm (for example, data with more fields (columns) than observations (rows) for some regression algorithms)</em></li>
</ul></li>
<li><strong>G4.7</strong> <strong>Noise susceptibility tests</strong> <em>Packages should test for expected
stochastic behaviour, such as through the following conditions:</em>
<ul>
<li><strong>G4.7a</strong> <em>Adding trivial noise (for example, at the scale of <code>.Machine$double.eps</code>) to data does not meaningfully change results</em></li>
<li><strong>G4.7b</strong> <em>Running under different random seeds or initial conditions does not meaningfully change results</em></li>
</ul></li>
</ul>
<div id="extended-tests" class="section level4">
<h4><span class="header-section-number">5.2.4.1</span> Extended tests</h4>
<p>Thorough testing of statistical software may require tests on large data sets,
tests with many permutations, or other conditions leading to long-running
tests. In such cases it may be neither possible nor advisable to execute tests
continuously, or with every code change. Software should nevertheless test any
and all conditions regardless of how long tests may take, and in doing so
should adhere to the following standards:</p>
<ul>
<li><strong>G4.8</strong> <em>Extended tests should included and run under a common framework with
other tests but be switched on by flags such as as a
<code>&lt;MYPKG&gt;_EXTENDED_TESTS=1</code> environment variable.</em></li>
<li><strong>G4.9</strong> <em>Where extended tests require large data sets or other assets, these
should be provided for downloading and fetched as part of the testing
workflow.</em>
<ul>
<li><strong>G4.9a</strong> <em>When any downloads of additional data necessary for extended tests fail, the tests themselves should not fail, rather be skipped and implicitly succeed with an appropriate diagnostic message.</em></li>
</ul></li>
<li><strong>G4.10</strong> <em>Any conditions necessary to run extended tests such as platform
requirements, memory, expected runtime, and artefacts produced that may need
manual inspection, should be described in developer documentation such as a
<code>CONTRIBUTING.md</code> or <code>tests/README.md</code> file.</em></li>
</ul>
<!-- Edit the .Rmd not the .md file -->
</div>
</div>
</div>
<div id="bayesian-and-monte-carlo-software" class="section level2">
<h2><span class="header-section-number">5.3</span> Bayesian and Monte Carlo Software</h2>
<p>Bayesian and Monte Carlo Software (hereafter referred to for simplicity as
“Bayesian Software”) is presumed to perform one or more of the following steps:</p>
<ul>
<li><ol style="list-style-type: decimal">
<li>Document how to specify inputs including:</li>
</ol>
<ul>
<li>1.1 Data</li>
<li>1.2 Hyperparameters determining prior distributions</li>
<li>1.3 Parameters determining the computational processes</li>
</ul></li>
<li><ol start="2" style="list-style-type: decimal">
<li>Accept and validate all of forms of input</li>
</ol></li>
<li><ol start="3" style="list-style-type: decimal">
<li>Apply data transformation and pre-processing steps</li>
</ol></li>
<li><ol start="4" style="list-style-type: decimal">
<li>Apply one or more analytic algorithms, generally sampling algorithms used to
generate estimates of posterior distributions</li>
</ol></li>
<li><ol start="5" style="list-style-type: decimal">
<li>Return the result of that algorithmic application</li>
</ol></li>
<li><ol start="6" style="list-style-type: decimal">
<li>Offer additional functionality such as printing or summarising return results</li>
</ol></li>
</ul>
<p>This document details standards for each of these steps, each prefixed with “BS”.</p>
<div id="documentation-of-inputs" class="section level3">
<h3><span class="header-section-number">5.3.1</span> Documentation of Inputs</h3>
<p>Prior to actual standards for documentation of inputs, we note one
terminological standard for Bayesian software:</p>
<ul>
<li><strong>BS1.0</strong> <em>Bayesian software should use the term “hyperparameter” exclusively to
refer to parameters determining the form of prior distributions, and should
use either the generic term “parameter” or some conditional variant(s) such
as “computation parameters” to refer to all other parameters.</em></li>
</ul>
<p>Bayesian Software should provide the following documentation of how to specify
inputs:</p>
<ul>
<li><strong>BS1.1</strong> <em>Description of how to enter data, both in textual form and via code
examples. Both of these should consider the simplest cases of single objects
representing independent and dependent data, and potentially more complicated
cases of multiple independent data inputs.</em></li>
<li><strong>BS1.2</strong> <em>Description of how to specify prior distributions, both in textual form
describing the general principles of specifying prior distributions, along
with more applied descriptions and examples, within:</em>
<ul>
<li><strong>B31.2a</strong> <em>The main package <code>README</code>, either as textual description or example code</em></li>
<li><strong>B31.2b</strong> <em>At least one package vignette, both as general and applied textual descriptions, and example code</em></li>
<li><strong>B31.2c</strong> <em>Function-level documentation, preferably with code included in examples</em></li>
</ul></li>
<li><strong>BS1.3</strong> <em>Description of all parameters which control the computational process
(typically those determining aspects such as numbers and lengths of sampling
processes, seeds used to start them, thinning parameters determining post-hoc
sampling from simulated values, and convergence criteria). In particular:</em>
<ul>
<li><strong>BS1.3a</strong> <em>Bayesian Software should document, both in text and examples, how to use the output of previous simulations as starting points of subsequent simulations.</em></li>
<li><strong>BS1.3b</strong> <em>Where applicable, Bayesian software should document, both in text and examples, how to use different sampling algorithms for a given model.</em></li>
</ul></li>
<li><strong>BS1.4</strong> <em>For Bayesian Software which implements or otherwise enables convergence
checkers, documentation should explicitly describe and provide examples of
use with and without convergence checkers.</em></li>
<li><strong>BS1.5</strong> <em>For Bayesian Software which implements or otherwise enables multiple
convergence checkers, differences between these should be explicitly tested.</em></li>
</ul>
</div>
<div id="input-data-structures-and-validation" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Input Data Structures and Validation</h3>
<p>This section contains standards primarily intended to ensure that input data,
including model specifications, are validated prior to passing through to the
main computational algorithms.</p>
<div id="input-data" class="section level4">
<h4><span class="header-section-number">5.3.2.1</span> Input Data</h4>
<p>Bayesian Software is commonly designed to accept generic one- or
two-dimensional forms such as vector, matrix, or <code>data.frame</code> objects. The
first standards concerns the range of possible generic forms for input <em>data</em>:</p>
<ul>
<li><strong>BS2.0</strong> <em>Bayesian Software which accepts one-dimensional input should ensure
values are appropriately pre-processed regardless of class structures. The
<a href="https://github.com/r-quantities/units/"><code>units</code> package</a> provides a good
example, in creating objects that may be treated as vectors, yet which have a
class structure that does not inherit from the <code>vector</code> class. Using these
objects as input often causes software to fail. The <code>storage.mode</code> of the
underlying objects may nevertheless be examined, and the objects transformed
or processed accordingly to ensure such inputs do not lead to errors.</em></li>
<li><strong>BS2.1</strong> <em>Bayesian Software which accepts two-dimension input should implement
pre-processing routines to ensure conversion of as many possible forms as
possible to some standard format which is then passed to all analytic
functions. In particular, tests should demonstrate that:</em>
<ul>
<li><strong>BS2.1a</strong> <em><code>data.frame</code> or equivalent objects which have columns which do not themselves have standard class attributes (typically, <code>vector</code>) are appropriately processed, and do not error without reason. This behaviour should be tested. Again, columns created by the <a href="https://github.com/r-quantities/units/"><code>units</code> package</a> provide a good test case.</em></li>
<li><strong>BS2.1b</strong> <em><code>data.frame</code> or equivalent objects which have list columns should ensure that those columns are appropriately pre-processed either through being removed, converted to equivalent vector columns where appropriate, or some other appropriate treatment. This behaviour should be tested.</em></li>
</ul></li>
<li><strong>BS2.2</strong> <em>Bayesian Software should implement pre-processing routines to ensure
all input data is dimensionally commensurate, for example by ensuring
commensurate lengths of vectors or numbers of rows of tabular inputs.</em></li>
</ul>
</div>
<div id="prior-distributions-model-specifications-and-hyperparameters" class="section level4">
<h4><span class="header-section-number">5.3.2.2</span> Prior Distributions, Model Specifications, and Hyperparameters</h4>
<p>The second set of standards in this section concern specification of prior
distributions, model structures, or other equivalent ways of specifying
hypothesised relationships among input data structures. R already has a diverse
range of Bayesian Software with distinct approaches to this task, commonly
either through specifying a model as a character vector representing an R
function, or an external file either as R code, or encoded according to some
alternative system (such as for <a href="https://mc-stan.org/rstan/"><code>rstan</code></a>).</p>
<p>As explicated above, the term “hyperparameters” is interpreted here to refer to
parameters which define prior distributions, while a “model specification”, or
simply “model”, is an encoded description of how those hyperparameters are
hypothesised to transform to a posterior distribution.</p>
<p>Bayesian Software should:</p>
<ul>
<li><strong>BS2.3</strong> <em>Ensure that all appropriate validation and pre-processing of
hyperparameters are implemented as distinct pre-processing steps prior to
submitting to analytic routines, and especially prior to submitting to
multiple parallel computational chains.</em></li>
<li><strong>BS2.4</strong> <em>Ensure that lengths of hyperparameter vectors are checked, with no
excess values silently discarded (unless such output is explicitly
suppressed, as detailed below).</em></li>
<li><strong>BS2.5</strong> <em>Ensure that lengths of hyperparameter vectors are commensurate with
expected model input (see example immediately below)</em></li>
<li><strong>BS2.6</strong> <em>Where possible, implement pre-processing checks to validate
appropriateness of numeric values submitted for hyperparameters; for example,
by ensuring that hyperparameters defining second-order moments such as
distributional variance or shape parameters, or any parameters which are
logarithmically transformed, are non-negative.</em></li>
</ul>
<p>The following example demonstrates how standards like the above (BS2.5-2.6)
might be addressed. Consider the following function which defines a
log-likelihood estimator for a linear regression, controlled via a vector of
three hyperparameters, <code>p</code>:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="standards.html#cb8-1"></a>ll &lt;-<span class="st"> </span><span class="cf">function</span> (x, y, p) <span class="kw">dnorm</span> (y <span class="op">-</span><span class="st"> </span>(p[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>x <span class="op">*</span><span class="st"> </span>p[<span class="dv">2</span>]), <span class="dt">sd =</span> p[<span class="dv">3</span>], <span class="dt">log =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p>Pre-processing stages should be used to determine:</p>
<ol style="list-style-type: decimal">
<li>That the dimensions of the input data, <code>x</code> and <code>y</code>, are commensurate (BS2.2);
non-commensurate inputs should error by default.</li>
<li>The length of the vector <code>p</code> (BS2.4)</li>
</ol>
<p>The latter task is not necessarily straightforward, because the definition of
the function, <code>ll()</code>, will itself generally be part of the input to an actual
Bayesian Software function. This functional input thus needs to be examined to
determine expected lengths of hyperparameter vectors. The following code
illustrates one way to achieve this, relying on utilities for parsing function
calls in R, primarily through the
<a href="https://stat.ethz.ch/R-manual/R-devel/library/utils/html/getParseData.html"><code>getParseData</code></a>
function from the <code>utils</code> package. The parse data for a function can be
extracted with the following line:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="standards.html#cb9-1"></a>x &lt;-<span class="st"> </span><span class="kw">getParseData</span> (<span class="kw">parse</span> (<span class="dt">text =</span> <span class="kw">deparse</span> (ll)))</span></code></pre></div>
<p>The object <code>x</code> is a <code>data.frame</code> of every R token (such as an expression,
symbol, or operator) parsed from the function <code>ll</code>. The following section
illustrates how this data can be used to determine the expected lengths of
vector inputs to the function, <code>ll()</code>.</p>
<details>
<summary>
click to see details
</summary>
<p>
<p>Input arguments used to define parameter vectors in any R software are accessed
through R’s standard vector access syntax of <code>vec[i]</code>, for some element <code>i</code> of
a vector <code>vec</code>. The parse data for such begins with the <code>SYMBOL</code> of <code>vec</code>, the
<code>[</code>, a <code>NUM_CONST</code> for the value of <code>i</code>, and a closing <code>]</code>. The following code
can be used to extract elements of the parse data which match this pattern, and
ultimately to extract the various values of <code>i</code> used to access members of
<code>vec</code>.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="standards.html#cb10-1"></a>vector_length &lt;-<span class="st"> </span><span class="cf">function</span> (x, i) {</span>
<span id="cb10-2"><a href="standards.html#cb10-2"></a>    xn &lt;-<span class="st"> </span>x [<span class="kw">which</span> (x<span class="op">$</span>token <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span> (<span class="st">&quot;SYMBOL&quot;</span>, <span class="st">&quot;NUM_CONST&quot;</span>, <span class="st">&quot;&#39;[&#39;&quot;</span>, <span class="st">&quot;&#39;]&#39;&quot;</span>)), ]</span>
<span id="cb10-3"><a href="standards.html#cb10-3"></a>    <span class="co"># split resultant data.frame at first &quot;SYMBOL&quot; entry</span></span>
<span id="cb10-4"><a href="standards.html#cb10-4"></a>    xn &lt;-<span class="st"> </span><span class="kw">split</span> (xn, <span class="kw">cumsum</span> (xn<span class="op">$</span>token <span class="op">==</span><span class="st"> &quot;SYMBOL&quot;</span>))</span>
<span id="cb10-5"><a href="standards.html#cb10-5"></a>    <span class="co"># reduce to only those matching the above pattern</span></span>
<span id="cb10-6"><a href="standards.html#cb10-6"></a>    xn &lt;-<span class="st"> </span>xn [<span class="kw">which</span> (<span class="kw">vapply</span> (xn, <span class="cf">function</span> (j)</span>
<span id="cb10-7"><a href="standards.html#cb10-7"></a>                             j<span class="op">$</span>text [<span class="dv">1</span>] <span class="op">==</span><span class="st"> </span>i <span class="op">&amp;</span><span class="st"> </span><span class="kw">nrow</span> (j) <span class="op">&gt;</span><span class="st"> </span><span class="dv">3</span>,</span>
<span id="cb10-8"><a href="standards.html#cb10-8"></a>                             <span class="kw">logical</span> (<span class="dv">1</span>)))]</span>
<span id="cb10-9"><a href="standards.html#cb10-9"></a>    ret &lt;-<span class="st"> </span><span class="ot">NA_integer_</span> <span class="co"># default return value</span></span>
<span id="cb10-10"><a href="standards.html#cb10-10"></a>    <span class="cf">if</span> (<span class="kw">length</span> (xn) <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) {</span>
<span id="cb10-11"><a href="standards.html#cb10-11"></a>        <span class="co"># get all values of NUM_CONST as integers</span></span>
<span id="cb10-12"><a href="standards.html#cb10-12"></a>        n &lt;-<span class="st"> </span><span class="kw">vapply</span> (xn, <span class="cf">function</span> (j)</span>
<span id="cb10-13"><a href="standards.html#cb10-13"></a>                         <span class="kw">as.integer</span> (j<span class="op">$</span>text [j<span class="op">$</span>token <span class="op">==</span><span class="st"> &quot;NUM_CONST&quot;</span>] [<span class="dv">1</span>]),</span>
<span id="cb10-14"><a href="standards.html#cb10-14"></a>                         <span class="kw">integer</span> (<span class="dv">1</span>), <span class="dt">USE.NAMES =</span> <span class="ot">FALSE</span>)</span>
<span id="cb10-15"><a href="standards.html#cb10-15"></a>        <span class="co"># and return max of these</span></span>
<span id="cb10-16"><a href="standards.html#cb10-16"></a>        ret &lt;-<span class="st"> </span><span class="kw">max</span> (n)</span>
<span id="cb10-17"><a href="standards.html#cb10-17"></a>    }</span>
<span id="cb10-18"><a href="standards.html#cb10-18"></a>    <span class="kw">return</span> (ret)</span>
<span id="cb10-19"><a href="standards.html#cb10-19"></a>}</span></code></pre></div>
<p>That function can then be used to determine the length of any inputs which are
used as hyperparameter vectors:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="standards.html#cb11-1"></a>ll &lt;-<span class="st"> </span><span class="cf">function</span> (p, x, y) <span class="kw">dnorm</span> (y <span class="op">-</span><span class="st"> </span>(p[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>x <span class="op">*</span><span class="st"> </span>p[<span class="dv">2</span>]), <span class="dt">sd =</span> p[<span class="dv">3</span>], <span class="dt">log =</span> <span class="ot">TRUE</span>)</span>
<span id="cb11-2"><a href="standards.html#cb11-2"></a>p &lt;-<span class="st"> </span><span class="kw">parse</span> (<span class="dt">text =</span> <span class="kw">deparse</span> (ll))</span>
<span id="cb11-3"><a href="standards.html#cb11-3"></a>x &lt;-<span class="st"> </span>utils<span class="op">::</span><span class="kw">getParseData</span> (p)</span>
<span id="cb11-4"><a href="standards.html#cb11-4"></a></span>
<span id="cb11-5"><a href="standards.html#cb11-5"></a><span class="co"># extract the names of the parameters:</span></span>
<span id="cb11-6"><a href="standards.html#cb11-6"></a>params &lt;-<span class="st"> </span><span class="kw">unique</span> (x<span class="op">$</span>text [x<span class="op">$</span>token <span class="op">==</span><span class="st"> &quot;SYMBOL&quot;</span>])</span>
<span id="cb11-7"><a href="standards.html#cb11-7"></a>lens &lt;-<span class="st"> </span><span class="kw">vapply</span> (params, <span class="cf">function</span> (i) <span class="kw">vector_length</span> (x, i), <span class="kw">integer</span> (<span class="dv">1</span>))</span>
<span id="cb11-8"><a href="standards.html#cb11-8"></a>lens</span>
<span id="cb11-9"><a href="standards.html#cb11-9"></a><span class="co">#&gt;  y  p  x </span></span>
<span id="cb11-10"><a href="standards.html#cb11-10"></a><span class="co">#&gt; NA  3 NA</span></span></code></pre></div>
<p>And the vector <code>p</code> is used as a hyperparameter vector containing three
parameters. Any initial value vectors can then be examined to ensure that they
have this same length.</p>
<hr />
</p>
</details>
<p><br></p>
<p>Not all Bayesian Software is designed to accept model inputs expressed as R
code. The <a href="https://github.com/stan-dev/rstan"><code>rstan</code> package</a>, for example,
implements its own model specification language, and only allows
hyperparameters to be named, and not addressed by index. While this largely
avoids problems of mismatched lengths of parameter vectors, the software (at
v2.21.1) does not ensure the existence of named parameters prior to starting
the computational chains. This ultimately results in each chain generating an
error when a model specification refers to a non-existent or undefined
hyperparameter. Such controls should be part of a single pre-processing stage,
and so should only generate a single error.</p>
</div>
<div id="computational-parameters" class="section level4">
<h4><span class="header-section-number">5.3.2.3</span> Computational Parameters</h4>
<p>Computational parameters are considered here as those passed to Bayesian
functions other than hyperparameters determining the forms of prior
distributions. They typically include parameters controlling lengths of runs,
lengths of burn-in periods, numbers of parallel computations, other parameters
controlling how samples are to be generated, or convergence criteria. All
Computational Parameters should be checked for general “sanity” prior to
calling primary computational algorithms. The standards for such sanity checks
include that Bayesian Software should:</p>
<ul>
<li><strong>BS2.7</strong> <em>Check that values for parameters are positive (except where negative
values may be accepted)</em></li>
<li><strong>BS2.8</strong> <em>Check lengths and/or dimensions of inputs, and either automatically
reject or provide appropriate diagnostic messaging for parameters of
inappropriate length or dimension; for example passing a vector of length &gt; 1
to a parameter presumed to define a single value (unless such output is
explicitly suppressed, as detailed below)</em></li>
<li><strong>BS2.9</strong> <em>Check that arguments are of expected classes or types (for example,
check that <code>integer</code>-type arguments are indeed <code>integer</code>, with explicit
conversion via <code>as.integer</code> where not)</em></li>
<li><strong>BS2.10</strong> <em>Automatically reject parameters of inappropriate type (for example
<code>character</code> values passed for <code>integer</code>-type parameters that are unable to be
appropriately converted).</em></li>
</ul>
<p>The following two sub-sections consider particular cases of computational
parameters.</p>
</div>
<div id="seed-parameters" class="section level4">
<h4><span class="header-section-number">5.3.2.4</span> Seed Parameters</h4>
<p>Bayesian software should:</p>
<ul>
<li><strong>BS2.11</strong> <em>Enable seeds to be passed as a parameter (through a direct <code>seed</code>
argument or similar), or as a vector of parameters, one for each chain.</em></li>
<li><strong>BS2.12</strong> <em>Enable results of previous runs to be used as starting points for
subsequent runs</em></li>
</ul>
<p>Bayesian Software which implements parallel processing should:</p>
<ul>
<li><strong>BS2.13</strong> <em>Ensure each chain is started with a different seed by default</em></li>
<li><strong>BS2.14</strong> <em>Issue diagnostic messages when identical seeds are passed to distinct
computational chains</em></li>
<li><strong>BS2.15</strong> <em>Explicitly document advice </em>not* to use <code>set.seed()</code>*</li>
<li><strong>BS2.16</strong> <em>Provide the parameter with a </em>plural* name: for example,
“starting_values” and not “starting_value”*</li>
</ul>
<p>To avoid potential confusion between separate parameters to control random
seeds and starting values, we recommended a single “starting values” rather
than “seeds” argument, with appropriate translation of these parameters into
seeds where necessary.</p>
</div>
<div id="output-verbosity" class="section level4">
<h4><span class="header-section-number">5.3.2.5</span> Output Verbosity</h4>
<p>All Bayesian Software should implement computational parameters to control
output verbosity. Bayesian computations are often time-consuming, and often
performed as batch computations. The following standards should be adhered to
in regard to output verbosity:</p>
<ul>
<li><strong>BS2.17</strong> <em>Bayesian Software should implement at least one parameter controlling
the verbosity of output, defaulting to verbose output of all appropriate
messages, warnings, errors, and progress indicators.</em></li>
<li><strong>BS2.18</strong> <em>Bayesian Software should enable suppression of messages and progress
indicators, while retaining verbosity of warnings and errors. This should be
tested.</em></li>
<li><strong>BS2.19</strong> <em>Bayesian Software should enable suppression of warnings where
appropriate. This should be tested.</em></li>
<li><strong>BS2.20</strong> <em>Bayesian Software should explicitly enable errors to be caught, and
appropriately processed either through conversion to warnings, or otherwise
captured in return values. This should be tested.</em></li>
</ul>
</div>
</div>
<div id="pre-processing-and-data-transformation" class="section level3">
<h3><span class="header-section-number">5.3.3</span> Pre-processing and Data Transformation</h3>
<div id="missing-values" class="section level4">
<h4><span class="header-section-number">5.3.3.1</span> Missing Values</h4>
<p>Bayesian Software should:</p>
<ul>
<li><strong>BS3.0</strong> <em>Explicitly document assumptions made in regard to missing values; for
example that data is assumed to contain no missing (<code>NA</code>, <code>Inf</code>) values, and
that such values, or entire rows including any such values, will be
automatically removed from input data.</em></li>
<li><strong>BS3.1</strong> <em>Implement appropriate routines to pre-process missing values prior to
passing data through to main computational algorithms.</em></li>
</ul>
</div>
<div id="perfect-collinearity" class="section level4">
<h4><span class="header-section-number">5.3.3.2</span> Perfect Collinearity</h4>
<p>Where appropriate, Bayesian Software should:</p>
<ul>
<li><strong>BS3.2</strong> <em>Implement pre-processing routines to diagnose perfect collinearity, and
provide appropriate diagnostic messages or warnings</em></li>
<li><strong>BS3.3</strong> <em>Provide distinct routines for processing perfectly collinear data,
potentially bypassing sampling algorithms</em></li>
</ul>
<p>An appropriate test for BS3.3 would confirm that <code>system.time()</code> or equivalent
timing expressions for perfectly collinear data should be <em>less</em> than
equivalent routines called with non-collinear data. Alternatively, a test could
ensure that perfectly collinear data passed to a function with a stopping
criteria generated no results, while specifying a fixed number of iterations
may generate results.</p>
</div>
</div>
<div id="analytic-algorithms" class="section level3">
<h3><span class="header-section-number">5.3.4</span> Analytic Algorithms</h3>
<p>As mentioned, analytic algorithms for Bayesian Software are commonly algorithms
to simulate posterior distributions, and to draw samples from those
simulations. Numerous extent R packages implement and offer sampling
algorithms, and not all Bayesian Software will internally implement sampling
algorithms. The following standards apply to packages which do implement
internal sampling algorithms:</p>
<ul>
<li><strong>BS4.0</strong> <em>Packages should document sampling algorithms (generally via
literary citation, or reference to other software)</em></li>
<li><strong>BS4.1</strong> <em>Packages should provide explicit comparisons with external samplers
which demonstrate intended advantage of implementation (generally via tests,
vignettes, or both).</em></li>
</ul>
<p>Regardless of whether or not Bayesian Software implements internal sampling
algorithms, it should:</p>
<ul>
<li><strong>BS4.2</strong> <em>Implement at least one means to validate posterior estimates (for
example through the functionality of the <a href="https://cran.r-project.org/package=BayesValidate"><code>BayesValidate</code>
package</a>, noting that that
package has not been updated for almost 15 years, and such approaches may
need adapting; or the <a href="https://arxiv.org/abs/1804.06788">Simulation Based
Calibration</a> approach implemented in the
<a href="https://mc-stan.org/rstan"><code>rstan</code></a> function
<a href="https://mc-stan.org/rstan/reference/sbc.html"><code>sbc</code></a>).</em></li>
</ul>
<p>Where possible or applicable, Bayesian Software should:</p>
<ul>
<li><strong>BS4.3</strong> <em>Implement at least one type of convergence checker, and provide a
documented reference for that implementation.</em></li>
<li><strong>BS4.3</strong> <em>Enable computations to be stopped on convergence (although not
necessarily by default).</em></li>
<li><strong>BS4.5</strong> <em>Ensure that appropriate mechanisms are provided for models which do not
converge. This is often achieved by having default behaviour to stop after
specified numbers of iterations regardless of convergence.</em></li>
<li><strong>BS4.6</strong> <em>Implement tests to confirm that results with convergence checker are
statistically equivalent to results from equivalent fixed number of samples
without convergence checking.</em></li>
<li><strong>BS4.7</strong> <em>Where convergence checkers are themselves parametrised, the effects of
such parameters should also be tested. For threshold parameters, for example,
lower values should result in longer sequence lengths.</em></li>
</ul>
</div>
<div id="return-values" class="section level3">
<h3><span class="header-section-number">5.3.5</span> Return Values</h3>
<p>Unlike software in many other categories, Bayesian Software should generally
return several kinds of distinct data, both the raw data derived from
statistical algorithms, and associated metadata. Such distinct and generally
disparate forms of data will be generally best combined into a single object
through implementing a defined class structure, although other options are
possible, including (re-)using extant class structures (see the CRAN Task view
on <a href="https://cran.r-project.org/web/views/TimeSeries.html">Bayesian Inference</a>.
<a href="https://cran.r-project.org/web/views/Bayesian.html" class="uri">https://cran.r-project.org/web/views/Bayesian.html</a>) for reference to other
packages and class systems). Regardless of the precise form of return object,
and whether or not defined class structures are used or implemented, the
objects returned from Bayesian Software should include:</p>
<ul>
<li><strong>BS5.0</strong> <em>Seed(s) or starting value(s), including values for each sequences where
multiple sequences are included</em></li>
<li><strong>BS5.1</strong> <em>Appropriate metadata on types (or classes) and dimensions of input data</em></li>
</ul>
<p>With regard to the input function, or alternative means of specifying prior
distributions:</p>
<ul>
<li><strong>BS5.2</strong> <em>Bayesian Software should either:</em>
<ul>
<li><strong>BS5.2a</strong> <em>Return the input function or prior distributional specification in the return object; or</em></li>
<li><strong>BS5.2b</strong> <em>Enable direct access to such via additional functions which accept the return object as single argument.</em></li>
</ul></li>
</ul>
<p>Where convergence checkers are implemented or provided, Bayesian Software should:</p>
<ul>
<li><strong>BS5.3</strong> <em>Return convergence statistics or equivalent</em></li>
<li><strong>BS5.4</strong> <em>Where multiple checkers are enabled, return details of convergence checker used</em></li>
<li><strong>BS5.5</strong> <em>Appropriate diagnostic statistics to indicate absence of convergence
are either returned or immediately able to be accessed.</em></li>
</ul>
</div>
<div id="additional-functionality" class="section level3">
<h3><span class="header-section-number">5.3.6</span> Additional Functionality</h3>
<p>Bayesian Software should:</p>
<ul>
<li><strong>BS6.0</strong> <em>Implement a default <code>print</code> method for return objects</em></li>
<li><strong>BS6.1</strong> <em>Implement a default <code>plot</code> method for return objects</em></li>
<li><strong>BS6.2</strong> <em>Provide and document straightforward abilities to plot sequences of
posterior samples, with burn-in periods clearly distinguished</em></li>
<li><strong>BS6.3</strong> <em>Provide and document straightforward abilities to plot posterior
distributional estimates</em></li>
</ul>
<p>Bayesian Software may:</p>
<ul>
<li><strong>BS6.4</strong> <em>Provide <code>summary</code> methods for return objects</em></li>
<li><strong>BS6.5</strong> <em>Provide abilities to plot both sequences of posterior samples and
distributional estimates together in single graphic</em></li>
</ul>
</div>
<div id="tests" class="section level3">
<h3><span class="header-section-number">5.3.7</span> Tests</h3>
<ul>
<li>Parameter recovery tests:
<ul>
<li>Recover the prior with no data or data with no information, especially where priors are implicit.<br />
</li>
<li>Even in empirical Bayes, recover the estimated prior</li>
<li>Recover posterior given expected data <em>and</em> prior</li>
</ul></li>
<li>Algorithmic scaling tests with data - is it linear, log, etc</li>
<li>Test for prediction/fitted values on same scale as input values</li>
</ul>
<!-- Edit the .Rmd not the .md file -->
</div>
</div>
<div id="regression-and-supervised-learning" class="section level2">
<h2><span class="header-section-number">5.4</span> Regression and Supervised Learning</h2>
<p>This document details standards for Regression and Supervised Learning
Software – referred to from here on for simplicity as “Regression Software”.
Regression Software implements algorithms which aim to construct or analyse one
or more mappings between two defined data sets (for example, a set of
“independent” data, <span class="math inline">\(X\)</span>, and a set of “dependent” data, <span class="math inline">\(Y\)</span>). In contrast, the
analogous category of Unsupervised Learning Software aims to construct or
analyse one or more mappings between a defined set of input or independent
data, and a second set of “output” data which are not necessarily known or
given prior to the analysis.</p>
<p>Common purposes of Regression Software are to fit models to estimate
relationships or to make predictions between specified inputs and outputs.
Regression Software includes tools with inferential or predictive foci,
Bayesian, frequentist, or probability-free Machine Learning (ML) approaches,
parametric or or non-parametric approaches, discrete outputs (such as in
classification tasks) or continuous outputs, and models and algorithms specific
to applications or data such as time series or spatial data. In many cases
other standards specific to these subcategories may apply.</p>
<p>The following standards are divided among several sub-categories, with each
standard prefixed with “RE”.</p>
<div id="input-data-structures-and-validation-1" class="section level3">
<h3><span class="header-section-number">5.4.1</span> Input data structures and validation</h3>
<ul>
<li><strong>RE1.0</strong> <em>Regression Software should enable models to be specified via a formula
interface, unless reasons for not doing so are explicitly documented.</em></li>
<li><strong>RE1.1</strong> <em>Regression Software should document how formula interfaces are
converted to matrix representations of input data. See Max Kuhn’s <a href="https://rviews.rstudio.com/2017/02/01/the-r-formula-method-the-good-parts/">RStudio
blog
post</a>
for examples.</em></li>
<li><strong>RE1.2</strong> <em>Regression Software should document expected format (types or
classes) for inputting predictor variables, including descriptions of types
or classes which are not accepted; for example, specification that software
accepts only numeric inputs in <code>vector</code> or <code>matrix</code> form, or that all inputs
must be in <code>data.frame</code> form with both column and row names.</em></li>
<li><strong>RE1.3</strong> <em>Regression Software should transfer all relevant aspects of input
data, notably including row and column names, and potentially information
from other <code>attributes()</code>, to corresponding aspects of return objects (see
RE4, below).</em>
<ul>
<li><strong>RE1.3a</strong> <em>Where otherwise relevant information is not transferred, this should be explicitly documented.</em></li>
</ul></li>
<li><strong>RE1.4</strong> <em>Regression Software should document any assumptions made with regard to
input data; for example distributional assumptions, or assumptions that
predictor data have mean values of zero. Implications of violations of these
assumptions should be both documented and tested.</em></li>
</ul>
</div>
<div id="pre-processing-and-variable-transformation" class="section level3">
<h3><span class="header-section-number">5.4.2</span> Pre-processing and Variable Transformation</h3>
<ul>
<li><strong>RE2.0</strong> <em>Regression Software should document any transformations applied to
input data, for example conversion of label-values to <code>factor</code>, and should
provide ways to explicitly avoid any default transformations (with error or
warning conditions where appropriate).</em></li>
<li><strong>RE2.1</strong> <em>Regression Software should implement explicit parameters controlling
the processing of missing values, ideally distinguishing <code>NA</code> or <code>NaN</code> values
from <code>Inf</code> values (for example, through use of <code>na.omit()</code> and related
functions from the <code>stats</code> package).</em></li>
<li><strong>RE2.2</strong> <em>Regression Software should provide different options for processing
missing values in predictor and response data. For example, it should be
possible to fit a model with no missing predictor data in order to generate
values for all associated response points, even where submitted response
values may be missing.</em></li>
<li><strong>RE2.3</strong> <em>Where applicable, Regression Software should enable data to be centred
(for example, through converting to zero-mean equivalent values; or to
z-scores) or offset (for example, to zero-intercept equivalent values) via
additional parameters, with the effects of any such parameters clearly
documented and tested.</em></li>
<li><strong>RE2.4</strong> <em>Regression Software should implement pre-processing routines to
identify whether aspects of input data are perfectly collinear, notably
including:</em>
<ul>
<li><strong>RE2.4a</strong> <em>Perfect collinearity among predictor variables</em></li>
<li><strong>RE2.4b</strong> <em>Perfect collinearity between independent and dependent variables</em></li>
</ul></li>
</ul>
<p>These pre-processing routines should also be tested as described below.</p>
</div>
<div id="algorithms" class="section level3">
<h3><span class="header-section-number">5.4.3</span> Algorithms</h3>
<p>The following standards apply to the model fitting algorithms of Regression
Software which implements or relies on iterative algorithms which are expected
to converge to generate model statistics. Regression Software which implements
or relies on iterative convergence algorithms should:</p>
<ul>
<li><strong>RE3.0</strong> <em>Issue appropriate warnings or other diagnostic messages for models
which fail to converge.</em></li>
<li><strong>RE3.1</strong> <em>Enable such messages to be optionally suppressed, yet should ensure
that the resultant model object nevertheless includes sufficient data to
identify lack of convergence.</em></li>
<li><strong>RE3.2</strong> <em>Ensure that convergence thresholds have sensible default values,
demonstrated through explicit documentation.</em></li>
<li><strong>RE3.3</strong> <em>Allow explicit setting of convergence thresholds, unless reasons
against doing so are explicitly documented.</em></li>
</ul>
</div>
<div id="return-results" class="section level3">
<h3><span class="header-section-number">5.4.4</span> Return Results</h3>
<ul>
<li><strong>RE4.0</strong> <em>Regression Software should return some form of “model” object,
generally through using or modifying existing class structures for model
objects (such as <code>lm</code>, <code>glm</code>, or model objects from other packages), or
creating a new class of model objects.</em></li>
<li><strong>RE4.1</strong> <em>Regression Software may enable an ability to generate a model object
without actually fitting values. This may be useful for controlling batch
processing of computationally intensive fitting algorithms.</em></li>
</ul>
<div id="accessor-methods" class="section level4">
<h4><span class="header-section-number">5.4.4.1</span> Accessor Methods</h4>
<p>Regression Software should provide functions to access or extract as much of
the following kinds of model data as possible or practicable. Access should
ideally rely on class-specific methods which extend, or implement otherwise
equivalent versions of, the methods from the <code>stats</code> package which are named in
parentheses in each of the following standards.</p>
<p>Model objects should include, or otherwise enable effectively immediate access
to the following descriptors. It is acknowledged that not all regression models
can sensibly provide access to these descriptors, yet should include access
provisions to all those that are applicable.</p>
<ul>
<li><strong>RE4.2</strong> <em>Model coefficients (via <code>coeff()</code> / <code>coefficients()</code>)</em></li>
<li><strong>RE4.3</strong> <em>Confidence intervals on those coefficients (via <code>confint()</code>)</em></li>
<li><strong>RE4.4</strong> <em>The specification of the model, generally as a formula (via
<code>formula()</code>)</em></li>
<li><strong>RE4.5</strong> <em>Numbers of observations submitted to model (via <code>nobs()</code>)</em></li>
<li><strong>RE4.6</strong> <em>The variance-covariance matrix of the model parameters (via <code>vcov()</code>)</em></li>
<li><strong>RE4.7</strong> <em>Where appropriate, convergence statistics</em></li>
</ul>
<p>Regression Software <em>should</em> provide simple and direct methods to return or
otherwise access the following form of data and metadata, where the latter
includes information on any transformations which may have been applied to the
data prior to submission to modelling routines.</p>
<ul>
<li><strong>RE4.8</strong> <em>Response variables, and associated “metadata” where applicable.</em></li>
<li><strong>RE4.9</strong> <em>Modelled values of response variables.</em></li>
<li><strong>RE4.10</strong> <em>Model Residuals, including sufficient documentation to enable
interpretation of residuals, and to enable users to submit residuals to their
own tests.</em></li>
<li><strong>RE4.11</strong> <em>Goodness-of-fit and other statistics associated such as effect sizes
with model coefficients.</em></li>
<li><strong>RE4.12</strong> <em>Where appropriate, functions used to transform input data, and
associated inverse transform functions.</em></li>
</ul>
<p>Regression software <em>may</em> provide simple and direct methods to return or
otherwise access the following:</p>
<ul>
<li><strong>RE4.13</strong> <em>Predictor variables, and associated “metadata” where applicable.</em></li>
</ul>
</div>
<div id="prediction-extrapolation-and-forecasting" class="section level4">
<h4><span class="header-section-number">5.4.4.2</span> Prediction, Extrapolation, and Forecasting</h4>
<p>Not all regression software is intended to, or can, provide distinct abilities
to extrapolate or forecast. Moreover, identifying cases in which a regression
model is used to extrapolate or forecast may often be a non-trivial exercise.
It may nevertheless be possible, for example when input data used to construct
a model are unidimensional, and data on which a prediction is to be based
extend beyond the range used to construct the model. Where reasonably
unambiguous identification of extrapolation or forecasting using a model is
possible, the following standards apply:</p>
<ul>
<li><strong>RE4.14</strong> <em>Where possible, values should also be provided for extrapolation
or forecast </em>errors<em>.</em></li>
<li><strong>RE4.15</strong> <em>Sufficient documentation and/or testing should be provided to
demonstrate that forecast errors, confidence intervals, or equivalent values
increase with forecast horizons.</em></li>
</ul>
<p>Distinct from extrapolation or forecasting abilities, the following standard
applies to regression software which relies on, or otherwise provides abilities
to process, categorical grouping variables:</p>
<ul>
<li><strong>RE4.16</strong> <em>Regression Software which models distinct responses for different
categorical groups should include the ability to submit new groups to
<code>predict()</code> methods.</em></li>
</ul>
</div>
<div id="reporting-return-results" class="section level4">
<h4><span class="header-section-number">5.4.4.3</span> Reporting Return Results</h4>
<ul>
<li><strong>RE4.17</strong> <em>Model objects returned by Regression Software should implement or
appropriately extend a default <code>print</code> method which provides an on-screen
summary of model (input) parameters and (output) coefficients.</em></li>
<li><strong>RE4.18</strong> <em>Regression Software may also implement <code>summary</code> methods for model
objects, and in particular should implement distinct <code>summary</code> methods for
any cases in which calculation of summary statistics is computationally
non-trivial (for example, for bootstrapped estimates of confidence
intervals).</em></li>
</ul>
</div>
</div>
<div id="documentation-1" class="section level3">
<h3><span class="header-section-number">5.4.5</span> Documentation</h3>
<p>Beyond the general standards for documentation, Regression Software should
explicitly describe the following aspects, and ideally provide extended
documentation including summary graphical reports of:</p>
<ul>
<li><strong>RE5.0</strong> <em>Scaling relationships between sizes of input data (numbers of
observations, with potential extension to numbers of variables/columns) and
speed of algorithm.</em></li>
</ul>
</div>
<div id="visualization" class="section level3">
<h3><span class="header-section-number">5.4.6</span> Visualization</h3>
<ul>
<li><strong>RE6.0</strong> <em>Model objects returned by Regression Software (see RE3.0) should have
default <code>plot</code> methods, either through explicit implementation, extension of
methods for existing model objects, or through ensuring default methods work
appropriately.</em></li>
<li><strong>RE6.1</strong> <em>Where the default <code>plot</code> method is <strong>NOT</strong> a generic <code>plot</code> method
dispatched on the class of return objects (that is, through
a <code>plot.&lt;myclass&gt;</code> function), that method dispatch should nevertheless exist
in order to explicitly direct users to the appropriate function.</em></li>
<li><strong>RE6.2</strong> <em>The default <code>plot</code> method should produce a plot of the <code>fitted</code> values
of the model, with optional visualisation of confidence intervals or
equivalent.</em></li>
</ul>
<p>The following standard applies only to software fulfilling RE4.14-4.15, and the
conditions described prior to those standards.</p>
<ul>
<li><strong>RE6.3</strong> <em>Where a model object is used to generate a forecast (for example,
through a <code>predict()</code> method), the default <code>plot</code> method should provide clear
visual distinction between modelled (interpolated) and forecast
(extrapolated) values.</em></li>
</ul>
</div>
<div id="testing-1" class="section level3">
<h3><span class="header-section-number">5.4.7</span> Testing</h3>
<p>Tests for Regression Software should include the following conditions and cases:</p>
<ul>
<li><strong>RE7.0</strong> <em>Tests with noiseless, exact relationships between predictor
(independent) data.</em>
<ul>
<li><strong>RE7.0a</strong> <em>In particular, these tests should confirm that model fitting is at
least as fast or (preferably) faster than testing with equivalent noisy
data (see RE2.4a).</em></li>
</ul></li>
<li><strong>RE7.1</strong> <em>Tests with noiseless, exact relationships between predictor
(independent) and response (dependent) data.</em>
<ul>
<li><strong>RE7.1a</strong> <em>In particular, these tests should confirm that model fitting is at
least as fast or (preferably) faster than testing with equivalent noisy
data (see RE2.4b).</em></li>
</ul></li>
</ul>
<!-- Edit the .Rmd not the .md file -->
</div>
</div>
<div id="dimensionality-reduction-clustering-and-unsupervised-learning" class="section level2">
<h2><span class="header-section-number">5.5</span> Dimensionality Reduction, Clustering, and Unsupervised Learning</h2>
<p>This document details standards for Dimensionality Reduction, Clustering, and
Unsupervised Learning Software – referred to from here on for simplicity as
“Unsupervised Learning Software”. Software in this category is distinguished
from Regression Software though the latter aiming to construct or analyse one
or more mappings between two defined data sets (for example, a set of
“independent” data, <span class="math inline">\(X\)</span>, and a set of “dependent” data, “Y”), whereas
Unsupervised Learning Software aims to construct or analyse one or more
mappings between a defined set of input or independent data, and a second set
of “output” data which are not necessarily known or given prior to the
analysis. A key distinction in Unsupervised Learning Software and Algorithms is
between that for which output data represent (generally numerical)
transformations of the input data set, and that for which output data are
discrete labels applied to the input data. Examples of the former type include
dimensionality reduction and ordination software and algorithms, and examples
of the latter include clustering and discrete partitioning software and
algorithms.</p>
<div id="input-data-structures-and-validation-2" class="section level3">
<h3><span class="header-section-number">5.5.1</span> Input Data Structures and Validation</h3>
<ul>
<li><strong>UL1.0</strong> <em>Unsupervised Learning Software should explicitly document expected
format (types or classes) for input data, including descriptions of types or
classes which are not accepted; for example, specification that software
accepts only numeric inputs in <code>vector</code> or <code>matrix</code> form, or that all inputs
must be in <code>data.frame</code> form with both column and row names.</em></li>
<li><strong>UL1.1</strong> <em>Unsupervised Learning Software should provide distinct
sub-routines to assert that all input data is of the expected form, and issue
informative error messages when incompatible data are submitted.</em></li>
</ul>
<p>The following code demonstrates an example of a routine from the base <code>stats</code>
package which fails to meet this standard.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="standards.html#cb12-1"></a>d &lt;-<span class="st"> </span><span class="kw">dist</span>(USArrests) <span class="co"># example from help file for &#39;hclust&#39; function</span></span>
<span id="cb12-2"><a href="standards.html#cb12-2"></a>hc &lt;-<span class="st"> </span><span class="kw">hclust</span>(d) <span class="co"># okay</span></span>
<span id="cb12-3"><a href="standards.html#cb12-3"></a>hc &lt;-<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">as.matrix</span>(d))</span></code></pre></div>
<pre><code>## Error in if (is.na(n) || n &gt; 65536L) stop(&quot;size cannot be NA nor exceed 65536&quot;): missing value where TRUE/FALSE needed</code></pre>
<p>The latter fails, yet issues an uninformative error message that clearly
indicates a failure to provide sufficient checks on the class of input data.</p>
<ul>
<li><strong>UL1.2</strong> <em>Unsupervised learning which uses row or column names to label
output objects should assert that input data have non-default row or column
names, and issue an informative message when these are not provided. (Such
messages need not necessarily be provided by default, but should at least be
optionally available.)</em></li>
</ul>
<p>The following code provides simple examples of checks whether row and column
names appear to have generic default values.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="standards.html#cb14-1"></a>x &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">matrix</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dt">ncol =</span> <span class="dv">2</span>))</span>
<span id="cb14-2"><a href="standards.html#cb14-2"></a>x</span></code></pre></div>
<pre><code>##   X1 X2
## 1  1  6
## 2  2  7
## 3  3  8
## 4  4  9
## 5  5 10</code></pre>
<p>Generic row names are almost always simple integer sequences, which the
following condition confirms.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="standards.html#cb16-1"></a><span class="kw">identical</span>(<span class="kw">rownames</span>(x), <span class="kw">as.character</span>(<span class="kw">seq</span>(<span class="kw">nrow</span>(x))))</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>Generic column names may come in a variety of formats. The following code uses
a <code>grep</code> expression to match any number of characters plus an optional leading
zero followed by a generic sequence of column numbers, appropriate for matching
column names produced by generic construction of <code>data.frame</code> objects.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="standards.html#cb18-1"></a><span class="kw">all</span>(<span class="kw">vapply</span>(<span class="kw">seq</span>(<span class="kw">ncol</span>(x)), <span class="cf">function</span>(i) {</span>
<span id="cb18-2"><a href="standards.html#cb18-2"></a>  <span class="kw">grepl</span>(<span class="kw">paste0</span>(<span class="st">&quot;[[:alpha:]]0?&quot;</span>, i), <span class="kw">colnames</span>(x) [i])</span>
<span id="cb18-3"><a href="standards.html#cb18-3"></a>}, <span class="kw">logical</span>(<span class="dv">1</span>)))</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>Messages should be issued in both of these cases. The following code
illustrates that the <code>hclust</code> function does not implement any such checks or
assertions, rather it silently returns an object with default labels.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="standards.html#cb20-1"></a>u &lt;-<span class="st"> </span>USArrests</span>
<span id="cb20-2"><a href="standards.html#cb20-2"></a><span class="kw">rownames</span>(u) &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="kw">nrow</span>(u))</span>
<span id="cb20-3"><a href="standards.html#cb20-3"></a>hc &lt;-<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">dist</span>(u))</span>
<span id="cb20-4"><a href="standards.html#cb20-4"></a><span class="kw">head</span>(hc<span class="op">$</span>labels)</span></code></pre></div>
<pre><code>## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot;</code></pre>
<ul>
<li><strong>UL1.3</strong> <em>Unsupervised Learning Software should transfer all relevant aspects
of input data, notably including row and column names, and potentially
information from other <code>attributes()</code>, to corresponding aspects of return
objects.</em>
<ul>
<li><strong>UL1.3a</strong> <em>Where otherwise relevant information is not transferred,
this should be explicitly documented.</em></li>
</ul></li>
</ul>
<p>An example of a function according with UL1.3 is
<a href="https://stat.ethz.ch/R-manual/R-patched/library/stats/html/cutree.html"><code>stats::cutree()</code></a></p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="standards.html#cb22-1"></a>hc &lt;-<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">dist</span>(USArrests))</span>
<span id="cb22-2"><a href="standards.html#cb22-2"></a><span class="kw">head</span>(<span class="kw">cutree</span>(hc, <span class="dv">10</span>))</span></code></pre></div>
<pre><code>##    Alabama     Alaska    Arizona   Arkansas California   Colorado 
##          1          2          3          4          5          4</code></pre>
<p>The row names of <code>USArrests</code> are transferred to the output object. In contrast,
some routines from the <a href="https://cran.r-project.org/package=cluster"><code>cluster</code>
package</a> do not comply with this standard:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="standards.html#cb24-1"></a><span class="kw">library</span>(cluster)</span>
<span id="cb24-2"><a href="standards.html#cb24-2"></a>ac &lt;-<span class="st"> </span><span class="kw">agnes</span>(USArrests) <span class="co"># agglomerative nesting</span></span>
<span id="cb24-3"><a href="standards.html#cb24-3"></a><span class="kw">head</span>(<span class="kw">cutree</span>(ac, <span class="dv">10</span>))</span></code></pre></div>
<pre><code>## [1] 1 2 3 4 3 4</code></pre>
<p>The case labels are not appropriately carried through to the object returned by
<a href="https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/agnes.html"><code>agnes()</code></a>
to enable them to be transferred within
<a href="https://stat.ethz.ch/R-manual/R-patched/library/stats/html/cutree.html"><code>cutree()</code></a>.
(The labels are transferred to the object returned by <code>agnes</code>, just not in
a way that enables <code>cutree</code> to inherit them.)</p>
<ul>
<li><strong>UL1.4</strong> <em>Unsupervised Learning Software should explicitly document whether
input data may include missing values.</em></li>
<li><strong>UL1.5</strong> <em>Functions in Unsupervised Learning Software which do not admit
input data with missing values should provide informative error messages when
data with missing values are submitted.</em></li>
<li><strong>UL1.6</strong> <em>Unsupervised Learning Software should document any assumptions made
with regard to input data; for example assumptions about distributional forms
or locations (such as that data are centred or on approximately equivalent
distributional scales). Implications of violations of these assumptions
should be both documented and tested, in particular:</em>
<ul>
<li><strong>UL1.6a</strong> <em>Software which responds qualitatively differently to input
data which has components on markedly different scales should explicitly
document such differences, and implications of submitting such data.</em></li>
<li><strong>UL1.6b</strong> <em>Examples or other documentation should not use <code>scale()</code> or
equivalent transformations without explaining why scale is applied,
and explicitly illustrating and contrasting the consequences of not
applying such transformations.</em></li>
</ul></li>
</ul>
</div>
<div id="pre-processing-and-variable-transformation-1" class="section level3">
<h3><span class="header-section-number">5.5.2</span> Pre-processing and Variable Transformation</h3>
<ul>
<li><strong>UL2.0</strong> <em>Routines likely to give unreliable or irreproducible results in
response to violations of assumptions regarding input data (see UL1.6) should
implement pre-processing steps to diagnose potential violations, and issue
appropriately informative messages, and/or include parameters to enable
suitable transformations to be applied (such as the <code>center</code> and <code>scale.</code>
parameters of the
<a href="https://stat.ethz.ch/R-manual/R-patched/library/stats/html/prcomp.html"><code>stats::prcomp()</code></a>
function).</em></li>
<li><strong>UL2.1</strong> <em>Unsupervised Learning Software should document any transformations
applied to input data, for example conversion of label-values to <code>factor</code>,
and should provide ways to explicitly avoid any default transformations (with
error or warning conditions where appropriate).</em></li>
<li><strong>UL2.2</strong> <em>For Unsupervised Learning Software which accepts missing values in
input data, functions should implement explicit parameters controlling the
processing of missing values, ideally distinguishing <code>NA</code> or <code>NaN</code> values
from <code>Inf</code> values (for example, through use of <code>na.omit()</code> and related
functions from the <code>stats</code> package).</em></li>
<li><strong>UL2.3</strong> <em>Unsupervised Learning Software should implement pre-processing
routines to identify whether aspects of input data are perfectly collinear.</em></li>
</ul>
</div>
<div id="algorithms-1" class="section level3">
<h3><span class="header-section-number">5.5.3</span> Algorithms</h3>
<div id="labelling" class="section level4">
<h4><span class="header-section-number">5.5.3.1</span> Labelling</h4>
<ul>
<li><strong>UL3.1</strong> <em>Algorithms which apply sequential labels to input data (such as
clustering or partitioning algorithms) should ensure that the sequence
follows decreasing group sizes (so labels of “1”, “a”, or “A” describe the
largest group, “2”, “b”, or “B” the second largest, and so on.)</em></li>
</ul>
<p>Note that the <a href="https://stat.ethz.ch/R-manual/R-patched/library/stats/html/cutree.html"><code>stats::cutree()</code>
function</a>
does not accord with this standard:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="standards.html#cb26-1"></a>hc &lt;-<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">dist</span>(USArrests))</span>
<span id="cb26-2"><a href="standards.html#cb26-2"></a><span class="kw">table</span>(<span class="kw">cutree</span>(hc, <span class="dt">k =</span> <span class="dv">10</span>))</span></code></pre></div>
<pre><code>## 
##  1  2  3  4  5  6  7  8  9 10 
##  3  3  3  6  5 10  2  5  5  8</code></pre>
<p>The <a href="https://stat.ethz.ch/R-manual/R-patched/library/stats/html/cutree.html"><code>cutree()</code>
function</a>
applies arbitrary integer labels to the groups, yet the order of labels is not
related to the order of group sizes.</p>
<ul>
<li><strong>UL3.2</strong> <em>Dimensionality reduction or equivalent algorithms which label
dimensions should ensure that that sequences of labels follows decreasing
“importance” (for example, eigenvalues or variance contributions).</em></li>
</ul>
<p>The
<a href="https://stat.ethz.ch/R-manual/R-patched/library/stats/html/prcomp.html"><code>stats::prcomp</code></a>
function accords with this standard:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="standards.html#cb28-1"></a>z &lt;-<span class="st"> </span><span class="kw">prcomp</span>(eurodist, <span class="dt">rank =</span> <span class="dv">5</span>) <span class="co"># return maximum of 5 components</span></span>
<span id="cb28-2"><a href="standards.html#cb28-2"></a><span class="kw">summary</span>(z)</span></code></pre></div>
<pre><code>## Importance of first k=5 (out of 21) components:
##                              PC1       PC2       PC3       PC4       PC5
## Standard deviation     2529.6298 2157.3434 1459.4839 551.68183 369.10901
## Proportion of Variance    0.4591    0.3339    0.1528   0.02184   0.00977
## Cumulative Proportion     0.4591    0.7930    0.9458   0.96764   0.97741</code></pre>
<p>The proportion of variance explained by each component decreasing with
increasing numeric labelling of the components.</p>
<ul>
<li><strong>UL3.3</strong> <em>Unsupervised Learning Software for which input data does not
generally include labels (such as <code>array</code>-like data with no row names) should
provide an additional parameter to enable cases to be labelled.</em></li>
</ul>
</div>
<div id="prediction" class="section level4">
<h4><span class="header-section-number">5.5.3.2</span> Prediction</h4>
<ul>
<li><strong>UL3.4</strong> <em>Where applicable, Unsupervised Learning Software should implement
routines to predict the properties (such as numerical ordinates, or cluster
memberships) of additional new data without re-running the entire algorithm.</em></li>
</ul>
<p>While many algorithms such as Hierarchical clustering can not (readily) be used
to predict memberships of new data, other algorithms can nevertheless be
applied to perform this task. The following demonstrates how the output of
<a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/hclust.html"><code>stats::hclust</code></a>
can be used to predict membership of new data using the <a href="https://stat.ethz.ch/R-manual/R-devel/library/class/html/knn.html"><code>class:knn()</code>
function</a>.
(This is intended to illustrate only one of many possible approaches.)</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="standards.html#cb30-1"></a><span class="kw">library</span>(class)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;class&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:igraph&#39;:
## 
##     knn</code></pre>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="standards.html#cb33-1"></a>hc &lt;-<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">dist</span>(iris [, <span class="dv">-5</span>]))</span>
<span id="cb33-2"><a href="standards.html#cb33-2"></a>groups &lt;-<span class="st"> </span><span class="kw">cutree</span>(hc, <span class="dt">k =</span> <span class="dv">3</span>)</span>
<span id="cb33-3"><a href="standards.html#cb33-3"></a><span class="co"># function to randomly select part of a data.frame and # add some randomness</span></span>
<span id="cb33-4"><a href="standards.html#cb33-4"></a>sample_df &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">n =</span> <span class="dv">5</span>) {</span>
<span id="cb33-5"><a href="standards.html#cb33-5"></a>  x [<span class="kw">sample</span>(<span class="kw">nrow</span>(x), <span class="dt">size =</span> n), ] <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(<span class="kw">ncol</span>(x) <span class="op">*</span><span class="st"> </span>n)</span>
<span id="cb33-6"><a href="standards.html#cb33-6"></a>}</span>
<span id="cb33-7"><a href="standards.html#cb33-7"></a>iris_new &lt;-<span class="st"> </span><span class="kw">sample_df</span>(iris [, <span class="dv">-5</span>], <span class="dt">n =</span> <span class="dv">5</span>)</span>
<span id="cb33-8"><a href="standards.html#cb33-8"></a><span class="co"># use knn to predict membership of those new points:</span></span>
<span id="cb33-9"><a href="standards.html#cb33-9"></a>knnClust &lt;-<span class="st"> </span><span class="kw">knn</span>(<span class="dt">train =</span> iris [, <span class="dv">-5</span>], <span class="dt">test =</span> iris_new, <span class="dt">k =</span> <span class="dv">1</span>, <span class="dt">cl =</span> groups)</span>
<span id="cb33-10"><a href="standards.html#cb33-10"></a>knnClust</span></code></pre></div>
<pre><code>## [1] 2 2 2 2 1
## Levels: 1 2 3</code></pre>
<p>The <a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/prcomp.html"><code>stats::prcomp()</code>
function</a>
implements its own <code>predict()</code> method which conforms to this standard:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="standards.html#cb35-1"></a>res &lt;-<span class="st"> </span><span class="kw">prcomp</span>(USArrests)</span>
<span id="cb35-2"><a href="standards.html#cb35-2"></a>arrests_new &lt;-<span class="st"> </span><span class="kw">sample_df</span>(USArrests, <span class="dt">n =</span> <span class="dv">5</span>)</span>
<span id="cb35-3"><a href="standards.html#cb35-3"></a><span class="kw">predict</span>(res, <span class="dt">newdata =</span> arrests_new)</span></code></pre></div>
<pre><code>##                    PC1       PC2        PC3        PC4
## Minnesota    -97.95811  5.823427  0.7498986  0.3587406
## Washington   -24.57368 10.200141  5.2021127  2.2892112
## Utah         -49.26660 17.963733  2.6560388  1.3241555
## Indiana      -56.69576  3.794480  4.4823340 -2.2705452
## Pennsylvania -64.13197  9.495397 -2.3875273 -1.9402747</code></pre>
</div>
<div id="group-distributions-and-associated-statistics" class="section level4">
<h4><span class="header-section-number">5.5.3.3</span> Group Distributions and Associated Statistics</h4>
<p>Many unsupervised learning algorithms serve to label, categorise, or partition
data. Software which performs any of these tasks will commonly output some kind
of labelling or grouping schemes. The above example of principal components
illustrates that the return object records the standard deviations associated
with each component:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="standards.html#cb37-1"></a>res &lt;-<span class="st"> </span><span class="kw">prcomp</span>(USArrests)</span>
<span id="cb37-2"><a href="standards.html#cb37-2"></a><span class="kw">print</span>(res)</span></code></pre></div>
<pre><code>## Standard deviations (1, .., p=4):
## [1] 83.732400 14.212402  6.489426  2.482790
## 
## Rotation (n x k) = (4 x 4):
##                 PC1         PC2         PC3         PC4
## Murder   0.04170432 -0.04482166  0.07989066 -0.99492173
## Assault  0.99522128 -0.05876003 -0.06756974  0.03893830
## UrbanPop 0.04633575  0.97685748 -0.20054629 -0.05816914
## Rape     0.07515550  0.20071807  0.97408059  0.07232502</code></pre>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="standards.html#cb39-1"></a><span class="kw">summary</span>(res)</span></code></pre></div>
<pre><code>## Importance of components:
##                            PC1      PC2    PC3     PC4
## Standard deviation     83.7324 14.21240 6.4894 2.48279
## Proportion of Variance  0.9655  0.02782 0.0058 0.00085
## Cumulative Proportion   0.9655  0.99335 0.9991 1.00000</code></pre>
<p>Such output accords with the following standard:</p>
<ul>
<li><strong>UL3.5</strong> <em>Objects returned from Unsupervised Learning Software which labels,
categorise, or partitions data into discrete groups should include
quantitative information on intra-group variances or equivalent, as well as
on inter-group relationships where applicable. (In lieu of directly including
such information in return objects, additional methods may be developed to
provide access.)</em></li>
</ul>
<p>The above example of principal components is one where there are no inter-group
relationships, and so that standard is fulfilled by providing information on
intra-group variances alone. Discrete clustering algorithms, in contrast, yield
results for which inter-group relationships are meaningful, and such
relationships can generally be meaningfully provided. The <a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/hclust.html"><code>hclust()</code>
routine</a>,
like many clustering routines, simply returns a <em>scheme</em> for devising an
arbitrary number of clusters, and so
can not meaningfully provide variances or relationships between such. The
<a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/cutree.html"><code>cutree()</code>
function</a>,
however, does yield defined numbers of clusters, yet devoid of any quantitative
information on variances or equivalent.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="standards.html#cb41-1"></a>res &lt;-<span class="st"> </span><span class="kw">hclust</span>(<span class="kw">dist</span>(USArrests))</span>
<span id="cb41-2"><a href="standards.html#cb41-2"></a><span class="kw">str</span>(<span class="kw">cutree</span>(res, <span class="dt">k =</span> <span class="dv">5</span>))</span></code></pre></div>
<pre><code>##  Named int [1:50] 1 1 1 2 1 2 3 1 4 2 ...
##  - attr(*, &quot;names&quot;)= chr [1:50] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ...</code></pre>
<p>Compare that with the output of a largely equivalent routine, the <a href="https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/clara.html"><code>clara()</code>
function</a>
from the <a href="https://cran.r-project.org/package=cluster"><code>cluster</code> package</a>.</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="standards.html#cb43-1"></a><span class="kw">library</span>(cluster)</span>
<span id="cb43-2"><a href="standards.html#cb43-2"></a>cl &lt;-<span class="st"> </span><span class="kw">clara</span>(USArrests, <span class="dt">k =</span> <span class="dv">10</span>) <span class="co"># direct clustering into specified number of clusters</span></span>
<span id="cb43-3"><a href="standards.html#cb43-3"></a>cl<span class="op">$</span>clusinfo</span></code></pre></div>
<pre><code>##       size  max_diss   av_diss isolation
##  [1,]    4 24.708298 14.284874 1.4837745
##  [2,]    6 28.857755 16.759943 1.7329563
##  [3,]    6 44.640565 23.718040 0.9677229
##  [4,]    6 28.005892 17.382196 0.8442061
##  [5,]    6 15.901258  9.363471 1.1037219
##  [6,]    7 29.407822 14.817031 0.9080598
##  [7,]    4 11.764353  6.781659 0.8165753
##  [8,]    3  8.766984  5.768183 0.3547323
##  [9,]    3 18.848077 10.101505 0.7176276
## [10,]    5 16.477257  8.468541 0.6273603</code></pre>
<p>That object contains information on dissimilarities between each observation
and cluster medoids, which in the context of UL3.4 is “information on
intra-group variances or equivalent”. Moreover, inter-group information is also
available as the
<a href="https://stat.ethz.ch/R-manual/R-devel/library/cluster/html/silhouette.html">“silhouette”</a>
of the clustering scheme.</p>
</div>
</div>
<div id="return-results-1" class="section level3">
<h3><span class="header-section-number">5.5.4</span> Return Results</h3>
<ul>
<li><strong>UL4.0</strong> <em>Unsupervised Learning Software should return some form of “model”
object, generally through using or modifying existing class structures for
model objects, or creating a new class of model objects.</em></li>
<li><strong>UL4.1</strong> <em>Unsupervised Learning Software may enable an ability to generate
a model object without actually fitting values. This may be useful for
controlling batch processing of computationally intensive fitting algorithms.</em></li>
<li><strong>UL4.2</strong> <em>The return object from Unsupervised Learning Software should
include, or otherwise enable immediate extraction of, all parameters used to
control the algorithm used.</em></li>
</ul>
<div id="reporting-return-results-1" class="section level4">
<h4><span class="header-section-number">5.5.4.1</span> Reporting Return Results</h4>
<ul>
<li><strong>UL4.2</strong> <em>Model objects returned by Unsupervised Learning Software should
implement or appropriately extend a default <code>print</code> method which provides an
on-screen summary of model (input) parameters and methods used to generate
results. The <code>print</code> method may also summarise statistical aspects of the
output data or results.</em>
<ul>
<li><strong>UL4.2a</strong> <em>The default <code>print</code> method should always ensure only
a restricted number of rows of any result matrices or equivalent are
printed to the screen.</em></li>
</ul></li>
</ul>
<p>The <a href="https://stat.ethz.ch/R-manual/R-patched/library/stats/html/prcomp.html"><code>prcomp</code>
objects</a>
returned from the function of the same name include potential large matrices of
component coordinates which are by default printed in their entirety to the
screen. This is because the default print behaviour for most tabular objects in
R (<code>matrix</code>, <code>data.frame</code>, and objects from the <code>Matrix</code> package, for example)
is to print objects in their entirety (limited only by such options as
<code>getOption("max.print")</code>, which determines maximal numbers of printed objects,
such as lines of <code>data.frame</code> objects). Such default behaviour ought be
avoided, particularly in Unsupervised Learning Software which commonly returns
objects containing large numbers of numeric entries.</p>
<ul>
<li><strong>UL4.3</strong> <em>Unsupervised Learning Software should also implement <code>summary</code>
methods for model objects which should summarise the primary statistics used
in generating the model (such as numbers of observations, parameters of
methods applied). The <code>summary</code> method may also provide summary statistics
from the resultant model.</em></li>
</ul>
</div>
</div>
<div id="documentation-2" class="section level3">
<h3><span class="header-section-number">5.5.5</span> Documentation</h3>
</div>
<div id="visualization-1" class="section level3">
<h3><span class="header-section-number">5.5.6</span> Visualization</h3>
<ul>
<li><strong>UL6.0</strong> <em>Objects returned by Unsupervised Learning Software should have
default <code>plot</code> methods, either through explicit implementation, extension of
methods for existing model objects, through ensuring default methods work
appropriately, or through explicit reference to helper packages such as
<a href="https://github.com/kassambara/factoextra"><code>factoextra</code></a> and associated
functions.</em></li>
<li><strong>UL6.1</strong> <em>Where the default <code>plot</code> method is <strong>NOT</strong> a generic <code>plot</code> method
dispatched on the class of return objects (that is, through
a <code>plot.&lt;myclass&gt;</code> function), that method dispatch should nevertheless exist
in order to explicitly direct users to the appropriate function.</em></li>
<li><strong>UL6.2</strong> <em>Where default plot methods include labelling components of return
objects (such as cluster labels), routines should ensure that labels are
automatically placed to ensure readability, and/or that appropriate
diagnostic messages are issued where readability is likely to be compromised
(for example, through attempting to place too many labels).</em></li>
</ul>
</div>
<div id="testing-2" class="section level3">
<h3><span class="header-section-number">5.5.7</span> Testing</h3>
<!-- Edit the .Rmd not the .md file -->
</div>
</div>
<div id="exploratory-data-analysis" class="section level2">
<h2><span class="header-section-number">5.6</span> Exploratory Data Analysis</h2>
<p>Exploration is a part of all data analyses, and Exploratory Data Analysis (EDA)
is not something that is entered into and exited from at some point prior to
“real” analysis. Exploratory Analyses are also not strictly limited to <em>Data</em>,
but may extend to exploration of <em>Models</em> of those data. The category could
thus equally be termed, “<em>Exploratory Data and Model Analysis</em>”, yet we opt to
utilise the standard acronym of EDA in this document.</p>
<p>EDA is nevertheless somewhat different to many other categories included within
rOpenSci’s program for peer-reviewing statistical software. Primary differences include:</p>
<ul>
<li>EDA software often has a strong focus upon visualization, which is a category
which we have otherwise explicitly excluded from the scope of the project at
the present stage.</li>
<li>The assessment of EDA software requires addressing more general questions
than software in most other categories, notably including the important
question of intended audience(s).</li>
</ul>
<p>The following standards are accordingly somewhat differently structured than
equivalent standards developed to date for other categories, particularly
through being more qualitative and abstract. In particular, while documentation
is an important component of standards for all categories, clear and
instructive documentation is of paramount importance for EDA Software, and so
warrants its own sub-section within this document.</p>
<div id="documentation-standards" class="section level3">
<h3><span class="header-section-number">5.6.1</span> Documentation Standards</h3>
<p>The following refer to <em>Primary Documentation</em>, implying in main package
<code>README</code> or vignette(s), and <em>Secondary Documentation</em>, implying function-level
documentation.</p>
<p>The <em>Primary Documentation</em> (<code>README</code> and/or vignette(s)) of EDA software
should:</p>
<ul>
<li><strong>EA1.0</strong> <em>Identify one or more target audiences for whom the software is intended</em></li>
<li><strong>EA1.1</strong> <em>Identify the kinds of data the software is capable of analysing (see
</em>Kinds of Data* below).*</li>
<li><strong>EA1.2</strong> <em>Identify the kinds of questions the software is intended to help
explore; for example, are these questions:</em>
<ul>
<li><em>inferential?</em></li>
<li><em>predictive?</em></li>
<li><em>associative?</em></li>
<li><em>causal?</em></li>
<li><em>(or other modes of statistical enquiry?)</em></li>
</ul></li>
</ul>
<p>The <em>Secondary Documentation</em> (within individual functions) of EDA software
should:</p>
<ul>
<li><strong>EA1.3</strong> <em>Identify the kinds of data each function is intended to accept as input</em></li>
</ul>
</div>
<div id="input-data-1" class="section level3">
<h3><span class="header-section-number">5.6.2</span> Input Data</h3>
<p>A further primary difference of EDA software from that of our other categories
is that input data for statistical software may be generally presumed of one or
more specific types, whereas EDA software often accepts data of more general
and varied types. EDA software should aim to accept and appropriately transform
as many diverse kinds of input data as possible, through addressing the
following standards, considered in terms of the two cases of input data in uni-
and multi-variate form. All of the general standards for kinds of input (G2.0 -
G2.7) apply to input data for EDA Software.</p>
<div id="index-columns" class="section level4">
<h4><span class="header-section-number">5.6.2.1</span> Index Columns</h4>
<p>The following standards refer to an <em>index column</em>, which is understood to
imply an explicitly named or identified column which can be used to provide a
unique index index into any and all rows of that table. Index columns ensure
the universal applicability of standard table join operations, such as those
implemented via the <a href="https://dplyr.tidyverse.org"><code>dplyr</code> package</a>.</p>
<ul>
<li><strong>EA2.0</strong> <em>EDA Software which accepts standard tabular data and implements or
relies upon extensive table filter and join operations should utilise an
<strong>index column</strong> system</em></li>
<li><strong>EA2.1</strong> <em>All values in an index column must be unique, and this uniqueness
should be affirmed as a pre-processing step for all input data.</em></li>
<li><strong>EA2.2</strong> <em>Index columns should be explicitly identified, either:</em>
<ul>
<li><strong>EA2.2a</strong> <em>by using an appropriate class system, or</em></li>
<li><strong>EA2.2b</strong> <em>through setting an <code>attribute</code> on a table, <code>x</code>, of <code>attr(x, "index") &lt;- &lt;index_col_name&gt;</code>.</em></li>
</ul></li>
</ul>
<p>For EDA software which either implements custom classes or explicitly sets
attributes specifying index columns, these attributes should be used as the
basis of all table join operations, and in particular:</p>
<ul>
<li><strong>EA2.3</strong> <em>Table join operations should not be based on any assumed variable or
column names</em></li>
</ul>
</div>
<div id="multi-tabular-input" class="section level4">
<h4><span class="header-section-number">5.6.2.2</span> Multi-tabular input</h4>
<p>EDA software designed to accept multi-tabular input should:</p>
<ul>
<li><strong>EA2.4</strong> <em>Use and demand an explicit class system for such input (for example,
via the <a href="https://github.com/krlmlr/dm"><code>DM</code> package</a>).</em></li>
<li><strong>EA2.5</strong> <em>Ensure all individual tables follow the above standards for Index Columns</em></li>
</ul>
</div>
<div id="classes-and-sub-classes" class="section level4">
<h4><span class="header-section-number">5.6.2.3</span> Classes and Sub-Classes</h4>
<p><em>Classes</em> are understood here to be the classes define single input objects,
while <em>Sub-Classes</em> refer to the class definitions of components of input
objects (for example, of columns of an input <code>data.frame</code>). EDA software which
is intended to receive input in general vector formats (see <em>Uni-variate Input</em>
section of <em>General Standards</em>) should ensure:</p>
<ul>
<li><strong>EA2.6</strong> <em>Routines appropriately process vector input of custom classes,
including those which do not inherit from the <code>vector</code> class</em></li>
<li><strong>EA2.7</strong> <em>Routines should appropriately process vector data regardless of
additional attributes</em></li>
</ul>
<p>The following code illustrates some ways by which “metadata” defining classes
and additional attributes associated with a standard vector object may by
modified.</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="standards.html#cb45-1"></a>x &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span></span>
<span id="cb45-2"><a href="standards.html#cb45-2"></a><span class="kw">class</span> (x) &lt;-<span class="st"> &quot;notvector&quot;</span></span>
<span id="cb45-3"><a href="standards.html#cb45-3"></a><span class="kw">attr</span> (x, <span class="st">&quot;extra_attribute&quot;</span>) &lt;-<span class="st"> &quot;another attribute&quot;</span></span>
<span id="cb45-4"><a href="standards.html#cb45-4"></a><span class="kw">attr</span> (x, <span class="st">&quot;vector attribute&quot;</span>) &lt;-<span class="st"> </span><span class="kw">runif</span> (<span class="dv">5</span>)</span>
<span id="cb45-5"><a href="standards.html#cb45-5"></a><span class="kw">attributes</span> (x)</span>
<span id="cb45-6"><a href="standards.html#cb45-6"></a><span class="co">#&gt; $class</span></span>
<span id="cb45-7"><a href="standards.html#cb45-7"></a><span class="co">#&gt; [1] &quot;notvector&quot;</span></span>
<span id="cb45-8"><a href="standards.html#cb45-8"></a><span class="co">#&gt; </span></span>
<span id="cb45-9"><a href="standards.html#cb45-9"></a><span class="co">#&gt; $extra_attribute</span></span>
<span id="cb45-10"><a href="standards.html#cb45-10"></a><span class="co">#&gt; [1] &quot;another attribute&quot;</span></span>
<span id="cb45-11"><a href="standards.html#cb45-11"></a><span class="co">#&gt; </span></span>
<span id="cb45-12"><a href="standards.html#cb45-12"></a><span class="co">#&gt; $`vector attribute`</span></span>
<span id="cb45-13"><a href="standards.html#cb45-13"></a><span class="co">#&gt; [1] 0.03521663 0.49418081 0.60129563 0.75804346 0.16073301</span></span></code></pre></div>
<p>All statistical software should appropriately deal with such input
data, as exemplified by the <code>storage.mode()</code>, <code>length()</code>, and <code>sum()</code> functions
of the <code>base</code> package, which return the appropriate values regardless of
redefinition of class or additional attributes.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="standards.html#cb46-1"></a><span class="kw">storage.mode</span> (x)</span>
<span id="cb46-2"><a href="standards.html#cb46-2"></a><span class="co">#&gt; [1] &quot;integer&quot;</span></span>
<span id="cb46-3"><a href="standards.html#cb46-3"></a><span class="kw">length</span> (x)</span>
<span id="cb46-4"><a href="standards.html#cb46-4"></a><span class="co">#&gt; [1] 10</span></span>
<span id="cb46-5"><a href="standards.html#cb46-5"></a><span class="kw">sum</span> (x)</span>
<span id="cb46-6"><a href="standards.html#cb46-6"></a><span class="co">#&gt; [1] 55</span></span>
<span id="cb46-7"><a href="standards.html#cb46-7"></a><span class="kw">storage.mode</span> (<span class="kw">sum</span> (x))</span>
<span id="cb46-8"><a href="standards.html#cb46-8"></a><span class="co">#&gt; [1] &quot;integer&quot;</span></span></code></pre></div>
<p>Tabular inputs in <code>data.frame</code> class may contain columns which are
themselves defined by custom classes, and which possess additional attributes.
EDA Software which accepts tabular inputs should accordingly ensure:</p>
<ul>
<li><strong>EA2.8</strong> <em>EDA routines appropriately process tabular input of custom classes,
ideally by means of a single pre-processing routine which converts
tabular input to some standard form subsequently passed to all analytic
routines.</em></li>
<li><strong>EA2.9</strong> <em>EDA routines accept and appropriately process tabular input in
which individual columns may be of custom sub-classes including additional
attributes.</em></li>
</ul>
</div>
</div>
<div id="analytic-algorithms-1" class="section level3">
<h3><span class="header-section-number">5.6.3</span> Analytic Algorithms</h3>
<p>(There are no specific standards for analytic algorithms in EDA Software.)</p>
</div>
<div id="return-results-output-data" class="section level3">
<h3><span class="header-section-number">5.6.4</span> Return Results / Output Data</h3>
<ul>
<li><strong>EA4.0</strong> <em>EDA Software should ensure all return results have types which are
consistent with input types. For example, <code>sum</code>, <code>min</code>, or <code>max</code> values
applied to <code>integer</code>-type vectors should return <code>integer</code> values, while
<code>mean</code> or <code>var</code> will generally return <code>numeric</code> types.</em></li>
<li><strong>EA4.1</strong> <em>EDA Software should implement parameters to enable explicit control
of numeric precision</em></li>
<li><strong>EA4.2</strong> <em>The primary routines of EDA Software should return objects for which
default <code>print</code> and <code>plot</code> methods give sensible results. Default <code>summary</code>
methods may also be implemented.</em></li>
</ul>
</div>
<div id="visualization-and-summary-output" class="section level3">
<h3><span class="header-section-number">5.6.5</span> Visualization and Summary Output</h3>
<p>Visualization commonly represents one of the primary functions of EDA Software,
and thus visualization output is given greater consideration in this category
than in other categories in which visualization may nevertheless play an
important role. In particular, one component of this sub-category is <em>Summary
Output</em>, taken to refer to all forms of screen-based output beyond conventional
graphical output, including tabular and other text-based forms. Standards for
visualization itself are considered in the two primary sub-categories of static
and dynamic visualization, where the latter includes interactive visualization.</p>
<p>Prior to these individual sub-categories, we consider a few standards
applicable to visualization in general, whether static or dynamic.</p>
<ul>
<li><strong>EA5.0</strong> <em>Graphical presentation in EDA software should be as accessible as
possible or practicable. In particular, EDA software should consider
accessibility in terms of:</em>
<ul>
<li><strong>EA5.0a</strong> *Typeface sizes should default to sizes which explicitly enhance accessibility</li>
<li><strong>EA5.0b</strong> Default colour schemes should be carefully constructed to ensure accessibility.*</li>
</ul></li>
<li><strong>EA5.1</strong> <em>Any explicit specifications of typefaces which override default values
should consider accessibility</em></li>
</ul>
<div id="summary-and-screen-based-output" class="section level4">
<h4><span class="header-section-number">5.6.5.1</span> Summary and Screen-based Output</h4>
<ul>
<li><strong>EA5.2</strong> <em>Screen-based output should never rely on default print formatting of
<code>numeric</code> types, rather should also use some version of <code>round(., digits)</code>,
<code>formatC</code>, <code>sprintf</code>, or similar functions for numeric formatting according
the parameter described in EDA4.2.</em></li>
<li><strong>EA5.3</strong> <em>Column-based summary statistics should always indicate the
<code>storage.mode</code>, <code>class</code>, or equivalent defining attribute of each column (as,
for example, implemented in the default <code>print.tibble</code> method).</em></li>
</ul>
</div>
<div id="general-standards-for-visualization-static-and-dynamic" class="section level4">
<h4><span class="header-section-number">5.6.5.2</span> General Standards for Visualization (Static and Dynamic)</h4>
<ul>
<li><strong>EA5.4</strong> <em>All visualisations should include units on all axes, with sensibly
rounded values (for example, as produced by the <code>pretty()</code> function).</em></li>
</ul>
</div>
<div id="dynamic-visualization" class="section level4">
<h4><span class="header-section-number">5.6.5.3</span> Dynamic Visualization</h4>
<p>Dynamic visualization routines are commonly implemented as interfaces to
<code>javascript</code> routines. Unless routines have been explicitly developed as an
internal part of an R package, standards shall not be considered to apply to
the code itself, rather only to decisions present as user-controlled parameters
exposed within the R environment. That said, one standard may nevertheless be
applied, with an aim to minimise</p>
<ul>
<li><strong>EA5.5</strong> <em>Any packages which internally bundle libraries used for dynamic
visualization and which are also bundled in other, pre-existing R packages,
should explain the necessity and advantage of re-bundling that library.</em></li>
</ul>
<!-- Edit the .Rmd not the .md file -->
</div>
</div>
</div>
<div id="time-series-software" class="section level2">
<h2><span class="header-section-number">5.7</span> Time Series Software</h2>
<p>Time series software is presumed to perform one or more of the following steps:</p>
<ol style="list-style-type: decimal">
<li>Accept and validate input data</li>
<li>Apply data transformation and pre-processing steps</li>
<li>Apply one or more analytic algorithms</li>
<li>Return the result of that algorithmic application</li>
<li>Offer additional functionality such as printing or summarising return results</li>
</ol>
<p>This document details standards for each of these steps, each prefixed with “TS”.</p>
<div id="input-data-structures-and-validation-3" class="section level3">
<h3><span class="header-section-number">5.7.1</span> Input data structures and validation</h3>
<p>Input validation is an important software task, and an important part of our
standards. While there are many ways to approach validation, the class systems
of R offer a particularly convenient and effective means. For Time Series
Software in particular, a range of class systems have been developed, for which
we refer to the section “Time Series Classes” in the CRAN Task view on <a href="https://cran.r-project.org/web/views/TimeSeries.html">Time
Series Analysis"</a>, and
the class-conversion package <a href="https://www.tsbox.help/"><code>tsbox</code></a>. Software which
uses and relies on defined classes can often validate input through affirming
appropriate class(es). Software which does not use or rely on class systems
will generally need specific routines to validate input data structures. In
particular, because of the long history of time series software in R, and the
variety of class systems for representing time series data, new time series
package should accept as many different classes of input as possible by
according with the following standards:</p>
<ul>
<li><strong>TS1.0</strong> <em>Time Series Software should explicitly document the types and classes
of input data able to be passed to each function.</em></li>
<li><strong>TS1.1</strong> <em>Time Series Software should accept input data in as many time series
specific classes as possible.</em></li>
<li><strong>TS1.2</strong> <em>Time Series Software should implement validation routines to confirm
that inputs are of acceptable classes (or represented in otherwise
appropriate ways for software which does not use class systems).</em></li>
<li><strong>TS1.3</strong> <em>Time Series Software should implement a single pre-processing routine
to validate input data, and to appropriately transform it to a single uniform
type to be passed to all subsequent data-processing functions (the <a href="https://www.tsbox.help/"><code>tsbox</code>
package</a> provides one convenient approach for this).</em></li>
<li><strong>TS1.4</strong> <em>The pre-processing function described above should maintain all time-
or date-based components or attributes of input data.</em></li>
</ul>
<p>For Time Series Software which relies on or implements custom classes or types
for representing time-series data, the following standards should be adhered
to:</p>
<ul>
<li><strong>TS1.5</strong> <em>The software should ensure strict ordering of the time, frequency, or
equivalent ordering index variable.</em></li>
<li><strong>TS1.6</strong> <em>Any violations of ordering should be caught in the pre-processing stages
of all functions.</em></li>
</ul>
<div id="time-intervals-and-relative-time" class="section level4">
<h4><span class="header-section-number">5.7.1.1</span> Time Intervals and Relative Time</h4>
<p>While most common packages and classes for time series data assume <em>absolute</em>
temporal scales such as those represented in <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/as.POSIXlt.html"><code>POSIX</code>
classes</a>
for dates or times, time series may also be quantified on <em>relative</em> scales
where the temporal index variable quantifies intervals rather than absolute
times or dates. Many analytic routines which accept time series inputs in
absolute form are also appropriately applied to analogous data in relative
form, and thus many packages should accept time series inputs both in absolute
and relative forms. Software which can or should accept times series inputs in
relative form should:</p>
<ul>
<li><strong>TS1.7</strong> <em>Accept inputs defined via the <a href="https://github.com/r-quantities/units/"><code>units</code>
package</a> for attributing SI units to
R vectors.</em></li>
<li><strong>TS1.8</strong> <em>Where time intervals or periods may be days or months, be explicit
about the system used to represent such, particularly regarding whether a
calendar system is used, or whether a year is presumed to have 365 days,
365.2422 days, or some other value.</em></li>
</ul>
</div>
</div>
<div id="pre-processing-and-variable-transformation-2" class="section level3">
<h3><span class="header-section-number">5.7.2</span> Pre-processing and Variable Transformation</h3>
<div id="missing-data" class="section level4">
<h4><span class="header-section-number">5.7.2.1</span> Missing Data</h4>
<p>One critical pre-processing step for Time Series Software is the appropriate
handling of missing data. It is convenient to distinguish between <em>implicit</em>
and <em>explicit</em> missing data. For regular time series, explicit missing data may
be represented by <code>NA</code> values, while for irregular time series, implicit
missing data may be represented by missing rows. The difference is demonstrated
in the following table.</p>
<table>
<caption>
Missing Values
</caption>
<tbody>
<tr class="odd">
<td style="text-align: left;">
Time
</td>
<td style="text-align: left;">
value
</td>
</tr>
<tr class="even">
<td style="text-align: left;">
08:43
</td>
<td style="text-align: left;">
0.71
</td>
</tr>
<tr class="odd">
<td style="text-align: left;">
08:44
</td>
<td style="text-align: left;">
NA
</td>
</tr>
<tr class="odd">
<td style="text-align: left;">
08:45
</td>
<td style="text-align: left;">
0.28
</td>
</tr>
<tr class="odd">
<td style="text-align: left;">
08:47
</td>
<td style="text-align: left;">
0.34
</td>
</tr>
<tr class="odd">
<td style="text-align: left;">
08:48
</td>
<td style="text-align: left;">
0.07
</td>
</tr>
</tbody>
</table>
<p>The value for 08:46 is <em>implicitly missing</em>, while the value for 08:44 is
<em>explicitly missing</em>. These two forms of missingness may connote different
things, and may require different forms of pre-processing. With this in mind,
the following standards apply:</p>
<ul>
<li><strong>TS2.0</strong> <em>Appropriate checks for missing data, and associated transformation
routines, should be performed as part of initial pre-processing prior to
passing data to analytic algorithms.</em></li>
<li><strong>TS2.1</strong> <em>Time Series Software which presumes or requires regular data should
only allow </em>explicit* missing values, and should issue appropriate diagnostic
messages, potentially including errors, in response to any <em>implicit</em> missing
values.*</li>
<li><strong>TS2.2</strong> <em>Where possible, all functions should provide options for users to
specify how to handle missing data, with options minimally including:</em>
<ul>
<li><strong>TS2.2a</strong> <em>error on missing data.</em></li>
<li><strong>TS2.2b</strong> <em>warn or ignore missing data, and proceed to analyse irregular data, ensuring that results from function calls with regular yet missing data return identical values to submitting equivalent irregular data with no missing values.</em></li>
<li><strong>TS2.2c</strong> <em>replace missing data with appropriately imputed values.</em></li>
</ul></li>
<li><strong>TS2.3</strong> <em>Functions should never assume non-missingness, and should never pass
data with potential missing values to any base routines with default <code>na.rm = FALSE</code>-type parameters (such as
<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/mean.html"><code>mean()</code></a>,
<a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/sd.html"><code>sd()</code></a> or
<a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/cor.html"><code>var()</code></a>).</em></li>
</ul>
</div>
<div id="stationarity" class="section level4">
<h4><span class="header-section-number">5.7.2.2</span> Stationarity</h4>
<p>Time Series Software should explicitly document assumptions or requirements
made with respect to the stationarity or otherwise of all input data. In
particular, any (sub-)functions which assume or rely on stationarity should:</p>
<ul>
<li><strong>TS2.4</strong> <em>Consider stationarity of all relevant moments - typically first (mean)
and second (variance) order, or otherwise document why such
consideration may be restricted to lower orders only.</em></li>
<li><strong>TS2.5</strong> <em>Explicitly document all assumptions and/or requirements of stationarity</em></li>
<li><strong>TS2.6</strong> <em>Implement appropriate checks for all relevant forms of stationarity,
and either:</em>
<ul>
<li><strong>TS2.6a</strong> <em>issue diagnostic messages or warnings; or</em></li>
<li><strong>TS2.6b</strong> <em>enable or advise on appropriate transformations to ensure stationarity.</em></li>
</ul></li>
</ul>
<p>The two options in the last point (TS2.6b) respectively translate to <em>enabling</em>
transformations to ensure stationarity by providing appropriate routines,
generally triggered by some function parameter, or <em>advising</em> on appropriate
transformations, for example by directing users to additional functions able to
implement appropriate transformations.</p>
</div>
<div id="covariance-matrices" class="section level4">
<h4><span class="header-section-number">5.7.2.3</span> Covariance Matrices</h4>
<p>Where covariance matrices are constructed or otherwise used within or as input
to functions, they should:</p>
<ul>
<li><strong>TS2.7</strong> <em>Incorporate a system to ensure that both row and column orders follow
the same ordering as the underlying time series data. This may, for example,
be done by including the <code>index</code> attribute of the time series data as an
attribute of the covariance matrix.</em></li>
<li><strong>TS2.8</strong> <em>Where applicable, covariance matrices should also include specification
of appropriate units.</em></li>
</ul>
</div>
</div>
<div id="analytic-algorithms-2" class="section level3">
<h3><span class="header-section-number">5.7.3</span> Analytic Algorithms</h3>
<p>Analytic algorithms are considered here to reflect the core analytic components
of Time Series Software. These may be many and varied, and we explicitly
consider only a small subset here.</p>
<div id="forecasting" class="section level4">
<h4><span class="header-section-number">5.7.3.1</span> Forecasting</h4>
<p>Statistical software which implements forecasting routines should:</p>
<ul>
<li><strong>TS3.0</strong> <em>Provide tests to demonstrate at least one case in which errors widen
appropriately with forecast horizon.</em></li>
<li><strong>TS3.1</strong> <em>If possible, provide at least one test which violates TS3.0</em></li>
<li><strong>TS3.2</strong> <em>Document the general drivers of forecast errors or horizons, as
demonstrated via the particular cases of TS3.0 and TS3.1</em></li>
<li><strong>TS3.3</strong> <em>Either:</em>
<ul>
<li><strong>TS3.3a</strong> <em>Document, preferable via an example, how to trim forecast values based on a specified error margin or equivalent; or</em></li>
<li><strong>TS3.3b</strong> <em>Provide an explicit mechanism to trim forecast values to a specified error margin, either via an explicit post-processing function, or via an input parameter to a primary analytic function.</em></li>
</ul></li>
</ul>
</div>
</div>
<div id="return-results-2" class="section level3">
<h3><span class="header-section-number">5.7.4</span> Return Results</h3>
<p>For (functions within) Time Series Software which return time series data:</p>
<ul>
<li><strong>TS4.0</strong> <em>Return values should either:</em>
<ul>
<li><strong>TS4.0a</strong> <em>Be in same class as input data, for example by using the <a href="https://www.tsbox.help/"><code>tsbox</code> package</a> to re-convert from standard internal format (see 1.4, above); or</em></li>
<li><strong>TS4.0b</strong> <em>Be in a unique, preferably class-defined, format.</em></li>
</ul></li>
<li><strong>TS4.1</strong> <em>Any units included as attributes of input data should also be included
within return values.</em></li>
<li><strong>TS4.2</strong> <em>The type and class of all return values should be explicitly documented.</em></li>
</ul>
<p>For (functions within) Time Series Software which return data other than direct
series:</p>
<ul>
<li><strong>TS4.3</strong> <em>Return values should explicitly include all appropriate units and/or
time scales</em></li>
</ul>
<div id="data-transformation" class="section level4">
<h4><span class="header-section-number">5.7.4.1</span> Data Transformation</h4>
<p>Time Series Software which internally implements routines for transforming data
to achieve stationarity and which returns forecast values should:</p>
<ul>
<li><strong>TS4.4</strong> <em>Document the effect of any such transformations on forecast data,
including potential effects on both first- and second-order estimates.</em></li>
<li><strong>TS4.5</strong> <em>In decreasing order of preference, either:</em>
<ul>
<li><strong>TS4.5a</strong> <em>Provide explicit routines or options to back-transform data commensurate with original, non-stationary input data</em></li>
<li><strong>TS4.5b</strong> <em>Demonstrate how data may be back-transformed to a form commensurate with original, non-stationary input data.</em></li>
<li><strong>TS4.5c</strong> <em>Document associated limitations on forecast values</em></li>
</ul></li>
</ul>
</div>
<div id="forecasting-1" class="section level4">
<h4><span class="header-section-number">5.7.4.2</span> Forecasting</h4>
<p>Where Time Series Software implements or otherwise enables forecasting
abilities, it should return one of the following three kinds of information.
These are presented in decreasing order of preference, such that software
should strive to return the first kind of object, failing that the second, and
only the third as a last resort.</p>
<ul>
<li><strong>TS4.6</strong> <em>Time Series Software which implements or otherwise enables forecasting
should return either:</em>
<ul>
<li><strong>TS4.6a</strong> <em>A distribution object, for example via one of the many packages described in the CRAN Task View on <a href="https://cran.r-project.org/web/views/Distributions.html"><em>Probability Distributions</em></a> (or the new <a href="https://pkg.mitchelloharawild.com/distributional/"><code>distributional</code> package</a> as used in the <a href="https://fable.tidyverts.org"><code>fable</code> package</a> for time-series forecasting).</em></li>
<li><strong>TS4.6b</strong> <em>For each variable to be forecast, predicted values equivalent to first- and second-order moments (for example, mean and standard error values).</em></li>
<li><strong>TS4.6c</strong> <em>Some more general indication of error involved with forecast estimates.</em></li>
</ul></li>
</ul>
<p>Beyond these particular standards for return objects, Time Series Software
which implements or otherwise enables forecasting should:</p>
<ul>
<li><strong>TS4.7</strong> <em>Ensure that forecast (modelled) values are clearly distinguished from
observed (model or input) values, either (in this case in no order of
preference) by</em>
<ul>
<li><strong>TS4.7a</strong> <em>Returning forecast values alone</em></li>
<li><strong>TS4.7b</strong> <em>Returning distinct list items for model and forecast values</em></li>
<li><strong>TS4.7c</strong> <em>Combining model and forecast values into a single return object with an appropriate additional column clearly distinguishing the two kinds of data.</em></li>
</ul></li>
</ul>
</div>
</div>
<div id="visualization-2" class="section level3">
<h3><span class="header-section-number">5.7.5</span> Visualization</h3>
<p>Time Series Software should:</p>
<ul>
<li><strong>TS5.0</strong> <em>Implement default <code>plot</code> methods for any implemented class system.</em></li>
<li><strong>TS5.1</strong> <em>When representing results in temporal domain(s), ensure that one axis
is clearly labelled “time” (or equivalent), with continuous units.</em></li>
<li><strong>TS5.2</strong> <em>Default to placing the “time” (or equivalent) variable on the
horizontal axis.</em></li>
<li><strong>TS5.3</strong> <em>Ensure that units of the time, frequency, or index variable are printed
by default on the axis.</em></li>
<li><strong>TS5.4</strong> <em>For frequency visualization, abscissa spanning <span class="math inline">\([-\pi, \pi]\)</span> should be
avoided in favour positive units of <span class="math inline">\([0, 2\pi]\)</span> or <span class="math inline">\([0, 0.5]\)</span>, in all cases
with appropriate additional explanation of units.</em></li>
<li><strong>TS5.5</strong> <em>Provide options to determine whether plots of data with missing values
should generate continuous or broken lines.</em></li>
</ul>
<p>For the results of forecast operations, Time Series Software should</p>
<ul>
<li><strong>TS5.6</strong> <em>By default indicate distributional limits of forecast on plot</em></li>
<li><strong>TS5.7</strong> <em>By default include model (input) values in plot, as well as forecast
(output) values</em></li>
<li><strong>TS5.8</strong> <em>By default provide clear visual distinction between model (input)
values and forecast (output) values.</em></li>
</ul>
<!-- Edit the .Rmd not the .md file -->
</div>
</div>
<div id="machine-learning-software" class="section level2">
<h2><span class="header-section-number">5.8</span> Machine Learning Software</h2>
<p>R has an extensive and diverse ecosystem of Machine Learning (ML) software
which is very well described in the corresponding <a href="https://cran.r-project.org/web/views/MachineLearning.html">CRAN Task
View</a>. Unlike most
other categories of statistical software considered here, the primary
distinguishing feature of ML software is not (necessarily or directly)
algorithmic, rather pertains to a <em>workflow</em> typical of machine learning tasks.
In particular, we consider ML software to approach data analysis via the two
primary steps of:</p>
<ol style="list-style-type: decimal">
<li>Passing a set of <em>training</em> data to an algorithm in order to generate a
candidate mapping between that data and some form of pre-specified output
or response variable. Such mappings will be referred to here as “models”,
with a single analysis of a single set of training data generating one
model.</li>
<li>Passing a set of test data to the model(s) generated by the first step in
order to derive some measure of predictive accuracy for that model.</li>
</ol>
<p>A single ML task generally yields two distinct outputs:</p>
<ol style="list-style-type: decimal">
<li>The model derived in the first of the previous steps; and</li>
<li>Associated statistics of model performance (as evaluated within the context
of the test data used to assess that performance).</li>
</ol>
<p><strong>A Machine Learning Workflow</strong></p>
<p>Given those initial considerations, we now attempt the difficult task of
envisioning a typical standard workflow for inherently diverse ML software. The
following workflow ought to be considered an “extensive” workflow, with shorter
versions, and correspondingly more restricted sets of standards, possible
dependent upon envisioned areas of application. For example, the workflow
presumes input data to be too large to be stored as a single entity in local
memory. Adaptation to situations in which all training data can be loaded into
memory may mean that some of the following workflow stages, and therefore
corresponding standards, may not apply.</p>
<p>Just as typical workflows are potentially very diverse, so are outputs of ML
software, which depend on areas of application and intended purpose of
software. The following refers to the “desired output” of ML software,
a phrase which is intentionally left non-specific, but which it intended to
connote any and all forms of “response variable” and other “pre-specified
outputs” such as categorical labels or validation data, along with outputs
which may not necessarily be able to be pre-specified in simple uni- or
multi-variate form, such as measures of distance between sets of training and
validation data.</p>
<p>Such “desired outputs” are presumed to be quantified in terms of a “loss” or
“cost” function (hereafter, simply “loss function”) quantifying some measure of
distance between a model estimate (resulting from applying the model to one or
more components of a training data set) and a pre-defined “valid” output
(during training), or a test data set (following training).</p>
<p>Given the foregoing considerations, we consider a typical ML workflow to
progress through (at least some of) the following steps:</p>
<ol style="list-style-type: decimal">
<li><strong><em>Input Data Specification</em></strong> Obtain a local copy of input data, often as
multiple <em>objects</em> (either on-disk or in memory) in some suitably structured
form such as in a series of sub-directories or accompanied by additional
data defining the structural properties of input objects. Regardless of
form, multiple objects are commonly given generic labels which distinguish
between <code>training</code> and <code>test</code> data, along with optional additional
categories and labels such as <code>validation</code> data used, for example, to
determine accuracy of models applied to training data yet prior to testing.</li>
<li><strong><em>Pre-Processing</em></strong> Define transformations of input data, including but not
restricted to, broadcasting dimensions (as defined below) and standardising
data ranges (typically to defined values of mean and standard deviation).</li>
<li><strong><em>Model and Algorithm Specification</em></strong> Specify the model and associated
processes which will be applied to map the input data on to the desired
output. This step minimally includes the following distinct stages
(generally in no particular order):
<ol style="list-style-type: lower-alpha">
<li>Specify the kind of model which will be applied to the training data. ML
software often allows the use of pre-trained models, in which case this
this step includes downloading or otherwise obtaining a pre-trained
model, along with specification of which aspects of those models are to
be modified through application to a particular set of training and
validation data.</li>
<li>Specify the kind of algorithm which will be used to explore the search
space (for example some kind of gradient descent algorithm), along with
parameters controlling how that algorithm will be applied (for example
a learning rate, as defined above).</li>
<li>Specify the kind of loss function will be used to quantify distance
between model estimates and desired output.</li>
</ol></li>
<li><strong><em>Model Training</em></strong> Apply the specified model to the training data to
generate a series of estimates from the specified loss function. This stage
may also include specifying parameters such as stopping or exit criteria,
and parameters controlling batch processing of input data. Moreover, this
stage may involve retaining some of the following additional data:
<ol style="list-style-type: lower-alpha">
<li>Potential “pre-processing” stages such as initial estimates of optimal
learning rates (see above).</li>
<li>Details of summaries of actual paths taken through the search space
towards convergence on local or global minimum.</li>
</ol></li>
<li><strong><em>Model Output and Performance</em></strong> Measure the performance of the trained
model when applied to the test data set, generally requiring the
specification of a metric of model performance or accuracy.</li>
</ol>
<p>Importantly, ML workflows may be partly iterative. This may in turn potentially
confound distinctions between training and test data, and accordingly confound
expectations commonly placed upon statistical analyses of statistical
independence of response variables. ML routines such as cross-validation
repeatedly (re-)partition data between training and test sets. Resultant models
can then not be considered to have been developed through application to any
single set of truly “independent” data. In the context of the standards that
follow, these considerations admit a potential lack of clarity in any notional
categorical distinction between training and test data, and between model
specification and training.</p>
<p>The preceding workflow mentioned a couple of concepts the definitions of which
may be seen by clicking on the corresponding items below. Following that, we
proceed to standards for ML software, enumerated and developed with reference
to the preceding workflow steps. As described above, these steps may not be
applicable to all ML software, and so all of the following standards should be
considered to be conditioned on “where applicable.” In order that the following
standards initially adhere to the enumeration of workflow steps given above,
more general standards pertaining to aspects such as documentation and testing
are given following the initial five “workflow” standards.</p>
<details>
<summary>
Click for a definition of <em>broadcasting</em>, referred to in Step 2, above.
</summary>
<p>
<p>The following definition comes from a vignette for the <a href="https://github.com/r-lib/rray"><code>rray</code>
package</a> named
<a href="https://rray.r-lib.org/articles/broadcasting.html"><em>Broadcasting</em></a>.</p>
<ul>
<li><strong><em>Broadcasting</em></strong> is, “repeating the dimensions of one object to match the
dimensions of another.”</li>
</ul>
<p>This concept runs counter to aspects of standards in other categories, which
often suggest that functions should error when passed input objects which do
not have commensurate dimensions. Broadcasting is a pre-processing step which
enables objects with incommensurate dimensions to be dimensionally reconciled.</p>
<p>The following demonstration is taken directly from the <a href="https://github.com/r-lib/rray"><code>rray</code>
package</a> (which is not currently on CRAN).</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="standards.html#cb47-1"></a><span class="kw">library</span> (rray)</span>
<span id="cb47-2"><a href="standards.html#cb47-2"></a>a &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="dt">dim =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb47-3"><a href="standards.html#cb47-3"></a>b &lt;-<span class="st"> </span><span class="kw">array</span>(<span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">4</span>), <span class="dt">dim =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb47-4"><a href="standards.html#cb47-4"></a><span class="co"># rbind (a, b) # error!</span></span>
<span id="cb47-5"><a href="standards.html#cb47-5"></a><span class="kw">rray_bind</span> (a, b, <span class="dt">.axis =</span> <span class="dv">1</span>)</span>
<span id="cb47-6"><a href="standards.html#cb47-6"></a><span class="co">#&gt;      [,1] [,2]</span></span>
<span id="cb47-7"><a href="standards.html#cb47-7"></a><span class="co">#&gt; [1,]    1    1</span></span>
<span id="cb47-8"><a href="standards.html#cb47-8"></a><span class="co">#&gt; [2,]    2    2</span></span>
<span id="cb47-9"><a href="standards.html#cb47-9"></a><span class="co">#&gt; [3,]    3    4</span></span>
<span id="cb47-10"><a href="standards.html#cb47-10"></a><span class="kw">rray_bind</span> (a, b, <span class="dt">.axis =</span> <span class="dv">2</span>)</span>
<span id="cb47-11"><a href="standards.html#cb47-11"></a><span class="co">#&gt;      [,1] [,2] [,3]</span></span>
<span id="cb47-12"><a href="standards.html#cb47-12"></a><span class="co">#&gt; [1,]    1    3    4</span></span>
<span id="cb47-13"><a href="standards.html#cb47-13"></a><span class="co">#&gt; [2,]    2    3    4</span></span></code></pre></div>
<p>Broadcasting is commonly employed in ML software because it enables ML
operations to be implemented on objects with incommensurate dimensions.
One example is image analysis, in which training data may all be dimensionally
commensurate, yet test images may have different dimensions. Broadcasting
allows data to be submitted to ML routines regardless of potentially
incommensurate dimensions.</p>
</p>
</details>
<details>
<summary>
Click for a definition of <em>learning rate</em>, referred to in Step 5, above.
</summary>
<p>
<ul>
<li><strong><em>Learning Rate</em></strong> (generally) determines the step size used to search for
local optima as a fraction of the local gradient.</li>
</ul>
<p>This parameter is particularly important for training ML algorithms like
neural networks, the results of which can be very sensitive to
variations in learning rates. A useful overview of the importance of
learning rates, and a useful approach to automatically determining
appropriate values, is given in <a href="https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html">this blog
post</a>.</p>
</p>
</details>
<p>Partly because of widespread and current relevance, the category of Machine
Learning software is one for which there have been other notable attempts to
develop standards. A particularly useful reference is the <a href="https://www.mlperf.org/">MLPerf
organization</a> which, among other activities, hosts
several <a href="https://github.com/mlperf">github repositories</a> providing reference
datasets and benchmark conditions for comparing performance aspects of ML
software. While such reference or benchmark standards are not explicitly
referred to in the current version of the following standards, we expect them
to be gradually adapted and incorporated as we start to apply and refine our
standards in application to software submitted to our review system.</p>
<div id="input-data-specification" class="section level3">
<h3><span class="header-section-number">5.8.1</span> Input Data Specification</h3>
<p>Many of the following standards refer to the labelling of input data as
“testing” or “training” data, along with potentially additional labels such as
“validation” data. In regard to such labelling, the following two standards apply,</p>
<ul>
<li><strong>ML1.0</strong> Documentation should make a clear conceptual distinction between
training and test data (even where such may ultimately be confounded as
described above.)
<ul>
<li><strong>ML1.0a</strong> Where these terms are ultimately eschewed, these should
nevertheless be used in initial documentation, along with clear
explanation of, and justification for, alternative terminology.</li>
</ul></li>
<li><strong>ML1.1</strong> Absent clear justification for alternative design decisions, input
data should be expected to be labelled “test”, “training”, and, where
applicable, “validation” data.
<ul>
<li><strong>ML1.1a</strong> The presence and use of these labels should be explicitly
confirmed via pre-processing steps (and tested in accordance with
<strong>ML7.0</strong>, below).</li>
<li><strong>ML1.1b</strong> Matches to expected labels should be case-insensitive and
based on partial matching such that, for example, “Test”, “test”, or
“testing” should all suffice.</li>
</ul></li>
</ul>
<p>The following three standards (<strong>ML1.2</strong>–<strong>ML1.4</strong>) represent three possible
design intentions for ML software. Only one of these three will generally be
applicable to any one piece of software, although it is nevertheless possible
that more than one of these standards may apply. The first of these three
standards applies to ML software which is intended to process, or capable of
processing, input data as a single (generally tabular) object.</p>
<ul>
<li><strong>ML1.2</strong> Training and test data sets for ML software should be able to be
input as a single, generally tabular, data object, with the training and test
data distinguished either by
<ul>
<li>A specified variable containing, for example, <code>TRUE</code>/<code>FALSE</code> or <code>0</code>/<code>1</code>
values, or which uses some other system such as missing (<code>NA</code>) values to
denote test data); and/or</li>
<li>An additional parameter designating case or row numbers, or labels of
test data.</li>
</ul></li>
</ul>
<p>The second of these three standards applies to ML software which is intended to
process, or capable of processing, input data represented as multiple objects
which exist in local memory.</p>
<ul>
<li><strong>ML1.3</strong> Input data should be clearly partitioned between training and test
data (for example, through having each passed as a distinct <code>list</code> item), or
should enable an additional means of categorically distinguishing training
from test data (such as via an additional parameter which provides explicit
labels). Where applicable, distinction of validation and any other data
should also accord with this standard.</li>
</ul>
<p>The third of these three standards for data input applies to ML software for
which data are expected to be input as references to multiple external objects,
generally expected to be read from either local or remote connections.</p>
<ul>
<li><strong>ML1.4</strong> Training and test data sets, along with other necessary components
such as validation data sets, should be stored in their own distinctly
labelled sub-directories (for distinct files), or according to an explicit
and distinct labelling scheme (for example, for database connections).
Labelling should in all cases adhere to <strong>ML1.1</strong>, above.</li>
</ul>
<p>The following standard applies to all ML software regardless of the
applicability or otherwise of the preceding three standards.</p>
<ul>
<li><strong>ML1.5</strong> ML software should implement a single function which summarises the
contents of test and training (and other) data sets, minimally including
counts of numbers of cases, records, or files, and potentially extending to
tables or summaries of file or data types, sizes, and other information (such
as unique hashes for each component).</li>
</ul>
<div id="missing-values-1" class="section level4">
<h4><span class="header-section-number">5.8.1.1</span> Missing Values</h4>
<p>Missing data are handled differently by different ML routines, and it is also
difficult to suggest generally applicable standards for pre-processing missing
values in ML software. The following standards attempt to cover a practical
range of typical approaches and applications.</p>
<ul>
<li><strong>ML1.6</strong> ML software which does not admit missing values, and which expects
no missing values, should implement explicit pre-processing routines to
identify whether data has any missing values, and should generally error
appropriately and informatively when passed data with missing values. In
addition, ML software which does not admit missing values should:
<ul>
<li><strong>ML1.6a</strong> Explain why missing values are not admitted.</li>
<li><strong>ML1.6b</strong> Provide explicit examples (in function documentation,
vignettes, or both) for how missing values may be imputed, rather than
simply discarded.</li>
</ul></li>
<li><strong>ML1.7</strong> ML software which admits missing values should clearly document how
such values are processed.
<ul>
<li><strong>ML1.7a</strong> Where missing values are imputed, software should offer
multiple user-defined ways to impute missing data.</li>
<li><strong>ML1.7b</strong> Where missing values are imputed, the precise imputation steps
should also be explicitly documented, either in tests (see <strong>ML7.2</strong>
below), function documentation, or vignettes.</li>
</ul></li>
<li><strong>ML1.8</strong> ML software should enable equal treatment of missing values for
both training and test data, with optional user ability to control
application to either one or both.</li>
</ul>
</div>
</div>
<div id="pre-processing" class="section level3">
<h3><span class="header-section-number">5.8.2</span> Pre-processing</h3>
<p>As reflected in the workflow envisioned at the outset, ML software operates
somewhat differently to statistical software in many other categories. In
particular, ML software often requires explicit specification of a workflow,
including specification of input data (as per the standards of the preceding
sub-section), and of both transformations and statistical models to be applied
to those data. This section of standards refers exclusively to the
transformation of input data as a pre-processing step prior to any
specification of, or submission to, actual models.</p>
<ul>
<li><strong>ML2.0</strong> A dedicated function should enable pre-processing steps to be
defined and parametrized.
<ul>
<li><strong>ML2.0a</strong> That function should return an object which can be directly
submitted to a specified model (see section 3, below).</li>
<li><strong>ML2.0b</strong> Absent explicit justification otherwise, that return object
should have a defined class minimally intended to implement a default
<code>print</code> method which summarizes the input data set (as per <strong>ML1.5</strong>
above) and associated transformations (see the following standard).</li>
</ul></li>
</ul>
<p>Standards for most other categories of statistical software suggest that
pre-processing routines should ensure that input data sets are commensurate,
for example, through having equal numbers of cases or rows. In contrast, ML
software is commonly intended to accept input data which can not be guaranteed
to be dimensionally commensurate, such as software intended to process
rectangular image files which may be of different sizes.</p>
<ul>
<li><strong>ML2.1</strong> ML software which uses broadcasting to reconcile dimensionally
incommensurate input data should offer an ability to at least optionally
record transformations applied to each input file.</li>
</ul>
<p>Beyond broadcasting and dimensional transformations, the following standards
apply to the pre-processing stages of ML software.</p>
<ul>
<li><strong>ML2.2</strong> ML software which requires or relies upon numeric transformations
of input data (such as change in mean values or variances) should allow
optimal explicit specification of target values, rather than restricting
transformations to default generic values only (such as transformations to
z-scores).
<ul>
<li><strong>ML2.2a</strong> Where the parameters have default values, reasons for those
particular defaults should be explicitly described.</li>
<li><strong>ML2.2b</strong> Any extended documentation (such as vignettes) which
demonstrates the use of explicit values for numeric transformations
should explicitly describe why particular values are used.</li>
</ul></li>
</ul>
<p>For all transformations applied to input data, whether of dimension (<strong>ML2.1</strong>)
or scale (<strong>ML2.2</strong>),</p>
<ul>
<li><strong>ML2.3</strong> The values associated with all transformations should be recorded
in the object returned by the function described in the preceding standard
(<strong>ML2.0</strong>).</li>
<li><strong>ML2.4</strong> Default values of all transformations should be explicitly
documented, both in documentation of parameters where appropriate (such as
for numeric transformations), and in extended documentation such as
vignettes.</li>
<li><strong>ML2.5</strong> ML software should provide options to bypass or otherwise switch
off all default transformations.</li>
<li><strong>ML2.6</strong> Where transformations are implemented via distinct functions, these
should be exported to a package’s namespace so they can be applied in other
contexts.</li>
<li><strong>ML2.7</strong> Where possible, documentation should be provided for how
transformations may be reversed. For example, documentation may demonstrate
how the values retained via <strong>ML2.3</strong>, above, can be used along with
transformations either exported via <strong>ML2.6</strong> or otherwise exemplified in
demonstration code to independently transform data, and then to reverse those
transformations.</li>
</ul>
</div>
<div id="model-and-algorithm-specification" class="section level3">
<h3><span class="header-section-number">5.8.3</span> Model and Algorithm Specification</h3>
<p>A “model” in the context of ML software is understood to be a means of
specifying a mapping between input and output data, generally applied to
training and validation data. Model specification is the step of specifying
<em>how</em> such a mapping is to be constructed. The specification of <em>what</em> the
values of such a model actually are occurs through training the model, and is
described in the following sub-section. These standards also refer to <em>control
parameters</em> which specify how models are trained. These parameters commonly
include values specifying numbers of iterations, training rates, and parameters
controlling algorithmic processes such as re-sampling or cross-validation.</p>
<ul>
<li><strong>ML3.0</strong> Model specification should be implemented as a distinct stage
subsequent to specification of pre-processing routines (see Section 2, above)
and prior to actual model fitting or training (see Section 4, below). In
particular,
<ul>
<li><strong>ML3.0a</strong> A dedicated function should enable models to be specified
without actually fitting or training them, <em>or</em> if this (<strong>ML3</strong>) and the
following (<strong>ML4</strong>) stages are controlled by a single function, that
function should have a parameter enabling models to be specified yet not
fitted (for example, <code>nofit = FALSE</code>).</li>
<li><strong>ML3.0b</strong> That function should accept as input the objects produced by
the previous <em>Input Data Specification</em> stage, and defined according to
<strong>ML2.0</strong>, above.</li>
<li><strong>ML3.0c</strong> The function described above (<strong>ML3.0a</strong>) should return an
object which can be directly trained as described in the following
sub-section (<strong>ML4</strong>).</li>
<li><strong>ML3.0d</strong> That return object should have a defined class minimally
intended to implement a default <code>print</code> method which summarises the model
specification, including values of all relevant parameters.</li>
</ul></li>
<li><strong>ML3.1</strong> ML software should allow the use of both untrained models,
specified through model parameters only, as well as pre-trained models. Use
of the latter commonly entails an ability to submit a previously-trained
model object to the function defined according to <strong>ML3.0a</strong>, above.</li>
<li><strong>ML3.2</strong> ML software should enable different models to be applied to the
object specifying data inputs and transformations (see sub-sections 1–2,
above) without needing to re-define those preceding steps.</li>
</ul>
<p>A function fulfilling <strong>ML3.0–3.2</strong> might, for example, permit the following
arguments:</p>
<ol style="list-style-type: decimal">
<li><code>data</code>: Input data specification constructed according to <strong>ML1</strong></li>
<li><code>model</code>: An optional previously-trained model</li>
<li><code>control</code>: A list of parameters controlling how the model algorithm is to be
applied during the subsequent training phase (<strong>ML4</strong>).</li>
</ol>
<p>A function with the arguments defined above would fulfil the preceding three
standards, because the <code>data</code> stage would represent the output of <strong>ML1</strong>,
while the <code>model</code> stage would allow for different pre-trained models to be
submitted using the same data and associated specifications (<strong>ML3.1</strong>). The
provision of a separate <code>.data</code> argument would fulfil <strong>ML3.2</strong> by allowing one
or both <code>model</code> or <code>control</code> parameters to be re-defined while submitting the
same <code>data</code> object.</p>
<ul>
<li><strong>ML3.3</strong> Where ML software implements its own distinct classes of model
objects, the properties and behaviours of those specific classes of objects
should be explicitly compared with objects produced by other ML software. In
particular, where possible, ML software should provide extended documentation
(as vignettes or equivalent) comparing model objects with those from other ML
software, noting both unique abilities and restrictions of any implemented
classes.</li>
<li><strong>ML3.4</strong> Where training rates are used, ML software should provide explicit
documentation both in all functions which use training rates, and in extended
form such as vignettes, of the importance of, and/or sensitivity to, different
values of training rates. In particular,
<ul>
<li><strong>ML3.4a</strong> Unless explicitly justified otherwise, ML software should
offer abilities to automatically determine appropriate or optimal
training rates, either as distinct pre-processing stages, or as implicit
stages of model training.</li>
<li><strong>ML3.4b</strong> ML software which provides default values for training rates
should clearly document anticipated restrictions of validity of those
default values; for example through clear suggestions that
user-determined and -specified values may generally be necessary or
preferable.</li>
</ul></li>
</ul>
<div id="control-parameters" class="section level4">
<h4><span class="header-section-number">5.8.3.1</span> Control Parameters</h4>
<p>Control parameters are considered here to specify how a model is to be applied
to a set of training data. These are generally distinct from parameters
specifying the actual model (such as model architecture). While we recommend
that control parameters be submitted as items of a single named list, this is
neither a firm expectation nor an explicit part of the current standards.</p>
<ul>
<li><strong>ML3.5</strong> Parameters controlling optimization algorithms should minimally include:
<ul>
<li><strong>ML3.5a</strong> Specification of the type of algorithm used to explore the
search space (commonly, for example, some kind of gradient descent
algorithm)</li>
<li><strong>ML3.5b</strong> The kind of loss function used to assess distance between
model estimates and desired output.</li>
</ul></li>
<li><strong>ML3.6</strong> Unless explicitly justified otherwise (for example because ML
software under consideration is an implementation of one specific algorithm),
ML software should:
<ul>
<li><strong>ML3.6a</strong> Implement or otherwise permit usage of multiple ways of exploring search space</li>
<li><strong>ML3.6b</strong> Implement or otherwise permit usage of multiple loss functions.</li>
</ul></li>
</ul>
</div>
<div id="cpu-and-gpu-processing" class="section level4">
<h4><span class="header-section-number">5.8.3.2</span> CPU and GPU processing</h4>
<p>ML software often involves manipulation of large numbers of rectangular arrays
for which graphics processing units (GPUs) are often more efficient than
central processing units (CPUs). ML software thus commonly offers options to
train models using either CPUs or GPUs. While these standards do not currently
suggest any particular design choice in this regard, we do note the following:</p>
<ul>
<li><strong>ML3.7</strong> For ML software in which algorithms are coded in C++,
user-controlled use of either CPUs or GPUs (on NVIDIA processors at least)
should be implemented through direct use of
<a href="https://github.com/NVIDIA/libcudacxx"><code>libcudacxx</code></a>.</li>
</ul>
<p>This library can be “switched on” through activating a single C++ header file
to switch from CPU to GPU.</p>
</div>
</div>
<div id="model-training" class="section level3">
<h3><span class="header-section-number">5.8.4</span> Model Training</h3>
<p>Model training is the stage of the ML workflow envisioned here in which the
actual computation is performed by applying a model specified according to
<strong>ML3</strong> to data specified according to <strong>ML1</strong> and <strong>ML2</strong>.</p>
<ul>
<li><strong>ML4.0</strong> ML software should generally implement a unified single-function
interface to model training, able to receive as input a model specified
according to all preceding standards. In particular, models with
categorically different specifications, such as different model architectures
or optimization algorithms, should be able to be submitted to the same model
training function.</li>
<li><strong>ML4.1</strong> ML software should at least optionally retain explicit information
on paths taken as an optimizer advances towards minimal loss. Such
information should minimally include:
<ul>
<li><strong>ML4.1a</strong> Specification of all model-internal parameters, or equivalent
hashed representation.</li>
<li><strong>ML4.1b</strong> The value of the loss function at each point</li>
<li><strong>ML4.1c</strong> Information used to advance to next point, for example
quantification of local gradient.</li>
</ul></li>
<li><strong>ML4.2</strong> The subsequent extraction of information retained according to the
preceding standard should be explicitly documented, including through example
code.</li>
</ul>
<div id="batch-processing" class="section level4">
<h4><span class="header-section-number">5.8.4.1</span> Batch Processing</h4>
<p>The following standards apply to ML software which implements batch processing,
commonly to train models on data sets too large to be loaded in their entirety
into memory.</p>
<ul>
<li><strong>ML4.3</strong> All parameters controlling batch processing and associated
terminology should be explicitly documented, and it should not, for example,
be presumed that users will understand the definition of “epoch” as
implemented in any particular ML software.</li>
</ul>
<p>According to that standard, it would for example be inappropriate to have
a parameter, <code>nepochs</code>, described as “Number of epochs used in model training”.
Rather, the definition and particular implementation of “epoch” must be
explicitly defined.</p>
<ul>
<li><strong>ML4.4</strong> Explicit guidance should be provided on selection of appropriate
values for parameter controlling batch processing, for example, on trade-offs
between batch sizes and numbers of epochs (with both terms provided as
<em>Control Parameters</em> in accordance with the preceding standard, <strong>ML3</strong>).</li>
<li><strong>ML4.5</strong> ML software may optionally include a function to estimate likely
time to train a specified model, through estimating initial timings from
a small sample of the full batch.</li>
<li><strong>ML4.6</strong> ML software should by default provide explicit information on the
progress of batch jobs (even where those jobs may be implemented in parallel
on GPUs). That information may be optionally suppressed through additional
parameters.</li>
</ul>
</div>
<div id="re-sampling" class="section level4">
<h4><span class="header-section-number">5.8.4.2</span> Re-sampling</h4>
<p>As described at the outset, ML software does not always rely on pre-specified
and categorical distinctions between training and test data. For example,
models may be fit to what is effectively one single data set in which specified
cases or rows are used as training data, and the remainder as test data.
Re-sampling generally refers to the practice of re-defining categorical
distinctions between training and test data. One training run accordingly
connotes training a model on one particular set of training data and then
applying that model to the specified set of test data. Re-sampling starts that
process anew, through constructing an alternative categorical partition between
test and training data.</p>
<p>Even where test and training data are distinguished by more than a simple
data-internal category (such as a labelling column), for example,
by being stored in distinctly-named sub-directories, re-sampling may be
implemented by effectively shuffling data between training and test
sub-directories.</p>
<ul>
<li><strong>ML4.7</strong> ML software should provide an ability to combine results from
multiple re-sampling iterations using a single parameter specifying numbers
of iterations.</li>
<li><strong>ML4.8</strong> Absent any additional specification, re-sampling algorithms should
by default partition data according to proportions of original test and
training data.
<ul>
<li><strong>ML4.8a</strong> Re-sampling routines of ML software should nevertheless offer
an ability to explicitly control or override such default proportions of
test and training data.</li>
</ul></li>
</ul>
</div>
</div>
<div id="model-output-and-performance" class="section level3">
<h3><span class="header-section-number">5.8.5</span> Model Output and Performance</h3>
<p>Model output is considered here as a stage distinct from model performance.
Model output refers to the end result of model training (<strong>ML4</strong>), while model
performance involves the assessment of a trained model against a test data set.
The present section first describes standards for model output, which are
standards guiding the form of a model trained according to the preceding
standards (<strong>ML4</strong>). Model Performance is then considered as a separate stage.</p>
<div id="model-output" class="section level4">
<h4><span class="header-section-number">5.8.5.1</span> Model Output</h4>
<ul>
<li><strong>ML5.0</strong> The result of applying the training processes described above
should be contained within a single model object returned by the function
defined according to <strong>ML4.0</strong>, above. Even where the output reflects
application to a test data set, the resultant object need not include any
information on model performance (see <strong>ML5.3</strong>–<strong>ML5.4</strong>, below).
<ul>
<li><strong>ML5.0a</strong> That object should either have its own class, or extend some
previously-defined class.</li>
<li><strong>ML5.0b</strong> That class should have a defined <code>print</code> method which
summarises important aspects of the model object, including but not
limited to summaries of input data and algorithmic control parameters.</li>
</ul></li>
<li><strong>ML5.1</strong> As for the untrained model objects produced according to the above
standards, and in particular as a direct extension of <strong>ML3.3</strong>, the
properties and behaviours of trained models produced by ML software should be
explicitly compared with equivalent objects produced by other ML software.
(Such comparison will generally be done in terms of comparing model
performance, as described in the following standard <strong>ML5.3</strong>–<strong>ML5.4</strong>).</li>
<li><strong>ML5.2</strong> The structure and functionality of objects representing trained ML
models should be thoroughly documented. In particular,
<ul>
<li><strong>ML5.2a</strong> Either all functionality extending from the class of model
object should be explicitly documented, or a method for listing or
otherwise accessing all associated functionality explicitly documented
and demonstrated in example code.</li>
<li><strong>ML5.2b</strong> Documentation should include examples of how to save and
re-load trained model objects for their re-use in accordance with
<strong>ML3.1</strong>, above.</li>
<li><strong>ML5.2c</strong> Where general functions for saving or serializing objects,
such as
<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/readRDS.html"><code>saveRDS</code></a>
are not appropriate for storing local copies of trained models, an
explicit function should be provided for that purpose, and should be
demonstrated with example code.</li>
</ul></li>
</ul>
<p>The <a href="https://r6.r-lib.org"><code>R6</code> system</a> for representing classes in R is an
example of a system with explicit functionality, all components of which are
accessible by a simple
<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/ls.html"><code>ls()</code></a> call.
Adherence to <strong>ML5.2a</strong> would nevertheless
require explicit description of the ability of
<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/ls.html"><code>ls()</code></a> to
supply a list of all functions associated with an object. The <a href="https://github.com/mlr-org/mlr3"><code>mlr</code>
package</a>, for example, uses <a href="https://r6.r-lib.org"><code>R6</code>
classes</a>, yet neither explicitly describes the use of
<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/ls.html"><code>ls()</code></a> to
list all associated functions, nor explicitly lists those functions.</p>
</div>
<div id="model-performance" class="section level4">
<h4><span class="header-section-number">5.8.5.2</span> Model Performance</h4>
<p>Model performance refers to the quantitative assessment of a trained model when
applied to a set of test data.</p>
<ul>
<li><strong>ML5.3</strong> Assessment of model performance should be implemented as one or
more functions distinct from model training.</li>
<li><strong>ML5.4</strong> Model performance should be able to be assessed according to
a variety of metrics.
<ul>
<li><strong>ML5.4a</strong> All model performance metrics represented by functions
internal to a package must be clearly and distinctly documented.</li>
<li><strong>ML5.4b</strong> It should be possible to submit custom metrics to a model
assessment function, and the ability to do so should be clearly
documented including through example code.</li>
</ul></li>
</ul>
<p>The remaining sub-sections specify general standards beyond the preceding
workflow-specific ones.</p>
</div>
</div>
<div id="documentation-3" class="section level3">
<h3><span class="header-section-number">5.8.6</span> Documentation</h3>
<ul>
<li><strong>ML6.0</strong> Descriptions of ML software should make explicit reference to
a workflow which separates training and testing stages, and which clearly
indicates a need for distinct training and test data sets.</li>
</ul>
<p>The following standard applies to packages which are intended or other able to
only encompass a restricted subset of the six primary workflow steps enumerated
at the outset. Envisioned here are packages explicitly intended to aid one
particular aspect of the general workflow envisioned here, such as
implementations of ML optimization functions, or specific loss measures.</p>
<ul>
<li><strong>ML6.1</strong> ML software intentionally designed to address only a restricted
subset of the workflow described here should clearly document how it can be
embedded within a typical <em>full</em> ML workflow in the sense considered here.
<ul>
<li><strong>ML6.1</strong> Such demonstrations should include and contrast embedding
within a full workflow using at least two other packages to implement
that workflow.</li>
</ul></li>
</ul>
</div>
<div id="testing-3" class="section level3">
<h3><span class="header-section-number">5.8.7</span> Testing</h3>
<div id="input-data-2" class="section level4">
<h4><span class="header-section-number">5.8.7.1</span> Input Data</h4>
<ul>
<li><strong>ML7.0</strong> Test should explicitly confirm partial and case-insensitive
matching of “test”, “train”, and, where applicable, “validation” data.</li>
<li><strong>ML7.1</strong> Tests should demonstrate effects of different numeric scaling of
input data (see <strong>ML2.2</strong>).</li>
<li><strong>ML7.2</strong> For software which imputes missing data, tests should compare
internal imputation with explicit code which directly implements imputation
steps (even where such imputation is a single-step implemented via some
external package). These tests serve as an explicit reference for how
imputation is performed.</li>
</ul>
</div>
<div id="model-classes" class="section level4">
<h4><span class="header-section-number">5.8.7.2</span> Model Classes</h4>
<p>The following standard applies to models in both untrained and trained forms,
considered to be the respective outputs of the preceding standards <strong>ML3</strong> and
<strong>ML4</strong>.</p>
<ul>
<li><strong>ML7.3</strong> Where model objects are implemented as distinct classes, tests
should explicitly compare the functionality of these classes with
functionality of equivalent classes for ML model objects from other packages.
<ul>
<li><strong>ML7.3a</strong> These tests should explicitly identify restrictions on the
functionality of model objects in comparison with those of other
packages.</li>
<li><strong>ML7.3b</strong> These tests should explicitly identify functional advantages
and unique abilities of the model objects in comparison with those of
other packages.</li>
</ul></li>
</ul>
</div>
<div id="model-training-1" class="section level4">
<h4><span class="header-section-number">5.8.7.3</span> Model Training</h4>
<ul>
<li><strong>ML7.4</strong> ML software should explicit document the effects of different
training rates, and in particular should demonstrate divergence from optima
with inappropriate training rates.</li>
<li><strong>ML7.5</strong> ML software which implements routines to determine optimal training
rates (see <strong>ML3.4</strong>, above) should implement tests to confirm the optimality
of resultant values.</li>
<li><strong>ML7.6</strong> ML software which implement independent training “epochs” should
demonstrate in tests the effects of lesser versus greater numbers of epochs.</li>
<li><strong>ML7.7</strong> ML software should explicitly test different optimization
algorithms, even where software is intended to implement one specific
algorithm.</li>
<li><strong>ML7.8</strong> ML software should explicitly test different loss functions, even
where software is intended to implement one specific measure of loss.</li>
<li><strong>ML7.9</strong> Tests should explicitly compare all possible combinations in
categorical differences in model architecture, such as different model
architectures with same optimization algorithms, same model architectures
with different optimization algorithms, and differences in both.
<ul>
<li><strong>ML7.9a</strong> Such combinations will generally be formed from multiple
categorical factors, for which explicit use of functions such as
<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/expand.grid.html"><code>expand.grid()</code></a>
is recommended.</li>
</ul></li>
</ul>
<p>The following example illustrates:</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="standards.html#cb48-1"></a>architechture &lt;-<span class="st"> </span><span class="kw">c</span> (<span class="st">&quot;archA&quot;</span>, <span class="st">&quot;archB&quot;</span>)</span>
<span id="cb48-2"><a href="standards.html#cb48-2"></a>optimizers &lt;-<span class="st"> </span><span class="kw">c</span> (<span class="st">&quot;optA&quot;</span>, <span class="st">&quot;optB&quot;</span>, <span class="st">&quot;optC&quot;</span>)</span>
<span id="cb48-3"><a href="standards.html#cb48-3"></a>cost_fns &lt;-<span class="st"> </span><span class="kw">c</span> (<span class="st">&quot;costA&quot;</span>, <span class="st">&quot;costB&quot;</span>, <span class="st">&quot;costC&quot;</span>)</span>
<span id="cb48-4"><a href="standards.html#cb48-4"></a><span class="kw">expand.grid</span> (architechture, optimizers, cost_fns)</span></code></pre></div>
<pre><code>##     Var1 Var2  Var3
## 1  archA optA costA
## 2  archB optA costA
## 3  archA optB costA
## 4  archB optB costA
## 5  archA optC costA
## 6  archB optC costA
## 7  archA optA costB
## 8  archB optA costB
## 9  archA optB costB
## 10 archB optB costB
## 11 archA optC costB
## 12 archB optC costB
## 13 archA optA costC
## 14 archB optA costC
## 15 archA optB costC
## 16 archB optB costC
## 17 archA optC costC
## 18 archB optC costC</code></pre>
<p>All possible combinations of these categorical parameters could then be tested
by iterating over the rows of that output.</p>
<ul>
<li><strong>ML7.10</strong> The successful extraction of information on paths taken by
optimizers (see <strong>ML5.1</strong>, above), should be tested, including testing the
general properties, but not necessarily actual values of, such data.</li>
</ul>
</div>
<div id="model-performance-1" class="section level4">
<h4><span class="header-section-number">5.8.7.4</span> Model Performance</h4>
<ul>
<li><strong>ML7.11</strong> All performance metrics available for a given class of trained
model should be thoroughly tested and compared.
<ul>
<li><strong>ML7.11a</strong> Tests which compare metrics should do so over a range of
inputs (generally implying differently trained models) to demonstrate
relative advantages and disadvantages of different metrics.</li>
</ul></li>
</ul>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="scope.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="assessment.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": true,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/ropenscilabs/statistical-software-review-book/edit/master/standards.Rmd",
"text": "Edit this chapter"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "fixed"
},
"search": false,
"info": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
