[["index.html", "rOpenSci Statistical Software Peer Review Chapter 1 Welcome 1.1 Contributors 1.2 Content 1.3 Issue Authors", " rOpenSci Statistical Software Peer Review Mark Padgham and Noam Ross 2020-10-13 Chapter 1 Welcome Welcome to rOpenSci’s project developing software peer review system for statistical software in R packages and beyond. We invite you to join us in developing standards for processes of assessment and peer review of statistical software. rOpenSci currently runs software peer review for R packages focused on data life cycle management. To apply these processes to software implementing statistical methods and algorithms, we need new ways of evaluating and testing software, and managing the review process. This book serves as a home for collecting research and developing those standards. As the book develops, it will become an extension of rOpenSci Packages: Development, Maintenance, and Peer Review, documenting not only our guidelines for statistical software but also the process of expanding the scope of review so it may be reproduced in other domains. You are invited to contribute to this project by filing suggestions as issues in the book’s GitHub repository. We also have a dedicated area for discussion on the rOpenSci forum. We have noted in the chapters we’re actively seeking more input and feedback by highlighting them their titles in red and adding [SEEKING FEEDBACK] to them. Feedback on other sections is also welcome, although these may be at various stages of development. For this project we are lucky to have support of a great steering committee. The committee’s role is to help us with outreach to statistical communities, vetting community input and prioritizing our work. They are: Ben Bolker (@bolkerb) McMaster University Rebecca Killick, Lancaster University Stephanie Hicks (@stephaniehicks), Johns Hopkins University Max Kuhn (@topepos), RStudio Paula Moraga, King Abdullah University of Science and Technology, Saudi Arabia Leonardo Collado-Torres, Lieber Institute for Brain Development, USA This work is supported by the Sloan Foundation and is organized under an R Consortium Working Group. 1.1 Contributors This project uses the allcontributor package following the all-contributors specification. Contributions of any kind are welcome! 1.2 Content mpadge noamross maelle richfitz elong0527 Pakillo briatte 1.3 Issue Authors elimillera IndrajeetPatil "],["overview.html", "Chapter 2 Project Overview 2.1 Project Aims 2.2 Related projects and initiatives 2.3 Outline of this document 2.4 Community", " Chapter 2 Project Overview This chapter lays out key considerations, outstanding questions and tasks for purposes of generating community feedback for the project. It consists of the following sections: Scope of Statistical Software Review in which we address the scope of the project, and scopes of definition of “statistical software”. Standards for Statistical Software in which we consider the kinds of standards which might be developed and applied to assess statistical software. Software Assessment in which we provide a partial list of attributes and measures of software which might be usefully considered. Statistical Software Peer Review Process in which we consider questions regarding the possible forms and practices a peer review process might adopt. Each of these sections aims to highlight what we consider some of the most important questions and issues which the project will have to address. We generally do so by posing a list of “Proposals” at the end of each of the above-listed chapters. Each of these is intended to serve as a starting point for debate, and should be considered entirely open to discussion, modification, enhancement, removal, or any other suggestion which may arise. The Proposals are primarily intended in current form to provide focal points for further consideration and discussion prior to any concrete implementation. 2.1 Project Aims To foster a community of practice in which users and developers of statistical software mutually improve quality, reproducibility, and reliability of research. To provide software creators with a set of tools and standards to assess the quality of their work, and processes by which to improve it. To provide statistical software developers, users, and consumers of results a discoverable “badge” that transparently conveys a level of assurance of software quality and that may also serve as a symbol of professional credit. To create a set of standards that may be adopted and adapted by open source and private groups, academic journals, or other statistical software evaluation projects. To focus on R as primary language, but also to separate language-specific from language-agnostic components so as to maximize adaptability to other contexts. To focus on problems and solutions specific to statistical software. 2.2 Related projects and initiatives The following internal and external projects related projects have (non-exclusive) bearing on our work: rOpenSci is simultaneously working on improving the automation and documentation of infrastructure to support software peer reviews. This will support many of the processes described herein, initially those in metrics and diagnostic reports (below), as well as in managing the review process. Secondly, under a separate project funded by the Moore Foundation, rOpenSci is building a system to automate the retrieval of information related to use of software in published literature and automate reporting of software impact as part of metadata in software repositories. This builds on Depsy and CiteAs projects and may be leveraged for our work on metrics (below). Third, an initiative organized under the R Consortium, the R Validation Hub, seeks to provide guidance primarily to R users in the pharmaceutical industry on validating R packages in regulated contexts. We are working closely with that group to share products and avoid duplicated efforts. 2.3 Outline of this document We now briefly summarise the four sections identified at the outset of this chapter, and which comprise Chapters 4–5 and 7–8 of this book: Scope; the Peer Review Process; Standards; and Software Assessment. 2.3.1 Scope of Statistical Software Review The scope of statistical software review considers the core task of defining the kinds of software that will be covered by our review process and standards, for which key questions are: What categories of statistical software might be considered in scope? What categories of statistical software might be considered out of scope? How would these vary between a useful definition and typology for general use, and the specific case of our R-focused peer review system? A key consideration in scope is identifying categories of software that (a) will benefit from our peer review process, and (b) the review process will be equipped to handle. That is, can standards and procedures be defined that are applicable, and will authors and reviewers be able to apply them? In considering these issues of definition, it will be important to consider whether it may be advantageous or necessary to develop different procedures for different sub-categories, whether in terms of categories of external form or categories of statistical software. It will likely be particularly important for the present project to develop a categorization scheme, particularly because the set of standards envisioned by this project will be variably applicable to different categories, and understanding which standards may or may not apply to a particular piece of software will provide important information for review purposes. Individual pieces of software will often fit more than one of these categories, and we envision relating some kind of categorical checklist directly to a corresponding checklist of relevant or applicable standards. Potential schemes for defining the scope of statistical software acceptable within our review process are considered in Chapter 4 below, with extensive consideration of potential categories in the second section of that chapter. 2.3.2 Standards for Statistical Software Chapter 5 considers Standards for Statistical Software intended to serve both as expectations against which to compare software, and as guides which reviewers may use to assess software. Important general questions regarding standards include the following: What kind of standards might apply to software in general? What kind of standards might specifically apply to statistical software? How might such standards differ between different languages? To what extent should we aim for “verification” or “validation” of software, and how might be signify such? 2.3.3 Software Assessment The Software Assessment section in Chapter 6 presents a general (yet non-exhaustive) overview of aspects of software which may be usefully considered for standards-based assessment, both for retrospective purposes of peer review, and for prospective use in developing software both in general, and in preparation for peer-review. 2.3.4 Statistical Software Peer Review Process Our point of departure for our process is the rOpenSci software peer review process, which has operated for five years, and has reviewed over &gt;200 packages, primarily in areas of data life cycle management. However, we aim to reassess this process in light of other models and needs specific to statistical software. Chapter 7 considers a few analogous processes of peer review, gleaning aspects which may be useful to adopt and adapt for our system. Chapter 8 then describes the peer review process in terms of the series of steps we currently envision comprising our system. 2.4 Community A core goal of the project is the building and maintenance of a community of practice that will facilitate dissemination, adoption, and improvement of standards and peer review. In striving for this goal, the following questions are important: What outreach should we conduct to maximize diversity and inclusion in the process? How should this process involve other relevant communities in fields including software development, statistics, applied statistics (in various subfields) What fora should we manage for developers, users, reviewers and editors to communicate? To what extent should we reuse existing fora from rOpenSci or other organizations? We now briefly consider the three aspects of community relevant to this project: communities of users, of developers, and of reviewers. Note that several of the kinds of “metrics” alluded to in the following lines are given explicit consideration at the end of this document. Software use and surrounding community: What sort of metrics might provide insight into community use of software? How might such community engagement be enhanced to improve such metrics? Software development and surrounding community: What sort of metrics might provide insight into community development of software? How might such community engagement be enhanced to improve such metrics? Reviewer pool and qualifications: What is the extent and type of effort expected of reviewers? To what extent might searches for suitable reviewers be automated? What sort of metrics might be useful in such searches? In each case the project will strive to cultivate diverse, inclusive, and geographically expansive communities, and metrics to describe such aspects may also be important, as may automated tools to monitor community engagement and development. Note that these aspects of community are not explicitly addressed throughout any of the remainder of this document. It is important the future revisions return to this point, and ensure that each of the following sections are appropriately modified to ensure effective consideration and incorporation of the concerns listed immediately above. "],["reading.html", "Chapter 3 Some Light Reading: An Annotated Bibliography 3.1 Books 3.2 Journal Articles 3.3 Technical Reports 3.4 Computer Programs 3.5 Web Pages 3.6 Contributing to the bibliography", " Chapter 3 Some Light Reading: An Annotated Bibliography Below is an annotated bibliography of sources that have been useful in developing this document and the statistical software peer-review standards. Note this page is auto-generated from a Zotero collection. See Contributing to the Bibliography below to add more sources. 3.1 Books There is an enormous wealth of books published on software review, software assessment (often referred to via concepts of “validation” and/or “verification”), and related concepts. Most importantly in the present context, almost all such books address closed-source, proprietary software, with no published books specifically dedicated to review or assessment of open-source software. Software testing automation tips: 50 things automation engineers should know. Alpaev, Gennadiy ( 2017 ). A neat list of just what the title suggests, grouped into general topics of Scripting, Testing, Environment, Running Logging Verifying, and Reviewing. Introduction to software testing. Ammann, Paul; and Offutt, Jeff ( 2017 ). Has useful definitions of Verification as “The process of determining whether the products of a phase of the software development process fulfill the requirements established during the previous phase;” and  Validation as “The process of evaluating software at theend of software development to ensure compliance with intended usage.”   Also includes extensive consideration of testing, in particular chapters on “Model-driven test design” (2), and “Writing Test Plans” (11).   Software Verification and Validation: An Engineering and Scientific Approach. Fisher, Marcus S. ( 2007 ). Really useful considerations of risk as likelihood times consequence, with extensive worked examples of how these factors might be scaled (with most examples on simple four-point scales which prove to be sufficient to yield fruitful insight).   Transparent Statistics Guidelines. Group (http://transparentstatistics.org/), Transparent Statistics in HCI Working ( 2019 ). Currently only just begun, but aims to work towards what might be a very useful guide on how statistics might best be reported. Software Quality Approaches: Testing, Verification, and Validation: Software Best Practice 1. Haug, Michael; Olsen, Eric W; and Consolini, Luisa ( 2001 ). A huge EU project, drawn from results of a host of “Process Improvement Experiments” (PIEs), all of which were controlled experiments built on a generic model, and involved manipulating some standard baseline practice and analysing effects.   Software testing: concepts and operations. Mili, Ali ( 2015 ). Describes some problems facing modern software development (p9), including “Absence of Reuse Practice”, and lack of standard software architecture. Neither of these are applicable to R packages. Then goes on to describe (p11) “The Absence of Automation”, and “Limited Quality Control”, both of which are definitely still applicable. The art of software testing. Myers, Glenford J; Badgett, Tom; and Sandler, Corey ( 2012 ). Glenford Myers originally published “The Art of Software Testing” in 1979. Most of the book is either pretty straightforward, or overly specific to be able to be directly transferred or translated to open source software.   Medical device software verification, validation and compliance. Vogel, David A ( 2011 ). In spite of the applied focus, a highly relevant and insightful book. Of particular relevance are the extensive considerations of software lifecycles, verification and validation and associated metrics, and considerations of software testing in relation to the US FDA’s General Principles of Software Validation.0 The tidyverse style guide. Wickham, Hadley ( 2020 ). “All style guides are fundamentally opinionated. Some decisions genuinely do make code easier to use (especially matching indenting to programming structure), but many decisions are arbitrary. The most important thing about a style guide is that it provides consistency, making code easier to write because you need to make fewer decisions.” 3.2 Journal Articles Reconciling modern machine learning practice and the bias-variance trade-off. Belkin, Mikhail; Hsu, Daniel; Ma, Siyuan; and Mandal, Soumik ( 2019-09-10 ). arXiv:1812.11118 [cs, stat] A very good reference for the ubiquity of over-fitting in machine learning algorithms, which is nevertheless mostly dedicated to demonstrating a very practical approach to overcoming over-fitting. Datasheets for Datasets. Gebru, Timnit; Morgenstern, Jamie; Vecchione, Briana; Vaughan, Jennifer Wortman; Wallach, Hanna; Daumeé III, Hal; and Crawford, Kate ( 2019-04-14 ). arXiv:1803.09010 [cs] Comment: Working Paper, comments are encouraged Four simple recommendations to encourage best practices in research software. Jiménez, Rafael C.; Kuzak, Mateusz; Alhamdoosh, Monther; Barker, Michelle; Batut, Bérénice; Borg, Mikael; Capella-Gutierrez, Salvador; Chue Hong, Neil; Cook, Martin; Corpas, Manuel; Flannery, Madison; Garcia, Leyla; Gelpí, Josep Ll.; Gladman, Simon; Goble, Carole; González Ferreiro, Montserrat; Gonzalez-Beltran, Alejandra; Griffin, Philippa C.; Grüning, Björn; Hagberg, Jonas; Holub, Petr; Hooft, Rob; Ison, Jon; Katz, Daniel S.; Leskošek, Brane; López Gómez, Federico; Oliveira, Luis J.; Mellor, David; Mosbergen, Rowland; Mulder, Nicola; Perez-Riverol, Yasset; Pergl, Robert; Pichler, Horst; Pope, Bernard; Sanz, Ferran; Schneider, Maria V.; Stodden, Victoria; Suchecki, Radosław; Svobodová Vařeková, Radka; Talvik, Harry-Anton; Todorov, Ilian; Treloar, Andrew; Tyagi, Sonika; van Gompel, Maarten; Vaughan, Daniel; Via, Allegra; Wang, Xiaochuan; Watson-Haigh, Nathan S.; and Crouch, Steve ( 2017-6-13 ). F1000Research: 876 The four recommendations are: Make source code publicly accessible from day one Make software easy to discover by providing software metadata via a popular community registry Adopt a licence and comply with the licence of third-party dependencies Define clear and transparent contribution, governance, and communication processes Towards FAIR principles for research software. Lamprecht, Anna-Lena; Garcia, Leyla; Kuzak, Mateusz; Martinez, Carlos; Arcila, Ricardo; Martin Del Pico, Eva; Dominguez Del Angel, Victoria; van de Sandt, Stephanie; Ison, Jon; Martinez, Paula Andrea; McQuilton, Peter; Valencia, Alfonso; Harrow, Jennifer; Psomopoulos, Fotis; Gelpi, Josep Ll; Chue Hong, Neil; Goble, Carole; and Capella-Gutierrez, Salvador ( 2019/01/01 ). Data Science (Preprint): 1-23 An important reference for adapting FAIR (Findability, Accessibility, Interoperability, and Reusability) principles to software. All of these principles are effectively already met by rOpenSci’s current review system, noting in particular that the majority of them pertain to the creation, existence, and curation of metadata. Data Management Lifecycle and Software Lifecycle Management in the Context of Conducting Science. Lenhardt, W.; Ahalt, Stanley; Blanton, Brian; Christopherson, Laura; and Idaszak, Ray ( 2014-07-09 ). Journal of Open Research Software (1): e15 A relatively brief exploration of parallels between relatively well-established notions of lifecycle in regard to data management and curation, and less-established considerations of lifecycle in regard to software. Primarily an argument curation of appropriate metadata, it makes frequent reference to a variety of ISO standards. 3.3 Technical Reports Software Evaluation Guide. Jackson, Mike; Crouch, Steve; and Baxter, Rob ( 2011 ). Divides considerations into the following categories and sub-categories:   Usability     - 1.1 Understandability     - 1.2 Documentation     - 1.3 Learnability     - 1.4 Buildability     - 1.5 Installability     - 1.6 Performance Sustainability &amp; Maintainability     - 2.1 Identity     - 2.2 Copyright &amp; Licensing     - 2.3 Accessibility     - 2.4 Community     - 2.5 Testability     - 2.6 Portability     - 2.7 Supportability     - 2.8 Analysability     - 2.9 Changeability     - 2.10 Reusability     - 2.11 Security &amp; Privacy     - 2.12 Interoperability     - 2.13 Governance   The main document divides each of these into numerous explicit categories, with the resultant document serving as likely a useful comparison or contrast to the Core Infrastructure Best Practices checklist. Automated Source Code CISQ Maintainability Measure Specification Version 1.0. Object Management Group ( 2016 ). Considers a number of very specific metrics, including the following: Control Flow Transfer Control Element outside Switch Block Class Element ExcessiveInheritance of Class Elements with Concrete Implementation Storable and Member DataElement Initialization with Hard-Coded Literals Callable and Method ControlElement Number of Outward Calls Loop Value Update within the Loop Commented-out Code Element Excessive Volume (with default threshold of 2%) Inter-Module Dependency Cycles Source Element Excessive Size (with default of 1,000 lines per file) Horizontal Layer Excessive Number (with default of 8) Named Callable and Method Control Element Multi-Layer Span (“Avoid unclear allocation of software elements to a single architectural layer”) Callable and Method Control Element Excessive Cyclomatic Complexity Value (with default threshold of 20). Named Callable and Method Control Element with Layer-Skipping Call (Aim: Avoid calls from an upper layer to lower layers that are not adjacent). Callable and Method Control Element Excessive Number of Parameters (with default threshold of 7) Callable and Method Control Element Excessive Number of Control Elements Involving Data Element from Data Manager or File Resource (Aim: Reduce numbers of external data dependencies, with default threshold of 7). Public Member Element Method Control Element Usage of Member Element from other Class Element Class Element Excessive Inheritance Level (with default threshold of 7) Class Element Excessive Number of Children (with default threshold of 10) Named Callable and Method Control Element Excessive Similarity (based on assessment of numbers of identical compiled tokens within a unit) Unreachable Named Callable or Method Control Element (Aim: Avoid inactive code blocks) ESIP Software Guidelines Draft. Scott, Soren ( 2016 ). Earth Science Information Partners (ESIP) guidelines, divided among the primary categories of Sustainable Code     1.1 Clean, standardized code     1.2 Versioned code     1.3 Redistributable     1.4 Tested Interoperable Usable     3.1 Clear, understandable interface     3.2 Performant and stable Documented     4.1 Source code is documented     4.2 Codebase includes documentation to support adaptation and reuse     4.3 Software is documented     4.4 Code or software requirements are documented     4.5 Project is documented Secure Shareable Governed     7.1 Contribution policies are provided     7.2 Development activities are transparent Progression, sustainability, and reusability/adoption CLARIAH/software-quality-guidelines. van Gompel, Maarten; Noordzij, Jauco; de Valk, Reinier; and Scharnhorst, Andrea ( 2016 ). Divides considerations into the categories and sub-categories derived by the Software Sustainability Institute in 2011. Each of these has several explicit sub-components, with each of them scored on a 5-point scale. The entire document provides an outstandingly strong and usable reference or benchmark from which a set of standards for software quality might be adapted. 3.4 Computer Programs lifecycle: Manage the Life Cycle of your Package Functions. Henry, Lionel; and RStudio, ( 2020-03-06 ). A good potential candidate for recommended ways to manage software lifecycles RWsearch: Lazy Search in R Packages, Task Views, CRAN, the Web. All-in-One Download. Kiener, Patrice ( 2020-02-15 ). May be useful for it’s ability to download and search package documentation. microsoft/nni. Microsoft ( 2020-03-16T12:25:52Z ). Potentially useful as a benchmarking tool for ML implementations based on Neural Networks (see also uber/ludwig) The Turing Way: A Handbook for Reproducible Data Science. The Turing Way Community, ; Becky Arnold, ; Louise Bowler, ; Sarah Gibson, ; Patricia Herterich, ; Rosie Higman, ; Anna Krystalli, ; Alexander Morley, ; Martin O’Reilly, ; and Kirstie Whitaker, ( 2019-03-25 ). A useful reference for various aspects of reproducible science uber/ludwig. uber ( 2020-03-16T10:25:42Z ). Potentially useful as a benchmarking tool for ML implementations (see also microsoft/nni) 3.4.1 Computer Programs (Testing) Software specifically designed for testing. mikldk/roxytest. Andersen, Mikkel Meyer ( 2020-03-04T21:43:17Z ). Very useful extension of R testing through enabling tests to be specified in-line via roxygen entries, either through explicit specification (via @test hooks), or through converting examples to tests (via @testexamples hooks). markvanderloo/tinytest. Loo, Mark van der ( 2020-03-04T15:20:41Z ). Implements the unique (in R) and useful approach of treating test results as data amenable to analysis, rather than just throwing interrupt signals. Most testing is ineffective - Hypothesis. MacIver, David ( ). “Hypothesis is a family of testing libraries which let you write tests parametrized by a source of examples. A Hypothesis implementation then generates simple and comprehensible examples that make your tests fail. This simplifies writing your tests and makes them more powerful at the same time, by letting software automate the boring bits and do them to a higher standard than a human would, freeing you to focus on the higher level test logic.” LudvigOlsen/xpectr. Olsen, Ludvig Renbo ( 2020-03-02T09:31:14Z ). R package for generating expectations for testthat unit testing. The main utility is via functions which automatically generate tests appropriate to the input structure of functions. r-lib/waldo. Wickham, Hadley ( 2020-03-30T13:36:53Z ). Uses diffobj to create and return inline diffs between objects. Likely to be particularly useful in tests. 3.5 Web Pages Jakob Bossek - PhD candidate. Bossek, Jakob ( ). List of software packages developed by Jakob Bossek, most of which are devoted to providing benchmarking abilities for comparison of graphs and graph algorithms. coreinfrastructure/best-practices-badge. Core Infrastructure ( ). A definitive reference for general standards in open-source software, as detailed in the doc/criteria.md document. That said, it is a github-based document, and many of the standards directly describe github-based attributes and workflows.   Sections considered are: Basic Website License Documentation Other Must use https Must provide for discussion Must be in English Change Control Public version control repo Unique version numbering Semantic versioning Release notes Reporting Bug reporting Vulnerability reporting Quality Working build system Automated test suite (cover most branches, input fields, and functionality) Continuous integration New functionality testing Warning flags / linter Security Analysis Static code analysis (with links to lots of language-specific tools) Dynamic code analysis (with links to lots of language-specific tools) Use of memory check tools, or memory-safe languages testing statistical software. Hayes, Alex ( ). Discusses four types of tests for statistical software: Correctness Tests Parameter Recovery Tests Convergence Tests Identification Tests (uniqueness/stability of solution) CODECHECK process. Nüst, Stephen Eglen &amp; Daniel ( ). A badging service indicating whether or not code used to support published scientific manuscripts is reproducible or not. A Risk-based Approach for Assessing R package Accuracy within a Validated Infrastructure. R Validation HUb ( ). The White Paper from the R Validation Hub project for validation pharmaceutical software. “R Validation Hub is a cross-industry initiative whose mission is to enable the use of R by the Bio-Pharmaceutical Industry in a regulatory setting, where the output may be used in submissions to regulatory agencies.” It describes the motivations and principles of their risk-based assessment of statistical software, much of which is directly implemented in the riskmetric package. Pull Request review process - Reside-IC. Reside-IC ( ). A useful description of the pull request workflow developed by the Research Software for Infection Disease Epidemiology group at Imperial College, London. Shields.io: Quality metadata badges for open source projects. sheilds.io ( ). shields.io provides a badging service for many aspects of code, divided into the following general categories: Build Code coverage Analysis Chat Dependencies Size Downloads Funding Issue Tracking License Rating Social Version Platform &amp; Version Support Monitoring Activity PEP 8 – Style Guide for Python Code. van Rossum, Guido; Warsaw, Barry; and Coghlan, Nick ( ). The most widely used style guide for python code 3.6 Contributing to the bibliography This annotated bibliography is automatically generated from entries containing “Note” fields in the zotero library accompanying this project. Please feel free to add any additional entries you may see fit to this open library according to the following steps: Open your local zotero client and get a local copy of the project library; Enable zotero in your web browser so that you can click on any webpage to add an entry to the shared library; Manually add a “Note” to any new entries and they’ll automatically appear here the next time this page is updated. This annotated bibliography extracts all text in all Note fields up to the first markdown section break, itself defined by a single line with a least three consecutive dashes as in the following example: Add some note text to appear in this annotated bibliography --- This text below the three dashes and any subsequent lines will not appear here. Notes which do not contain a section break will appear here in their entirety. "],["scope.html", "Chapter 4 Scope 4.1 Software types 4.2 Statistical Categories – Background 4.3 Statistical Categories 4.4 Out Of Scope Categories 4.5 Proposals", " Chapter 4 Scope One task in extending the rOpenSci peer review system to statistical software is defining scope - what software is included or excluded. Defining scope requires some grouping of packages into categories. These categories play key roles in the peer review process and standards-setting. Categorical definitions can determine which kinds of software will be admitted; Different categories of software will be subject to different standards, so categories are key to developing standards, review guidance, and automated testing. Creating a categorization or ontology of statistical software can easily become an overwhelming project in itself. Here we attempt to derive categories or descriptors which are practically useful in the standards and review process, rather than a formally coherent system. We use a mix of empirical research on common groupings of software and subjective judgement as to their use in the review process. We consider two main types of categories: Categories of software structure, referred to as “software types”, determined by computer languages and package formats in those languages; and Categories defining different types of statistical software, referred to as “statistical categories”. 4.1 Software types 4.1.1 Languages This project extends an existing software peer-review process run by rOpenSci, and is primarily intended to target the R language. Nonetheless, given the popularity of Python in the field (see relevant analyses and notes in Appendix A), the impact of developing standards applicable to Python packages must be considered. rOpenSci also has a close collaboration with its sister organization, pyOpenSci. In addition it is particularly important to note that many R packages include code from a variety of other languages. The following table summarises statistics for the top ten languages from all 15,735 CRAN packages as of 13 Oct 2020 (including only code from the /R, /src, and /inst directories of each package). Table 4.1: Proportion of code lines in different languages in all CRAN packages. language lines proportion R 22,559,154 0.441 C/C++ Header 6,751,884 0.132 HTML 5,558,181 0.109 C 5,094,263 0.100 C++ 4,671,645 0.091 JavaScript 1,473,084 0.029 Fortran 77 822,194 0.016 JSON 762,253 0.015 CSS 638,341 0.012 Rmd 548,011 0.011 Close to one half of all code in all R packages to date has been written in the R language, clearly justifying a primary focus upon that language. Collating all possible ways of packaging and combining C and C++ code yields 16,517,792 lines or code or 32% of all code, indicating that 76% of all code has been written in either R or C/C++. Three of these top ten languages are likely related to web-based output (HTML, JavaScript, and CSS), representing a total of 15% of all code. While this is clearly a significant proportion, and while this may reflect an equivalent high frequency of code devoted to some form of web-based visualisation, these statistics represent all R packages. In many cases this represents extensive headers in supplementary documentation. There is no simple way to identify which of these might be considered statistical code in web-based languages, but knowing that there are packages exclusively constructed to generate web-based visualisations and documentation in a generic sense suggests that this value may be taken as an upper limit on the likely frequency of these types of visualisation packages (or parts thereof) in the context of statistical software. Key considerations: Expansion into the Python ecosystem has great potential for impact, but goes beyond the general areas of expertise in the core ecosystem. (And Python code represents just 157,814 lines of code, or 0.3% of all code within all R packages.) Compiled languages within R packages are core to many statistical applications; excluding them would exclude core functionality the project aims to addressed. The majority of compiled code is nevertheless C and/or C++, with Fortran representing under 2% of all code. Languages used for web-based visualisations comprise a significant proportion (15%) of all code. While this potentially indicates a likely importance of visualisation routines, this figure reflects general code in all R packages, and the corresponding proportion within the specific context of statistical software may be considerably lower. Any decision to include visualisation software and routines within our scope will likely entail an extension of linguistic scope to associated languages (HTML, JavaScript, and maybe CSS). 4.1.2 Structure R has a well-defined system for structuring software packages\" Other forms of packaging R software may nevertheless be considered within scope. These may include Python-like systems of modules for R; Packaging appropriate for other languages (such as Python) yet with some interface with the R language; R interfaces (“wrappers”) to algorithms or software developed independently in different languages, and which may or may not be bundled as a standard R package; and Web applications such as Shiny packages. Key considerations: Allowing non-package forms of code into the peer review system could potentially bring in a large pool of code typically published alongside scientific manuscripts, and web applications are a growing, new area of practice. However, there is far less standardization of code structure to allow for style guidelines and automated testing in these cases. 4.2 Statistical Categories – Background As alluded to at the outset of this chapter, a primary task of this project will be to categorise statistical software in order to: Determine the extent to which software fits within scope Enable fields of application of software to be readily identified Enable determination of applicable standards and assessment procedures Enable discernment of appropriate reviewers Any piece of statistical software need not necessarily be described by a single category, rather the categories proposed below are intended to serve as a checklist, with submitting authors ticking all applicable categories. A software submission will then be assessed with reference to the set of Standards and Assessment Procedures (S&amp;APs) corresponding to all categories describing that software. Our definition of categories is particularly guided by a need to develop applicable S&amp;APs. Each of the categories that follow has accordingly been proposed because it has been judged to reflect a domain within which S&amp;APs are likely to be both unique and important. These categories are not intended to reflect an attempt to define statistical software in general, rather an attempt to define areas in which S&amp;APs for statistical software may be productively developed and applied. We anticipate throughout our development of S&amp;APs that aspects will emerge which are common to several categories. It may be deemed to elevate such S&amp;APs to general procedures (largely) independent of specific categories. More generally, although we aim for category-specific S&amp;APs which are as independent of the other categories as possible, S&amp;APs in any one category will often be related to those from other categories, and co-development of procedures in multiple categories may be necessary. 4.2.1 Empirical Derivation of Categories We attempted to derive a realistic categorisation through using empirical data from several sources of potential software submissions, including all apparently “statistical” R packages published in the Journal of Open Source Software (JOSS), packages published in the Journal of Statistical Software, software presented at the 2018 and 2019 Joint Statistical Meetings (JSM), and Symposia on Data Science and Statistics (SDSS), well as CRAN task views. We have also compiled a list of the descriptions of all packages rejected by rOpenSci as being out of current scope because of current inability to consider statistical packages, along with a selection of recent statistical R packages accepted by JOSS. (The full list of all R package published by JOSS can be viewed at https://joss.theoj.org/papers//in/R). We allocated one or more key words (or phrases) to each abstract, and use the frequencies and inter-connections between these to inform the following categorisation are represented in the interactive graphic (also included in the Appendix), itself derived from analyses of abstracts from all statistical software submitted to both rOpenSci and JOSS. (Several additional analyses and graphical representations of these raw data are included an auxiliary github repository.) The primary nodes that emerge from these empirical analyses (with associated relative sizes in parentheses) are shown in the following table. Table 4.2: Most frequent key words from all JOSS abstracts (N = 92) for statistical software. Proportions are scaled per abstract, with each abstract generally having multiple key words, and so sum of proportions exceeds one. n term proportion 1 ML 0.133 2 statistical indices and scores 0.111 3 visualization 0.111 4 dimensionality reduction 0.100 5 probability distributions 0.100 6 regression 0.100 7 wrapper 0.100 8 estimates 0.089 9 Monte Carlo 0.089 10 Bayesian 0.078 11 categorical variables 0.078 12 EDA 0.078 13 networks 0.078 14 summary statistics 0.067 15 survival 0.067 16 workflow 0.067 ## `summarise()` ungrouping output (override with `.groups` argument) The top key words and their inter-relationships within the main network diagram were used to distinguish the following primary categories representing all terms which appear in over 5% of all abstracts, along with the two additional categories of “spatial” and “education”. We have excluded the key word “Estimates” as being too generic to usefully inform standards, and have also collected a few strongly-connected terms into single categories. Table 4.3: Proposed categorisation of statistical software, with corresponding proportions of all JOSS software matching each category n term proprtion comment 1 Bayesian &amp; Monte Carlo 0.167 2 dimensionality reduction &amp; feature selection 0.144 Commonly as a result of ML algorithms 3 ML 0.133 4 regression/splines/interpolation 0.133 Including function data analysis 5 statistical indices and scores 0.111 Software generally intended to produce specific indices or scores as statistical output 6 visualization 0.111 7 probability distributions 0.100 Including kernel densities, likelihood estimates and estimators, and sampling routines 8 wrapper 0.100 9 categorical variables 0.078 Including latent variables, and those output from ML algorithms. Note also that method for dimensionality reduction (such as clustering) often transform data to categorical forms. 10 Exploratory Data Analysis (EDA) 0.078 Including information statistics such as Akaike’s criterion, and techniques such as random forests. Often related to workflow software. 11 networks 0.078 12 summary statistics 0.067 Primarily related in the empirical data to regression and survival analyses, yet clearly a distinct category of its own. 13 survival 0.067 strongly related to EDA, yet differing in being strictly descriptive of software outputs whereas EDA may include routines to explore data inputs and other pre-output stages of analysis. 14 workflow 0.067 Often related to EDA, and very commonly also to ML. 15 spatial 0.033 Also an important intermediate node connecting several other nodes, yet defining its own distinct cluster reflecting a distinct area of expertise. 16 education 0.044 The full network diagram can then be reduced down to these categories only, with interconnections weighted by all first- and second-order interconnections between intermediate categories, to give the following, simplified diagram (in which “scores” denotes “statistical indices and scores”; with the diagram best inspected by dragging individual nodes to see their connections to others). ## `summarise()` regrouping output by &#39;from&#39; (override with `.groups` argument) ## `summarise()` regrouping output by &#39;from&#39; (override with `.groups` argument) ## `summarise()` ungrouping output (override with `.groups` argument) Standards considered under any of the ensuing categories must be developed with reference to inter-relationships between categories, and in particular to potential ambiguity within and between any categorisation. An example of such ambiguity, and of potential difficulties associated with categorisation, is the category of “network” software which appropriate describes the grapherator package (with accompanying JOSS paper) which is effectively a distribution generator for data represented in a particular format that happens to represent a graph; and three JSM presentations, one on network-based clustering of high-dimensional data, one on community structure in dynamic networks and one on Gaussian graphical models. Standards derived for network software must accommodate such diversity of applications, and must accommodate software for which the “network” category may pertain only to some relatively minor aspect, while the primary algorithms or routines may not be related to network software in any direct way. 4.2.2 Examples of Statistical Software We now consider a few brief categorical examples, to illustrate the kinds of decisions such a process of categorisation will likely face. gtsummary, submitted to rOpenSci and reject as out-of-scope. Creates presentation-ready tables summarizing data sets, regression models, and more. The code to create the tables is concise and highly customizable. Data frames can be summarized with any function, e.g. mean(), median(), even user-written functions. Regression models are summarized and include the reference rows for categorical variables. Common regression models, such as logistic regression and Cox proportional hazards regression, are automatically identified and the tables are pre-filled with appropriate column headers. This package appears not to contain any algorithmic implementations, yet is clearly aimed at enhancing a purely statistical workflow. Such a submission requires answering the question of whether software categorized as “workflow” only and which does not correspond to any other of the above categories, may be deemed in scope? greta: simple and scalable statistical modelling in R, published in JOSS. greta is an package for statistical modelling in R (R Core Team, 2019) that has three core differences to commonly used statistical modelling software packages: greta models are written interactively in R code rather than in a &gt; compiled domain specific language. greta can be extended by other R packages; providing a fully-featured &gt; package management system for extensions. greta performs statistical inference using TensorFlow (Abadi et al., &gt; 2015), enabling it to scale across modern high-performance computing &gt; systems. The greta package might be considered predominantly an interface to TensorFlow, yet it provides a new way to specify and work with purely statistical models. This might be considered under both workflow and wrapper categories, and serves here to illustrate the question of whether wrappers around, in this case, externally-installed software might be considered in scope? And if so, to what extent ought aspects of such externally-installed software also be directly addressed within a review process? modelStudio, published in JOSS. The modelStudio R package automates the process of model exploration. It generates advanced interactive and animated model explanations in the form of a serverless HTML site. It combines R(R Core Team, 2019) with D3.js (Bostock, 2016) to produce plots and descriptions for various local and global explanations. Tools for model exploration unite with tools for EDA to give a broad overview of the model behaviour. As with gtsummary above, this is clearly a package intended to enhance a workflow, and furthermore one which primarily serves to generate summary output as a html document, yet the models it considers, and all aspects of output produced, are purely statistical. This package could meet both workflow and visualization categories, and serves here to illustrate difficulties in considering the latter of these. The D3.js library contains numerous indubitably statistical routines, and so this package might be argued to be a wrapper in the same category as greta is a wrapper around TensorFlow. An important question likely to arise in considering both of these is the extent to which the library being wrapped should also be predominantly statistical for a package to be in scope? (A requirement which greta would more easily fulfil than gtsummary.) 4.3 Statistical Categories Based on the preceding categories, along with contributions via our discussion forum, we propose the following categories intended by define and guide the assessment of statistical software. These categories are intended to serve as checklist items, with each submission likely to check several categories. The categories are: Bayesian and Monte Carlo Routines Dimensionality Reduction, Clustering, and Unsupervised Learning Machine Learning Regression and Supervised Learning Probability Distributions Wrapper Packages Networks Exploratory Data Analysis (EDA) and Summary Statistics Workflow Support Spatial Analyses Time Series Analyses 4.3.1 Bayesian and Monte Carlo Routines Packages implementing or otherwise relying on Bayesian or Monte Carlo routines represent form the central “hub” of all categories in the above diagram, indicating that even though this category is roughly equally common to other categories, software in this category is more likely to share more other categories. In other words, this is the leading “hybrid” category within which standards for all other categories must also be kept in mind. Some examples of software in this category include: The bayestestR package “provides tools to describe … posterior distributions” The ArviZ package is a python package for exploratory analyses of Bayesian models, particularly posterior distributions. The GammaGompertzCR package features explicit diagnostics of MCMC convergence statistics. The BayesianNetwork package is in many ways a wrapper package primarily serving a shiny app, but also accordingly a package in both education and EDA categories. The fmcmc package is a “classic” MCMC package which directly provides its own implementation, and generates its own convergence statistics. The rsimsum package is a package to “summarise results from Monte Carlo simulation studies”. Many of the statistics generated by this package may prove useful in assessing and comparing Bayesian and Monte Carlo software in general. (See also the MCMCvis package, with more of a focus on visualisation.) The walkr package for “MCMC Sampling from Non-Negative Convex Polytopes” is indicative of the difficulties of deriving generally applicable assessments of software in this category, because MCMC sampling relies on fundamentally different inputs and outputs than many other MCMC routines. Key Considerations The extent to which the output of Bayesian routines with uninformative prior inputs can or do reflect equivalent frequentist analyses. Ways to standardise and compare diagnostic statistics for convergence of MCMC routines. Forms and structures of data using in these routines are very variable, likely making comparison among algorithms difficult. 4.3.2 Dimensionality Reduction, Clustering, and Unsupervised Learning Many packages either implement or rely upon techniques for dimensionality reduction or feature selection. One of the primary problems presented by such techniques is that they are constrained to yield a result independent on any measure of correctness of accuracy (Estivill-Castro 2002). This can make assessment of the accuracy or reliability of such routines difficult. Moreover, dimensionality reduction techniques are often developed for particular kinds of input data, reducing abilities to compare and contrast different implementations, as well as to compare them with any notional reference implementations. ivis implements a dimensionality reduction technique using a \"Siamese Neural Network architecture. tsfeaturex is a package to automate “time series feature extraction,” which also provides an example of a package for which both input and output data are generally incomparable with most other packages in this category. iRF is another example of a generally incomparable package within this category, here one for which the features extracted are the most distinct predictive features extracted from repeated iterations of random forest algorithms. compboost is a package for component-wise gradient boosting which may be sufficient general to potentially allow general application to problems addressed by several packages in this category. The iml package may offer usable functionality for devising general assessments of software within this category, through offering a “toolbox for making machine learning models interpretable” in a “model agnostic” way. Key Considerations It is often difficult to discern the accuracy of reliability of dimensionality reduction techniques. It is difficult to devise general routines to compare and assess different routines in this category, although possible starting points for the development of such may be offered by the compboost and iml packages. 4.3.3 Machine Learning Machine Learning (ML) routines play a central role in modern statistical analyses, and the ML node in the above diagram is roughly equally central, and equally connected, to the Bayesian and Monte Carlo node. Machine Learning algorithms represent perhaps some of the most difficult algorithms for which to develop standards and methods of comparison. Both input and output data can be categorically different or even incomparable, while even where these may be comparable, the abiding aims of different ML algorithms can differ sufficiently to make comparison of outputs to otherwise equivalent inputs largely meaningless. A few potentially fruitful routes towards productive comparison may nevertheless be discerned, here according to the sub-domains of input data, output data, and algorithms. Input Data One promising R package which may prove very useful for standardising and comparing data used as input to ML algorithms is the vtreat package that “prepares messy real world data for predictive modeling in a reproducible and statistically sound manner.” The routines in this package perform a series of tests for general sanity of input data, and may prove generally useful as part of a recommended ML workflow. Algorithms A number of packages attempt to offer unified interfaces to a variety of ML algorithms, and so may be used within the context of the present project either as potential recommended standards, or as ways by which different algorithms may be compared within a standard workflow. Foremost among such packages are mlr3, which represents one of the core R packages for ML, developed by the key developers of previous generations of ML software in R. It offers a modular and extensible interface for a range of ML routines, and may prove very useful in comparing different ML routines and implementations. Output Data There are several extant packages for (post-)processing data output from ML algorithms. Many, perhaps even most, of these primarily aim to derive insightful visualisations of output, whether in interactive (JavaScript-based) form, as with the modelStudio or modelDown packages, or more static plots using internal graphical routines from R, as in the iml (Interpretable Machine Learning) package. The latter package offers a host of additional functionality useful in interpreting the output of ML algorithms, and which may prove useful in general standards-based contexts. Potential “edge cases” which may be difficult to reconcile with the general aspects described above include the following: ReinforcementLearning is a simulation package employing ML routines to enable agents to learn through trial and error. It is an example of a package with inputs and outputs which may be difficult to compare with other ML software, and difficult to assess via general standards. BoltzMM is an implementation of a particular class of ML algorithms (“Boltmann Machines”), and so provides an obverse example to the above, for which in this case inputs and outputs may be compared in standard ways, yet the core algorithm may be difficult to compare. dml is a collection of different ML algorithms which perform the same task (“distance metric learning”). While comparing algorithms within the package is obviously straightforward, comparison in terms of external standards may not be. 4.3.4 Regression and Supervised Learning This category represents the most important intermediate node in the above network graphic between ML and Bayesian/Monte Carlo algorithms, as well as being strongly connected to several other nodes. While many regression or interpolation algorithms are developed as part of general frameworks within these contexts, there are nevertheless sufficiently many examples of regression and interpolation algorithms unrelated to these contexts to warrant the existence of this distinct category. That said, algorithms within this category share very little in common, and each implementation is generally devised for some explicit applied purpose which may be difficult to relate to any other implementations in this category. Perhaps one feature which almost of the following examples share in common is input and output data in (potentially multi-dimensional) vector format, very generally (but not exclusively) in numeric form. This may be one category in which the development of a system for property-based testing, like the hypothesis framework for python may be particularly useful. Such a system would facilitate tests in response to a range of differently input structures, such as values manifesting different distributional properties. Property-based testing is likely to be a particularly powerful technique for uncovering faults in regression and interpolation algorithms. Examples of the diversity of software in this category include the following. xrnet to perform “hierarchical regularized regression to incorporate external data”, where “external data” in this case refers to structured meta-data as applied to genomic features. survPen is, “an R package for hazard and excess hazard modelling with multidimensional penalized splines” areal is, “an R package for areal weighted interpolation”. ChiRP is a package for “Chinese Restaurant Process mixtures for regression and clustering”, which implements a class of non-parametric Bayesian Monte Carlo models. klrfome is a package for, “kernel logistic regression on focal mean embeddings,” with a specific and exclusive application to the prediction of likely archaeological sites. gravity is a package for “estimation methods for gravity models in R,” where “gravity models” refers to models of spatial interactions between point locations based on the properties of those locations. compboost is an example of an R package for gradient boosting, which is inherently a regression-based technique, and so standards for regression software ought to consider such applications. ungroup is, “an R package for efficient estimation of smooth distributions from coarsely binned data.” As such, this package is an example of regression-based software for which the input data are (effectively) categorical. The package is primarily intended to implement a particular method for “unbinning” the data, and so represents a particular class of interpolation methods. registr is a package for “registration for exponential family functional data,” where registration in this context is effectively an interpolation method applied within a functional data analysis context. One package which may be potential general use is the ggeffects package for “tidy data frames of marginal effects from regression models.” This package aims to make statistics quantifying marginal effects readily understandable, and so implements a standard (tidyverse-based) methodology for representing and visualising statistics relating to marginal effects. 4.3.5 Probability Distributions The category of probability distributions is an outlier in the preceding network diagram, connected only to ML and regression/interpolation algorithms. It is nevertheless included here as a distinct category because we anticipate software which explicitly represents or relies on probability distributions to be subject to distinct standards and assessment procedures, particularly through enabling routines to be tested for robustness against a variety of perturbations to assumed distributional forms. Packages which fall within this category include: univariateML which is, “an R package for maximum likelihood estimation of univariate densities,” which support more than 20 different forms of probability density. kdensity which is, “An R package for kernel density estimation with parametric starts and asymmetric kernels.” This package implements an effectively non-parametric approach to estimating probability densities. overlapping, which is, “a R package for estimating overlapping in empirical distributions.” The obverse process from estimating or fitting probability distributions is arguably drawing samples from defined distributions, of which the humanleague package is an example. This package has a particular application in synthesis of discrete populations, yet the implementation is quite generic and powerful. 4.3.6 Wrapper Packages “Wrapper” packages provide an interface to previously-written software, often in a different computer language to the original implementation. While this category is reasonably unambiguous, there may be instances in which a “wrapper” additionally offers extension beyond original implementations, or in which only a portion of a package’s functionality may be “wrapped.” Rather than internally bundling or wrapping software, a package may also serve as a wrapper thorough providing access to some external interface, such as a web server. Examples of potential wrapper packages include the following: The greta package (with accompanying JOSS article) “for writing statistical models and fitting them by MCMC and optimisation” provides a wrapper around google’s TensorFlow library. It is also clearly a workflow package, aiming to provide a single, unified workflow for generic machine learning processes and analyses. The nse package (with accompanying JOSS paper) which offers “multiple ways to calculate numerical standard errors (NSE) of univariate (or multivariate in some cases) time series,” through providing a unified interface to several other R packages to provide more than 30 NSE estimators. This is an example of a wrapper package which does not wrap either internal code or external interfaces, rather it effectively “wraps” the algorithms of a collection of R packages. Key Considerations: For many wrapper packages it may not be feasible for reviewers (or authors) to evaluate the quality or correctness of the wrapped software, so review could be limited to the interface or added value provided, or the statistical routines within. Wrapper packages include the extent of functionality represented by wrapped code, and the computer language being wrapped. - Internal or External: Does the software internally wrap of bundle previously developed routines, or does it provide a wrapper around some external service? If the latter, what kind of service (web-based, or some other form of remote access)? - Language: For internally-bundled routines, in which computer language e the routines written? And how are they bundled? (For R packages: In ./src? In ./inst? Elsewhere?) - Testing: Does the software test the correctness of the wrapped component? Does it rely on tests of the wrapped component elsewhere? - Unique Advances: What unique advances does the software offer beyond those offered by the (internally or externally) wrapped software? 4.3.7 Networks Network software is a particular area of application of what might often be considered more generic algorithms, as in the example described above of the grapherator package, for which this category is appropriate only because the input data are assumed to represent a particular form of graphical relationship, while most of the algorithms implemented in the package are not necessarily specific to graphs. That package might nevertheless be useful in developing standards because it, “implements a modular approach to benchmark graph generation focusing on undirected, weighted graphs”. This package, and indeed several others developed by its author Jakob Bossek, may be useful in developing benchmarks for comparison of graph or network models and algorithms. Cases of software which might be assessed using such generic graph generators and benchmarks include: mcMST, which is “a toolbox for the multi-criteria minimum spanning tree problem.” gwdegree, which is a package for, “improving interpretation of geometrically-weighted degree estimates in exponential random graph models.” This package essentially generates one key graph statistic from a particular class of input graphs, yet is clearly amenable to benchmarking, as well as measures of stability in response to variable input structures. Network software which is likely more difficult to assess or compare in any general way includes: tcherry is a package for “Learning the structure of tcherry trees,” which themselves are particular ways of representing relationships between categorical data. The package uses maximum likelihood techniques to find the best tcherry tree to represent a given input data set. Although very clearly a form of network software, this package might be considered better described by other categories, and accordingly not directly assessed or assessable under any standards derived for this category. BNLearn is a package “for learning the graphical structure of Bayesian networks.” It is indubitably a network package, yet the domain of application likely renders it incomparable to other network software, and difficult to assess in any standardised way. 4.3.8 Exploratory Data Analysis (EDA) and Summary Statistics Many packages aim to simplify and facilitate the reporting of complex statistical results or exploratory summaries of data. Such reporting commonly involves visualisation, and there is direct overlap between this and the Visualisation category (below). This roughly breaks out into software that summarizes and presents raw data, and software that reports complex data derived from statistical routines. However, this break is often not clean, as raw data exploration may involve an algorithmic or modeling step (e.g., projection pursuit.). Examples include: A package rejected by rOpenSci as out-of-scope, gtsummary, which provides, “Presentation-ready data summary and analytic result tables.” Other examples include: The smartEDA package (with accompanying JOSS paper) “for automated exploratory data analysis”. The package, “automatically selects the variables and performs the related descriptive statistics. Moreover, it also analyzes the information value, the weight of evidence, custom tables, summary statistics, and performs graphical techniques for both numeric and categorical variables.” This package is potentially as much a workflow package as it is a statistical reporting package, and illustrates the ambiguity between these two categories. The modeLLtest package (with accompanying JOSS paper) is “An R Package for Unbiased Model Comparison using Cross Validation.” Its main functionality allows different statistical models to be compared, likely implying that this represents a kind of meta package. The insight package (with accompanying JOSS paper provides “a unified interface to access information from model objects in R,” with a strong focus on unified and consistent reporting of statistical results. The arviz software for python (with accompanying JOSS paper provides “a unified library for exploratory analysis of Bayesian models in Python.” The iRF package (with accompanying JOSS paper enables “extracting interactions from random forests”, yet also focusses primarily on enabling interpretation of random forests through reporting on interaction terms. In addition to potential overlap with the Visualisation category, potential standards for Statistical Reporting and Meta-Software are likely to overlap to some degree with the preceding standards for Workflow Software. Checklist items unique to statistical reporting software might include the following: Automation Does the software automate aspects of statistical reporting, or of analysis at some sufficiently “meta”-level (such as variable or model selection), which previously (in a reference implementation) required manual intervention? General Reporting: Does the software report on, or otherwise provide insight into, statistics or important aspects of data or analytic processes which were previously not (directly) accessible using reference implementations? Comparison: Does the software provide or enable standardised comparison of inputs, processes, models, or outputs which could previously (in reference implementations) only be accessed or compared some comparably unstandardised form? Interpretation: Does the software facilitate interpretation of otherwise abstruse processes or statistical results? Exploration: Does the software enable or otherwise guide exploratory stages of a statistical workflow? 4.3.9 Workflow Support “Workflow” software may not implement particular methods or algorithms, but rather support tasks around the statistical process. In many cases, these may be generic tasks that apply across methods. These include: Classes (whether explicit or not) for representing or processing input and output data; Generic interfaces to multiple statistical methods or algorithms; Homogeneous reporting of the results of a variety of methods or algorithms; and Methods to synthesise, visualise, or otherwise collectively report on analytic results. Methods and Algorithms software may only provide a specific interface to a specific method or algorithm, although it may also be more general and offer several of the above “workflow” aspects, and so ambiguity may often arise between these two categories. We note in particular that the “workflow” node in the interactive network diagram mentioned above is very strongly connected to the “machine learning” node, generally reflecting software which attempts to unify varied interfaces to varied platforms for machine learning. Among the numerous examples of software in this category are: The mlr3 package (with accompanying JOSS paper), which provides, “A modern object-oriented machine learning framework in R.” The fmcmc package (with accompanying JOSS paper), which provides a unified framework and workflow for Markov-Chain Monte Carlo analyses. The bayestestR package (with accompanying JOSS paper) for \"describing effects and their uncertainty, existence and significance within the Bayesian framework. While this packages includes its own algorithmic implementations, it is primarily intended to aid general Bayesian workflows through a unified interface. Workflows are also commonly required and developed for specific areas of application, as exemplified by the tabular package (with accompanying JOSS article for “Analysis, Seriation, and visualisation of Archaeological Count Data”. Key Considerations: Workflow packages are popular and add considerable value and efficiency for users. One challenge in evaluating such packages is the importance of API design and potential subjectivity of this. For instance, mlr3 as well as tidymodels have similar uses of providing a common interface to multiple predictive models and tools for automating processes across these models. Similar, multiple packages have different approaches for handling MCMC data. Each package makes different choices in design and has different priorities, which may or may not agree with reviewers’ opinions or applications. Despite such differences, it may be possible to evaluate such packages for internal cohesion, and adherence to a sufficiently clearly stated design goal. Reviewers may be able to evaluate whether the package provides a more unified workflow or interface than other packages - this would require a standard of relative improvement over the field rather than baseline standards. These packages also often contain numerical routines (cross-validation, performance scoring, model comparison), that can be evaluated for correctness or accuracy. 4.3.10 Spatial Analyses Spatial analyses have a long tradition in R, as summarised and reflected in the CRAN Task Views on Spatial and Spatio-Temporal data and analyses. Those task views also make immediately apparent that the majority of development in both of these domains has been in representations of spatial data, rather than in statistical analyses per se. Spatial statistical analyses have nevertheless been very strong in R, notably through the spatstat and gstat packages, first published in 2002 and 2003, respectively. Spatial analyses entail a number of aspects which, while not necessarily unique in isolation, when considered in combination offer sufficiently unique challenges for this to warrant its own category. Some of these unique aspects include: A generally firm embeddedness in two dimensions Frequent assumptions of continuous rather than discrete processes (point-pattern processes notwithstanding) A pervasive decrease in statistical similarity with increasing distance - the so-called “First Law of Geography” - which is the observe of pervasive difficulties arising from auto-correlated observations. A huge variety of statistical techniques such as kriging and triangulation which have been developed for almost exclusive application in spatial domains. The unique challenges arising in the domain of Spatial Temporal Analyses. 4.3.11 Time Series Analyses We will also consider time series software as a distinct category, owing to unique ways of representing and processing such data. 4.4 Out Of Scope Categories The following categories arise in the preceding empirical analyses, yet have been deemed to lie beyond the scope of the project as currently envisioned. 4.4.1 Visualisation While many may consider software primarily aimed at visualisation to be out of scope, there are nevertheless cases which may indeed be within scope, notably including the ggfortify package which allows results of statistical tests to be “automatically” visualised using the ggplot2 package. The list of “fortified” functions on the packages webpage clearly indicates the very predominantly statistical scope of this software which is in effect a package for statistical reporting, yet in visual rather than tabular form. Other examples of visualisation software include: The modelStudio package (with accompanying JOSS paper), which is also very much a workflow package. The shinyEFA package (with accompanying JOSS paper) which provides a, “User-Friendly Shiny Application for Exploratory Factor Analysis.” The autoplotly package (with accompanying JOSS paper) which provides, “Automatic Generation of Interactive Visualisations for Statistical Results”, primarily by porting the output of the authors’ above-mentioned ggfortify package to plotly.js. Key considerations: The quality or utility visualization techniques can be strongly subjective, but also may be evaluated using standardized principles if the community can come to a consensus on those principles. Such considerations may be context-dependent - e.g., the requirements of a diagnostic plot designed to support model-checking are different from that designed to present raw data or model results to a new audience. This implies that the intended purpose of the visualization should be well-defined. Whether or not visualization is in-scope, many software packages with other primary purposes also include functions to visualise output. Visualization will thus never be strictly out of scope. However one option is not to include primarily visualization packages, or only statistical visualization packages in which visualization is closely tied to another category or purpose. Visualisation packages will include numerical or statistical routines for transforming data from raw form to graphics, which can be evaluated for correctness or accuracy. 4.4.2 Education A prominent class of statistical software is educational software designed to teach statistics. Such software many include its own implementations of statistical methods, and frequently include interactive components. Many examples of educational statistical software are listed on the CRAN Task View: Teaching Statistics. This page also clearly indicates the likely strong overlap between education and visualisation software. With specific regard to the educational components of software, the follow checklist items may be relevant. A prominent example is the LearnBayes package. Key Considerations: Correctness of implementation of educational or tutorial software is important. Evaluation of such software extends considerably beyond correctness, with heavy emphasis on documentation, interactive interface, and pedagogical soundness of the software. These areas enter a very different class of standards. It is likely that educational software will very greatly structurally, as interaction may be via graphical or web interfaces, text interaction or some other form. The Journal of Open Source Education accepts both educational software and curricula, and has a peer review system (almost) identical to JOSS. Educational statistical software reviewed by rOpenSci could thus potentially be fast-tracked through JOSE reviews just as current submissions have the opportunity to be fast-tracked through the JOSS review process. Demand: Does the software meet a clear demand otherwise absent from educational material? If so, how? Audience: What is the intended audience or user base? (For example, is the software intended for direct use by students of statistics, or does it provide a tool for educational professionals to use in their own practice?) Algorithms: What are the unique algorithmic processes implemented by the software? In what ways are they easier, simpler, faster, or otherwise better than reference implementations (where such exist)? Interactivity: Is the primary function of the software interactive? If so, is the interactivity primarily graphical (for example, web-based), text-based, or other? 4.5 Proposals Peer review in the system will primarily focus on code written in R, C, and C++. Standards will be written so as to separate language-specific and non-language-specific components with an eye towards further adoption by other groups in the future (in particular groups focussed on the Python language). The system will be limited to R packages, and tools developed will be specific to R package structure, although keeping in mind potential future adaptation and adaptability to non-packaged R code. Standards that may apply to non-packaged are code may also be noted for use in other contexts. Submissions will be required to nominate at least one statistical category, to nominate at least one “reference implementation”, and to explain how the submitted software is superior (along with a possibility to explain why software may be sufficiently unique that there is no reference implementation, and so no claims of superiority can be made). We will only review packages where the primary statistical functionality is in the main source code developed by the authors, and not in an external package. The following 11 categories of statistical software be defined, and be considered in scope: Bayesian and Monte Carlo algorithms Dimensionality Reduction and Feature Selection Machine Learning Regression and Interpolation Probability Distributions Wrapper Packages Networks Exploratory Data Analysis Workflow Software Summary Statistics Spatial Statistics The following categories be considered, at least initially, to be out-of-scope: Educational Software Visualisation Software Beyond these general Proposals, the following lists Proposals specific to particular categories of statistical software: For packages which parameterise or fit probability distributions, develop routines to assess and quantify the sensitivity of outputs to the distributional properties of inputs, and particularly to deviations from assumed distributional properties. We identify a sub-category of software which accepts network inputs, and develop (or adapt) general techniques to generate generic graphs to be used in benchmarking routines. Other software which falls within the category of Network Software only because of restricted aspects such as internal data representations (such as tcherry) not be considered or assessed within that category. References "],["standards.html", "Chapter 5 Standards [SEEKING FEEDBACK] 5.1 Other Standards 5.2 General Standards for Statistical Software 5.3 Bayesian and Monte Carlo Software 5.4 Regression and Supervised Learning 5.5 Dimensionality Reduction, Clustering, and Unsupervised Learning 5.6 Exploratory Data Analysis 5.7 Time Series Software 5.8 Machine Learning Software", " Chapter 5 Standards [SEEKING FEEDBACK] This Chapter is divided between: “General Standards” which may be applied to all software considered within this project, irrespective of how it may be categorized under the times of categories of statistical software listed above; and “Specific Standards” which apply to different degrees to statistical software depending on the software category. It is likely that standards developed under the first category may subsequently be deemed to be genuinely Statistical Standards yet which are applicable across all categories, and it may also be likely that the development of category-specific standards reveals aspects which are common across all categories, and which may subsequently be deemed general standards. We accordingly anticipate a degree of fluidity between these two broad categories. There is also a necessary relationship between the Standards described here, and processes of Assessment described below in Chapter 8. We consider the latter to describe concrete and generally quantitative aspects of post hoc software assessment, while the present Standards provides guides and benchmarks against which to prospectively compare software during development. As this entire document is intended to serve as the defining reference for our Standards, that term may in turn be interpreted to reflect this entire document, with the current section explicitly describing aspects of Standards not covered elsewhere. As described above, we anticipate the ongoing development of this current document to employ a versioning system, with software reviewed and hosted under the system mandated to flag the latest version of these standards to which it complies. 5.1 Other Standards Among the noteworthy instances of software standards which might be adapted for our purposes, and in addition to entries in our Annotated Bibliography, the following are particularly relevant: The Core Infrastructure Initiative’s Best Practices Badge, which is granted to software meeting an extensive list of criteria. This list of criteria provides a singularly useful reference for software standards. The Software Sustainability Institute’s Software Evaluation Guide, in particular their guide to Criteria-based software evaluation, which considers two primary categories of Usability and Sustainability and Maintainability, each of which is divided into numerous sub-categories. The guide identifies numerous concrete criteria for each sub-category, explicitly detailed below in order to provide an example of the kind of standards that might be adapted and developed for application to the present project. The Transparent Statistics Guidelines, by the “HCI (Human Computer Interaction) Working Group”. While currently only in its beginning phases, that document aims to provide concrete guidance on “transparent statistical communication.” If its development continues, it is likely to provide useful guidelines on best practices for how statistical software produces and reports results. The more technical considerations of the Object Management Group’s Automated Source Code CISQ Maintainability Measure (where CISQ refers to the Consortium for IT Software Quality). This guide describes a number of measures which can be automatically extracted and used to quantify the maintainability of source code. None of these measures are not already considered in one or both of the preceding two documents, but the identification of measures particularly amenable to automated assessment provides a particularly useful reference. There is also rOpenSci’s guide on package development, maintenance, and peer review, which provides standards of this type for R packages, primarily within its first chapter. Another notable example is the tidyverse design guide, and the section on Conventions for R Modeling Packages which provides guidance for model-fitting APIs. Specific standards for neural network algorithms have also been developed as part of a google 2019 Summer Of Code project, resulting in a dedicated R package, NNbenchmark, and accompanying results—their so-called “notebooks”—of applying their benchmarks to a suite of neural network packages. 5.2 General Standards for Statistical Software These standards refer to Data Types as the fundamental types defined by the R language itself between the following: Continuous (numeric) Integer String / character Date/Time Factor Ordered Factor The standards also refer to tabular data, intended to connote any rectangular data form, including but not limited to matrix, two-dimensional array, data.frame, and any extensions thereof such as tibble. 5.2.1 Documentation Standards will include requirements for form and completeness of documentation. As with interface, several sources already provide starting points for reasonable documentation. Some documentation requirements will be specific to the statistical context. For instance, it is likely we will have requirements for referencing appropriate literature or references for theoretical support of implementations. Another area of importance is correctness and clarity of definitions of statistical quantities produced by the software, e.g., the definition of null hypotheses or confidence intervals. Data included in software – that used in examples or tests – will also have documentation requirements. It is worth noting that the roxygen system for documenting R packages is readily extensible, as exemplified through the roxytest package for specifying tests in-line. The following standards describe several forms of what might be considered “Supplementary Material”. While there are many places within an R package where such material may be included, common locations include vignettes, or in additional directories (such as data-raw) listed in .Rbuildignore to prevent inclusion within installed packages. Where software supports a publication, all claims made in the publication with regard to software performance (for example, claims of algorithmic scaling or efficiency; or claims of accuracy), the following standard applies: G1.0 Software should include all code necessary to reproduce results which form the basis of performance claims made in associated publications. Where claims regarding aspects of software performance are made with respect to other extant R packages, the following standard applies: G1.1 Software should include code necessary to compare performance claims with alternative implementations in other R packages. 5.2.2 Input Structures This section considers general standards for Input Structures. These standards may often effectively be addressed through implementing class structures, although this is not a general requirement. Developers are nevertheless encouraged to examine the guide to S3 vectors in the vctrs package as an example of the kind of assurances and validation checks that are possible with regard to input data. Systems like those demonstrated in that vignette provide a very effective way to ensure that software remains robust to diverse and unexpected classes and types of input data. 5.2.2.1 Uni-variate (Vector) Input It is important to note for univariate data that single values in R are vectors with a length of one, and that 1 is of exactly the same data type as 1:n. Given this, inputs expected to be univariate should: G2.0 Provide explicit secondary documentation of any expectations on lengths of inputs (generally implying identifying whether an input is expected to be single- or multi-valued) G2.1 Provide explicit secondary documentation of expectations on data types of all vector inputs (see the above list). G2.2 Appropriately prohibit or restrict submission of multivariate input to parameters expected to be univariate. G2.3 For univariate character input: G2.3a Use match.arg() or equivalent where applicable to only permit expected values. G2.3b Either: use tolower() or equivalent to ensure input of character parameters is not case dependent; or explicitly document that parameters are strictly case-sensitive. G2.4 Provide appropriate mechanisms to convert between different data types, potentially including: G2.4a explicit conversion to integer via as.integer() G2.4b explicit conversion to continuous via as.numeric() G2.4c explicit conversion to character via as.character() (and not paste or paste0) G2.4d explicit conversion to factor via as.factor() G2.4e explicit conversion from factor via as...() functions G2.5 Where inputs are expected to be of factor type, secondary documentation should explicitly state whether these should be ordered or not, and those inputs should provide appropriate error or other routines to ensure inputs follow these expectations. 5.2.2.2 Tabular Input This sub-section concerns input in “tabular data” forms, implying the two primary distinctions within R itself between array or matrix representations, and data.frame and associated representations. Among important differences between these two forms are that array/matrix classes are restricted to storing data of a single uniform type (for example, all integer or all character values), whereas data.frame as associated representations store each column as a list item, allowing different columns to hold values of different types. Further noting that a matrix may, as of R version 4.0, be considered as a strictly two-dimensional array, tabular inputs for the purposes of these standards are considered to imply data represented in one or more of the following forms: Given this, tabular inputs may be in one or or more of the following forms: matrix form when referring to specifically two-dimensional data of one uniform type array form as a more general expression, or when referring to data that are not necessarily or strictly two-dimensional data.frame Extensions such as tibble data.table domain-specific classes such as tsibble for time series, or sf for spatial data. The term “data.frame and associated forms” is assumed to refer to data represented in either the base::data.frame format, and/or any of the classes listed in the final of the above points. General Standards applicable to software which is intended to accept any one or more of these tabular inputs are then that: G2.6 Software should accept as input as many of the above standard tabular forms as possible, including extension to domain-specific forms. G2.7 Software should provide appropriate conversion routines as part of initial pre-processing to ensure that all other sub-functions of a package receive inputs of a single defined class or type. G2.8 Software should issue diagnostic messages for type conversion in which information is lost (such as conversion of variables from factor to character; standardisation of variable names; or removal of meta-data such as those associated with sf-format data) or added (such as insertion of variable or column names where none were provided). The next standard concerns the following inconsistencies between three common tabular classes in regard the column extraction operator, [. class (x) # x is any kind of `data.frame` object #&gt; [1] &quot;data.frame&quot; class (x [, 1]) #&gt; [1] &quot;integer&quot; class (x [, 1, drop = TRUE]) # default #&gt; [1] &quot;integer&quot; class (x [, 1, drop = FALSE]) #&gt; [1] &quot;data.frame&quot; x &lt;- tibble::tibble (x) class (x [, 1]) #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; class (x [, 1, drop = TRUE]) #&gt; [1] &quot;integer&quot; class (x [, 1, drop = FALSE]) # default #&gt; [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; x &lt;- data.table::data.table (x) class (x [, 1]) #&gt; [1] &quot;data.table&quot; &quot;data.frame&quot; class (x [, 1, drop = TRUE]) # no effect #&gt; [1] &quot;data.table&quot; &quot;data.frame&quot; class (x [, 1, drop = FALSE]) # default #&gt; [1] &quot;data.table&quot; &quot;data.frame&quot; Extracting a single column from a data.frame returns a vector by default, and a data.frame if drop = FALSE. Extracting a single column from a tibble returns a single-column tibble by default, and a vector is drop = TRUE. Extracting a single column from a data.table always returns a data.table, and the drop argument has no effect. Given such inconsistencies, G2.9 Software should ensure that extraction or filtering of single columns from tabular inputs should not presume any particular default behaviour, and should ensure all column-extraction operations behave consistently regardless of the class of tabular data used as input. Adherence to the above standard G2.6 will ensure that any implicitly or explicitly assumed default behaviour will yield consistent results regardless of input classes. 5.2.2.3 Missing or Undefined Values G2.10 Statistical Software should implement appropriate checks for missing data as part of initial pre-processing prior to passing data to analytic algorithms. G2.11 Where possible, all functions should provide options for users to specify how to handle missing (NA) data, with options minimally including: G2.11a error on missing data G2.11b ignore missing data with default warnings or messages issued G2.11c replace missing data with appropriately imputed values G2.12 Functions should never assume non-missingness, and should never pass data with potential missing values to any base routines with default na.rm = FALSE-type parameters (such as mean(), sd() or cor()). G2.13 All functions should also provide options to handle undefined values (e.g., NaN, Inf and -Inf), including potentially ignoring or removing such values. 5.2.3 Output Structures G3.0 Statistical Software which enables outputs to be written to local files should parse parameters specifying file names to ensure appropriate file suffixes are automatically generated where not provided. 5.2.4 Testing All packages should follow rOpenSci standards on testing and continuous integration, including aiming for high test coverage. Extant R packages which may be useful for testing include testthat, tinytest, roxytest, and xpectr. G4.0 Where applicable or practicable, tests should use standard data sets with known properties (for example, the NIST Standard Reference Datasets, or data sets provided by other widely-used R packages). G4.1 Data sets created within, and used to test, a package should be exported (or otherwise made generally available) so that users can confirm tests and run examples. For testing statistical algorithms, tests should include tests of the following types: G4.2 Correctness tests to test that statistical algorithms produce expected results to some fixed test data sets (potentially through comparisons using binding frameworks such as RStata). G4.2a For new methods, it can be difficult to separate out correctness of the method from the correctness of the implementation, as there may not be reference for comparison. In this case, testing may be implemented against simple, trivial cases or against multiple implementations such as an initial R implementation compared with results from a C/C++ implementation. G4.2b For new implementations of existing methods, correctness tests should include tests against previous implementations. Such testing may explicitly call those implementations in testing, preferably from fixed-versions of other software, or use stored outputs from those where that is not possible. G4.2c Where applicable, stored values may be drawn from published paper outputs when applicable and where code from original implementations is not available G4.3 Correctness tests should be run with a fixed random seed G4.4 Parameter recovery tests to test that the implementation produce expected results given data with known properties. For instance, a linear regression algorithm should return expected coefficient values for a simulated data set generated from a linear model. G4.4a Parameter recovery tests should generally be expected to succeed within a defined tolerance rather than recovering exact values. G4.4b Parameter recovery tests should be run with multiple random seeds when either data simulation or the algorithm contains a random component. (When long-running, such tests may be part of an extended, rather than regular, test suite; see G4.8-4.10, below). G4.5 Algorithm performance tests to test that implementation performs as expected as properties of data change. For instance, a test may show that parameters approach correct estimates within tolerance as data size increases, or that convergence times decrease for higher convergence thresholds. G4.6 Edge condition tests to test that these conditions produce expected behaviour such as clear warnings or errors when confronted with data with extreme properties including but not limited to: G4.6a Zero-length data G4.6b Data of unsupported types (e.g., character or complex numbers in for functions designed only for numeric data) G4.6c Data with all-NA fields or columns or all identical fields or columns G4.6d Data outside the scope of the algorithm (for example, data with more fields (columns) than observations (rows) for some regression algorithms) G4.7 Noise susceptibility tests Packages should test for expected stochastic behaviour, such as through the following conditions: G4.7a Adding trivial noise (for example, at the scale of .Machine$double.eps) to data does not meaningfully change results G4.7b Running under different random seeds or initial conditions does not meaningfully change results 5.2.4.1 Extended tests Thorough testing of statistical software may require tests on large data sets, tests with many permutations, or other conditions leading to long-running tests. In such cases it may be neither possible nor advisable to execute tests continuously, or with every code change. Software should nevertheless test any and all conditions regardless of how long tests may take, and in doing so should adhere to the following standards: G4.8 Extended tests should included and run under a common framework with other tests but be switched on by flags such as as a &lt;MYPKG&gt;_EXTENDED_TESTS=1 environment variable. G4.9 Where extended tests require large data sets or other assets, these should be provided for downloading and fetched as part of the testing workflow. G4.9a When any downloads of additional data necessary for extended tests fail, the tests themselves should not fail, rather be skipped and implicitly succeed with an appropriate diagnostic message. G4.10 Any conditions necessary to run extended tests such as platform requirements, memory, expected runtime, and artefacts produced that may need manual inspection, should be described in developer documentation such as a CONTRIBUTING.md or tests/README.md file. 5.3 Bayesian and Monte Carlo Software Bayesian and Monte Carlo Software (hereafter referred to for simplicity as “Bayesian Software”) is presumed to perform one or more of the following steps: Document how to specify inputs including: 1.1 Data 1.2 Hyperparameters determining prior distributions 1.3 Parameters determining the computational processes Accept and validate all of forms of input Apply data transformation and pre-processing steps Apply one or more analytic algorithms, generally sampling algorithms used to generate estimates of posterior distributions Return the result of that algorithmic application Offer additional functionality such as printing or summarising return results This document details standards for each of these steps, each prefixed with “BS”. 5.3.1 Documentation of Inputs Prior to actual standards for documentation of inputs, we note one terminological standard for Bayesian software: BS1.0 Bayesian software should use the term “hyperparameter” exclusively to refer to parameters determining the form of prior distributions, and should use either the generic term “parameter” or some conditional variant(s) such as “computation parameters” to refer to all other parameters. Bayesian Software should provide the following documentation of how to specify inputs: BS1.1 Description of how to enter data, both in textual form and via code examples. Both of these should consider the simplest cases of single objects representing independent and dependent data, and potentially more complicated cases of multiple independent data inputs. BS1.2 Description of how to specify prior distributions, both in textual form describing the general principles of specifying prior distributions, along with more applied descriptions and examples, within: B31.2a The main package README, either as textual description or example code B31.2b At least one package vignette, both as general and applied textual descriptions, and example code B31.2c Function-level documentation, preferably with code included in examples BS1.3 Description of all parameters which control the computational process (typically those determining aspects such as numbers and lengths of sampling processes, seeds used to start them, thinning parameters determining post-hoc sampling from simulated values, and convergence criteria). In particular: BS1.3a Bayesian Software should document, both in text and examples, how to use the output of previous simulations as starting points of subsequent simulations. BS1.3b Where applicable, Bayesian software should document, both in text and examples, how to use different sampling algorithms for a given model. BS1.4 For Bayesian Software which implements or otherwise enables convergence checkers, documentation should explicitly describe and provide examples of use with and without convergence checkers. BS1.5 For Bayesian Software which implements or otherwise enables multiple convergence checkers, differences between these should be explicitly tested. 5.3.2 Input Data Structures and Validation This section contains standards primarily intended to ensure that input data, including model specifications, are validated prior to passing through to the main computational algorithms. 5.3.2.1 Input Data Bayesian Software is commonly designed to accept generic one- or two-dimensional forms such as vector, matrix, or data.frame objects. The first standards concerns the range of possible generic forms for input data: BS2.0 Bayesian Software which accepts one-dimensional input should ensure values are appropriately pre-processed regardless of class structures. The units package provides a good example, in creating objects that may be treated as vectors, yet which have a class structure that does not inherit from the vector class. Using these objects as input often causes software to fail. The storage.mode of the underlying objects may nevertheless be examined, and the objects transformed or processed accordingly to ensure such inputs do not lead to errors. BS2.1 Bayesian Software which accepts two-dimension input should implement pre-processing routines to ensure conversion of as many possible forms as possible to some standard format which is then passed to all analytic functions. In particular, tests should demonstrate that: BS2.1a data.frame or equivalent objects which have columns which do not themselves have standard class attributes (typically, vector) are appropriately processed, and do not error without reason. This behaviour should be tested. Again, columns created by the units package provide a good test case. BS2.1b data.frame or equivalent objects which have list columns should ensure that those columns are appropriately pre-processed either through being removed, converted to equivalent vector columns where appropriate, or some other appropriate treatment. This behaviour should be tested. BS2.2 Bayesian Software should implement pre-processing routines to ensure all input data is dimensionally commensurate, for example by ensuring commensurate lengths of vectors or numbers of rows of tabular inputs. 5.3.2.2 Prior Distributions, Model Specifications, and Hyperparameters The second set of standards in this section concern specification of prior distributions, model structures, or other equivalent ways of specifying hypothesised relationships among input data structures. R already has a diverse range of Bayesian Software with distinct approaches to this task, commonly either through specifying a model as a character vector representing an R function, or an external file either as R code, or encoded according to some alternative system (such as for rstan). As explicated above, the term “hyperparameters” is interpreted here to refer to parameters which define prior distributions, while a “model specification”, or simply “model”, is an encoded description of how those hyperparameters are hypothesised to transform to a posterior distribution. Bayesian Software should: BS2.3 Ensure that all appropriate validation and pre-processing of hyperparameters are implemented as distinct pre-processing steps prior to submitting to analytic routines, and especially prior to submitting to multiple parallel computational chains. BS2.4 Ensure that lengths of hyperparameter vectors are checked, with no excess values silently discarded (unless such output is explicitly suppressed, as detailed below). BS2.5 Ensure that lengths of hyperparameter vectors are commensurate with expected model input (see example immediately below) BS2.6 Where possible, implement pre-processing checks to validate appropriateness of numeric values submitted for hyperparameters; for example, by ensuring that hyperparameters defining second-order moments such as distributional variance or shape parameters, or any parameters which are logarithmically transformed, are non-negative. The following example demonstrates how standards like the above (BS2.5-2.6) might be addressed. Consider the following function which defines a log-likelihood estimator for a linear regression, controlled via a vector of three hyperparameters, p: ll &lt;- function (x, y, p) dnorm (y - (p[1] + x * p[2]), sd = p[3], log = TRUE) Pre-processing stages should be used to determine: That the dimensions of the input data, x and y, are commensurate (BS2.2); non-commensurate inputs should error by default. The length of the vector p (BS2.4) The latter task is not necessarily straightforward, because the definition of the function, ll(), will itself generally be part of the input to an actual Bayesian Software function. This functional input thus needs to be examined to determine expected lengths of hyperparameter vectors. The following code illustrates one way to achieve this, relying on utilities for parsing function calls in R, primarily through the getParseData function from the utils package. The parse data for a function can be extracted with the following line: x &lt;- getParseData (parse (text = deparse (ll))) The object x is a data.frame of every R token (such as an expression, symbol, or operator) parsed from the function ll. The following section illustrates how this data can be used to determine the expected lengths of vector inputs to the function, ll(). click to see details Input arguments used to define parameter vectors in any R software are accessed through R’s standard vector access syntax of vec[i], for some element i of a vector vec. The parse data for such begins with the SYMBOL of vec, the [, a NUM_CONST for the value of i, and a closing ]. The following code can be used to extract elements of the parse data which match this pattern, and ultimately to extract the various values of i used to access members of vec. vector_length &lt;- function (x, i) { xn &lt;- x [which (x$token %in% c (&quot;SYMBOL&quot;, &quot;NUM_CONST&quot;, &quot;&#39;[&#39;&quot;, &quot;&#39;]&#39;&quot;)), ] # split resultant data.frame at first &quot;SYMBOL&quot; entry xn &lt;- split (xn, cumsum (xn$token == &quot;SYMBOL&quot;)) # reduce to only those matching the above pattern xn &lt;- xn [which (vapply (xn, function (j) j$text [1] == i &amp; nrow (j) &gt; 3, logical (1)))] ret &lt;- NA_integer_ # default return value if (length (xn) &gt; 0) { # get all values of NUM_CONST as integers n &lt;- vapply (xn, function (j) as.integer (j$text [j$token == &quot;NUM_CONST&quot;] [1]), integer (1), USE.NAMES = FALSE) # and return max of these ret &lt;- max (n) } return (ret) } That function can then be used to determine the length of any inputs which are used as hyperparameter vectors: ll &lt;- function (p, x, y) dnorm (y - (p[1] + x * p[2]), sd = p[3], log = TRUE) p &lt;- parse (text = deparse (ll)) x &lt;- utils::getParseData (p) # extract the names of the parameters: params &lt;- unique (x$text [x$token == &quot;SYMBOL&quot;]) lens &lt;- vapply (params, function (i) vector_length (x, i), integer (1)) lens #&gt; y p x #&gt; NA 3 NA And the vector p is used as a hyperparameter vector containing three parameters. Any initial value vectors can then be examined to ensure that they have this same length. Not all Bayesian Software is designed to accept model inputs expressed as R code. The rstan package, for example, implements its own model specification language, and only allows hyperparameters to be named, and not addressed by index. While this largely avoids problems of mismatched lengths of parameter vectors, the software (at v2.21.1) does not ensure the existence of named parameters prior to starting the computational chains. This ultimately results in each chain generating an error when a model specification refers to a non-existent or undefined hyperparameter. Such controls should be part of a single pre-processing stage, and so should only generate a single error. 5.3.2.3 Computational Parameters Computational parameters are considered here as those passed to Bayesian functions other than hyperparameters determining the forms of prior distributions. They typically include parameters controlling lengths of runs, lengths of burn-in periods, numbers of parallel computations, other parameters controlling how samples are to be generated, or convergence criteria. All Computational Parameters should be checked for general “sanity” prior to calling primary computational algorithms. The standards for such sanity checks include that Bayesian Software should: BS2.7 Check that values for parameters are positive (except where negative values may be accepted) BS2.8 Check lengths and/or dimensions of inputs, and either automatically reject or provide appropriate diagnostic messaging for parameters of inappropriate length or dimension; for example passing a vector of length &gt; 1 to a parameter presumed to define a single value (unless such output is explicitly suppressed, as detailed below) BS2.9 Check that arguments are of expected classes or types (for example, check that integer-type arguments are indeed integer, with explicit conversion via as.integer where not) BS2.10 Automatically reject parameters of inappropriate type (for example character values passed for integer-type parameters that are unable to be appropriately converted). The following two sub-sections consider particular cases of computational parameters. 5.3.2.4 Seed Parameters Bayesian software should: BS2.11 Enable seeds to be passed as a parameter (through a direct seed argument or similar), or as a vector of parameters, one for each chain. BS2.12 Enable results of previous runs to be used as starting points for subsequent runs Bayesian Software which implements parallel processing should: BS2.13 Ensure each chain is started with a different seed by default BS2.14 Issue diagnostic messages when identical seeds are passed to distinct computational chains BS2.15 Explicitly document advice not* to use set.seed()* BS2.16 Provide the parameter with a plural* name: for example, “starting_values” and not “starting_value”* To avoid potential confusion between separate parameters to control random seeds and starting values, we recommended a single “starting values” rather than “seeds” argument, with appropriate translation of these parameters into seeds where necessary. 5.3.2.5 Output Verbosity All Bayesian Software should implement computational parameters to control output verbosity. Bayesian computations are often time-consuming, and often performed as batch computations. The following standards should be adhered to in regard to output verbosity: BS2.17 Bayesian Software should implement at least one parameter controlling the verbosity of output, defaulting to verbose output of all appropriate messages, warnings, errors, and progress indicators. BS2.18 Bayesian Software should enable suppression of messages and progress indicators, while retaining verbosity of warnings and errors. This should be tested. BS2.19 Bayesian Software should enable suppression of warnings where appropriate. This should be tested. BS2.20 Bayesian Software should explicitly enable errors to be caught, and appropriately processed either through conversion to warnings, or otherwise captured in return values. This should be tested. 5.3.3 Pre-processing and Data Transformation 5.3.3.1 Missing Values Bayesian Software should: BS3.0 Explicitly document assumptions made in regard to missing values; for example that data is assumed to contain no missing (NA, Inf) values, and that such values, or entire rows including any such values, will be automatically removed from input data. BS3.1 Implement appropriate routines to pre-process missing values prior to passing data through to main computational algorithms. 5.3.3.2 Perfect Collinearity Where appropriate, Bayesian Software should: BS3.2 Implement pre-processing routines to diagnose perfect collinearity, and provide appropriate diagnostic messages or warnings BS3.3 Provide distinct routines for processing perfectly collinear data, potentially bypassing sampling algorithms An appropriate test for BS3.3 would confirm that system.time() or equivalent timing expressions for perfectly collinear data should be less than equivalent routines called with non-collinear data. Alternatively, a test could ensure that perfectly collinear data passed to a function with a stopping criteria generated no results, while specifying a fixed number of iterations may generate results. 5.3.4 Analytic Algorithms As mentioned, analytic algorithms for Bayesian Software are commonly algorithms to simulate posterior distributions, and to draw samples from those simulations. Numerous extent R packages implement and offer sampling algorithms, and not all Bayesian Software will internally implement sampling algorithms. The following standards apply to packages which do implement internal sampling algorithms: BS4.0 Packages should document sampling algorithms (generally via literary citation, or reference to other software) BS4.1 Packages should provide explicit comparisons with external samplers which demonstrate intended advantage of implementation (generally via tests, vignettes, or both). Regardless of whether or not Bayesian Software implements internal sampling algorithms, it should: BS4.2 Implement at least one means to validate posterior estimates (for example through the functionality of the BayesValidate package, noting that that package has not been updated for almost 15 years, and such approaches may need adapting; or the Simulation Based Calibration approach implemented in the rstan function sbc). Where possible or applicable, Bayesian Software should: BS4.3 Implement at least one type of convergence checker, and provide a documented reference for that implementation. BS4.3 Enable computations to be stopped on convergence (although not necessarily by default). BS4.5 Ensure that appropriate mechanisms are provided for models which do not converge. This is often achieved by having default behaviour to stop after specified numbers of iterations regardless of convergence. BS4.6 Implement tests to confirm that results with convergence checker are statistically equivalent to results from equivalent fixed number of samples without convergence checking. BS4.7 Where convergence checkers are themselves parametrised, the effects of such parameters should also be tested. For threshold parameters, for example, lower values should result in longer sequence lengths. 5.3.5 Return Values Unlike software in many other categories, Bayesian Software should generally return several kinds of distinct data, both the raw data derived from statistical algorithms, and associated metadata. Such distinct and generally disparate forms of data will be generally best combined into a single object through implementing a defined class structure, although other options are possible, including (re-)using extant class structures (see the CRAN Task view on Bayesian Inference. https://cran.r-project.org/web/views/Bayesian.html) for reference to other packages and class systems). Regardless of the precise form of return object, and whether or not defined class structures are used or implemented, the objects returned from Bayesian Software should include: BS5.0 Seed(s) or starting value(s), including values for each sequences where multiple sequences are included BS5.1 Appropriate metadata on types (or classes) and dimensions of input data With regard to the input function, or alternative means of specifying prior distributions: BS5.2 Bayesian Software should either: BS5.2a Return the input function or prior distributional specification in the return object; or BS5.2b Enable direct access to such via additional functions which accept the return object as single argument. Where convergence checkers are implemented or provided, Bayesian Software should: BS5.3 Return convergence statistics or equivalent BS5.4 Where multiple checkers are enabled, return details of convergence checker used BS5.5 Appropriate diagnostic statistics to indicate absence of convergence are either returned or immediately able to be accessed. 5.3.6 Additional Functionality Bayesian Software should: BS6.0 Implement a default print method for return objects BS6.1 Implement a default plot method for return objects BS6.2 Provide and document straightforward abilities to plot sequences of posterior samples, with burn-in periods clearly distinguished BS6.3 Provide and document straightforward abilities to plot posterior distributional estimates Bayesian Software may: BS6.4 Provide summary methods for return objects BS6.5 Provide abilities to plot both sequences of posterior samples and distributional estimates together in single graphic 5.3.7 Tests Parameter recovery tests: Recover the prior with no data or data with no information, especially where priors are implicit. Even in empirical Bayes, recover the estimated prior Recover posterior given expected data and prior Algorithmic scaling tests with data - is it linear, log, etc Test for prediction/fitted values on same scale as input values 5.4 Regression and Supervised Learning This document details standards for Regression and Supervised Learning Software – referred to from here on for simplicity as “Regression Software”. Regression Software implements algorithms which aim to construct or analyse one or more mappings between two defined data sets (for example, a set of “independent” data, \\(X\\), and a set of “dependent” data, \\(Y\\)). In contrast, the analogous category of Unsupervised Learning Software aims to construct or analyse one or more mappings between a defined set of input or independent data, and a second set of “output” data which are not necessarily known or given prior to the analysis. Common purposes of Regression Software are to fit models to estimate relationships or to make predictions between specified inputs and outputs. Regression Software includes tools with inferential or predictive foci, Bayesian, frequentist, or probability-free Machine Learning (ML) approaches, parametric or or non-parametric approaches, discrete outputs (such as in classification tasks) or continuous outputs, and models and algorithms specific to applications or data such as time series or spatial data. In many cases other standards specific to these subcategories may apply. The following standards are divided among several sub-categories, with each standard prefixed with “RE”. 5.4.1 Input data structures and validation RE1.0 Regression Software should enable models to be specified via a formula interface, unless reasons for not doing so are explicitly documented. RE1.1 Regression Software should document how formula interfaces are converted to matrix representations of input data. See Max Kuhn’s RStudio blog post for examples. RE1.2 Regression Software should document expected format (types or classes) for inputting predictor variables, including descriptions of types or classes which are not accepted; for example, specification that software accepts only numeric inputs in vector or matrix form, or that all inputs must be in data.frame form with both column and row names. RE1.3 Regression Software should transfer all relevant aspects of input data, notably including row and column names, and potentially information from other attributes(), to corresponding aspects of return objects (see RE4, below). RE1.3a Where otherwise relevant information is not transferred, this should be explicitly documented. RE1.4 Regression Software should document any assumptions made with regard to input data; for example distributional assumptions, or assumptions that predictor data have mean values of zero. Implications of violations of these assumptions should be both documented and tested. 5.4.2 Pre-processing and Variable Transformation RE2.0 Regression Software should document any transformations applied to input data, for example conversion of label-values to factor, and should provide ways to explicitly avoid any default transformations (with error or warning conditions where appropriate). RE2.1 Regression Software should implement explicit parameters controlling the processing of missing values, ideally distinguishing NA or NaN values from Inf values (for example, through use of na.omit() and related functions from the stats package). RE2.2 Regression Software should provide different options for processing missing values in predictor and response data. For example, it should be possible to fit a model with no missing predictor data in order to generate values for all associated response points, even where submitted response values may be missing. RE2.3 Where applicable, Regression Software should enable data to be centred (for example, through converting to zero-mean equivalent values; or to z-scores) or offset (for example, to zero-intercept equivalent values) via additional parameters, with the effects of any such parameters clearly documented and tested. RE2.4 Regression Software should implement pre-processing routines to identify whether aspects of input data are perfectly collinear, notably including: RE2.4a Perfect collinearity among predictor variables RE2.4b Perfect collinearity between independent and dependent variables These pre-processing routines should also be tested as described below. 5.4.3 Algorithms The following standards apply to the model fitting algorithms of Regression Software which implements or relies on iterative algorithms which are expected to converge to generate model statistics. Regression Software which implements or relies on iterative convergence algorithms should: RE3.0 Issue appropriate warnings or other diagnostic messages for models which fail to converge. RE3.1 Enable such messages to be optionally suppressed, yet should ensure that the resultant model object nevertheless includes sufficient data to identify lack of convergence. RE3.2 Ensure that convergence thresholds have sensible default values, demonstrated through explicit documentation. RE3.3 Allow explicit setting of convergence thresholds, unless reasons against doing so are explicitly documented. 5.4.4 Return Results RE4.0 Regression Software should return some form of “model” object, generally through using or modifying existing class structures for model objects (such as lm, glm, or model objects from other packages), or creating a new class of model objects. RE4.1 Regression Software may enable an ability to generate a model object without actually fitting values. This may be useful for controlling batch processing of computationally intensive fitting algorithms. 5.4.4.1 Accessor Methods Regression Software should provide functions to access or extract as much of the following kinds of model data as possible or practicable. Access should ideally rely on class-specific methods which extend, or implement otherwise equivalent versions of, the methods from the stats package which are named in parentheses in each of the following standards. Model objects should include, or otherwise enable effectively immediate access to the following descriptors. It is acknowledged that not all regression models can sensibly provide access to these descriptors, yet should include access provisions to all those that are applicable. RE4.2 Model coefficients (via coeff() / coefficients()) RE4.3 Confidence intervals on those coefficients (via confint()) RE4.4 The specification of the model, generally as a formula (via formula()) RE4.5 Numbers of observations submitted to model (via nobs()) RE4.6 The variance-covariance matrix of the model parameters (via vcov()) RE4.7 Where appropriate, convergence statistics Regression Software should provide simple and direct methods to return or otherwise access the following form of data and metadata, where the latter includes information on any transformations which may have been applied to the data prior to submission to modelling routines. RE4.8 Response variables, and associated “metadata” where applicable. RE4.9 Modelled values of response variables. RE4.10 Model Residuals, including sufficient documentation to enable interpretation of residuals, and to enable users to submit residuals to their own tests. RE4.11 Goodness-of-fit and other statistics associated such as effect sizes with model coefficients. RE4.12 Where appropriate, functions used to transform input data, and associated inverse transform functions. Regression software may provide simple and direct methods to return or otherwise access the following: RE4.13 Predictor variables, and associated “metadata” where applicable. 5.4.4.2 Prediction, Extrapolation, and Forecasting Not all regression software is intended to, or can, provide distinct abilities to extrapolate or forecast. Moreover, identifying cases in which a regression model is used to extrapolate or forecast may often be a non-trivial exercise. It may nevertheless be possible, for example when input data used to construct a model are unidimensional, and data on which a prediction is to be based extend beyond the range used to construct the model. Where reasonably unambiguous identification of extrapolation or forecasting using a model is possible, the following standards apply: RE4.14 Where possible, values should also be provided for extrapolation or forecast errors. RE4.15 Sufficient documentation and/or testing should be provided to demonstrate that forecast errors, confidence intervals, or equivalent values increase with forecast horizons. Distinct from extrapolation or forecasting abilities, the following standard applies to regression software which relies on, or otherwise provides abilities to process, categorical grouping variables: RE4.16 Regression Software which models distinct responses for different categorical groups should include the ability to submit new groups to predict() methods. 5.4.4.3 Reporting Return Results RE4.17 Model objects returned by Regression Software should implement or appropriately extend a default print method which provides an on-screen summary of model (input) parameters and (output) coefficients. RE4.18 Regression Software may also implement summary methods for model objects, and in particular should implement distinct summary methods for any cases in which calculation of summary statistics is computationally non-trivial (for example, for bootstrapped estimates of confidence intervals). 5.4.5 Documentation Beyond the general standards for documentation, Regression Software should explicitly describe the following aspects, and ideally provide extended documentation including summary graphical reports of: RE5.0 Scaling relationships between sizes of input data (numbers of observations, with potential extension to numbers of variables/columns) and speed of algorithm. 5.4.6 Visualization RE6.0 Model objects returned by Regression Software (see RE3.0) should have default plot methods, either through explicit implementation, extension of methods for existing model objects, or through ensuring default methods work appropriately. RE6.1 Where the default plot method is NOT a generic plot method dispatched on the class of return objects (that is, through a plot.&lt;myclass&gt; function), that method dispatch should nevertheless exist in order to explicitly direct users to the appropriate function. RE6.2 The default plot method should produce a plot of the fitted values of the model, with optional visualisation of confidence intervals or equivalent. The following standard applies only to software fulfilling RE4.14-4.15, and the conditions described prior to those standards. RE6.3 Where a model object is used to generate a forecast (for example, through a predict() method), the default plot method should provide clear visual distinction between modelled (interpolated) and forecast (extrapolated) values. 5.4.7 Testing Tests for Regression Software should include the following conditions and cases: RE7.0 Tests with noiseless, exact relationships between predictor (independent) data. RE7.0a In particular, these tests should confirm that model fitting is at least as fast or (preferably) faster than testing with equivalent noisy data (see RE2.4a). RE7.1 Tests with noiseless, exact relationships between predictor (independent) and response (dependent) data. RE7.1a In particular, these tests should confirm that model fitting is at least as fast or (preferably) faster than testing with equivalent noisy data (see RE2.4b). 5.5 Dimensionality Reduction, Clustering, and Unsupervised Learning This document details standards for Dimensionality Reduction, Clustering, and Unsupervised Learning Software – referred to from here on for simplicity as “Unsupervised Learning Software”. Software in this category is distinguished from Regression Software though the latter aiming to construct or analyse one or more mappings between two defined data sets (for example, a set of “independent” data, \\(X\\), and a set of “dependent” data, “Y”), whereas Unsupervised Learning Software aims to construct or analyse one or more mappings between a defined set of input or independent data, and a second set of “output” data which are not necessarily known or given prior to the analysis. A key distinction in Unsupervised Learning Software and Algorithms is between that for which output data represent (generally numerical) transformations of the input data set, and that for which output data are discrete labels applied to the input data. Examples of the former type include dimensionality reduction and ordination software and algorithms, and examples of the latter include clustering and discrete partitioning software and algorithms. 5.5.1 Input Data Structures and Validation UL1.0 Unsupervised Learning Software should explicitly document expected format (types or classes) for input data, including descriptions of types or classes which are not accepted; for example, specification that software accepts only numeric inputs in vector or matrix form, or that all inputs must be in data.frame form with both column and row names. UL1.1 Unsupervised Learning Software should provide distinct sub-routines to assert that all input data is of the expected form, and issue informative error messages when incompatible data are submitted. The following code demonstrates an example of a routine from the base stats package which fails to meet this standard. d &lt;- dist(USArrests) # example from help file for &#39;hclust&#39; function hc &lt;- hclust(d) # okay hc &lt;- hclust(as.matrix(d)) ## Error in if (is.na(n) || n &gt; 65536L) stop(&quot;size cannot be NA nor exceed 65536&quot;): missing value where TRUE/FALSE needed The latter fails, yet issues an uninformative error message that clearly indicates a failure to provide sufficient checks on the class of input data. UL1.2 Unsupervised learning which uses row or column names to label output objects should assert that input data have non-default row or column names, and issue an informative message when these are not provided. (Such messages need not necessarily be provided by default, but should at least be optionally available.) The following code provides simple examples of checks whether row and column names appear to have generic default values. x &lt;- data.frame(matrix(1:10, ncol = 2)) x ## X1 X2 ## 1 1 6 ## 2 2 7 ## 3 3 8 ## 4 4 9 ## 5 5 10 Generic row names are almost always simple integer sequences, which the following condition confirms. identical(rownames(x), as.character(seq(nrow(x)))) ## [1] TRUE Generic column names may come in a variety of formats. The following code uses a grep expression to match any number of characters plus an optional leading zero followed by a generic sequence of column numbers, appropriate for matching column names produced by generic construction of data.frame objects. all(vapply(seq(ncol(x)), function(i) { grepl(paste0(&quot;[[:alpha:]]0?&quot;, i), colnames(x) [i]) }, logical(1))) ## [1] TRUE Messages should be issued in both of these cases. The following code illustrates that the hclust function does not implement any such checks or assertions, rather it silently returns an object with default labels. u &lt;- USArrests rownames(u) &lt;- seq(nrow(u)) hc &lt;- hclust(dist(u)) head(hc$labels) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; UL1.3 Unsupervised Learning Software should transfer all relevant aspects of input data, notably including row and column names, and potentially information from other attributes(), to corresponding aspects of return objects. UL1.3a Where otherwise relevant information is not transferred, this should be explicitly documented. An example of a function according with UL1.3 is stats::cutree() hc &lt;- hclust(dist(USArrests)) head(cutree(hc, 10)) ## Alabama Alaska Arizona Arkansas California Colorado ## 1 2 3 4 5 4 The row names of USArrests are transferred to the output object. In contrast, some routines from the cluster package do not comply with this standard: library(cluster) ac &lt;- agnes(USArrests) # agglomerative nesting head(cutree(ac, 10)) ## [1] 1 2 3 4 3 4 The case labels are not appropriately carried through to the object returned by agnes() to enable them to be transferred within cutree(). (The labels are transferred to the object returned by agnes, just not in a way that enables cutree to inherit them.) UL1.4 Unsupervised Learning Software should explicitly document whether input data may include missing values. UL1.5 Functions in Unsupervised Learning Software which do not admit input data with missing values should provide informative error messages when data with missing values are submitted. UL1.6 Unsupervised Learning Software should document any assumptions made with regard to input data; for example assumptions about distributional forms or locations (such as that data are centred or on approximately equivalent distributional scales). Implications of violations of these assumptions should be both documented and tested, in particular: UL1.6a Software which responds qualitatively differently to input data which has components on markedly different scales should explicitly document such differences, and implications of submitting such data. UL1.6b Examples or other documentation should not use scale() or equivalent transformations without explaining why scale is applied, and explicitly illustrating and contrasting the consequences of not applying such transformations. 5.5.2 Pre-processing and Variable Transformation UL2.0 Routines likely to give unreliable or irreproducible results in response to violations of assumptions regarding input data (see UL1.6) should implement pre-processing steps to diagnose potential violations, and issue appropriately informative messages, and/or include parameters to enable suitable transformations to be applied (such as the center and scale. parameters of the stats::prcomp() function). UL2.1 Unsupervised Learning Software should document any transformations applied to input data, for example conversion of label-values to factor, and should provide ways to explicitly avoid any default transformations (with error or warning conditions where appropriate). UL2.2 For Unsupervised Learning Software which accepts missing values in input data, functions should implement explicit parameters controlling the processing of missing values, ideally distinguishing NA or NaN values from Inf values (for example, through use of na.omit() and related functions from the stats package). UL2.3 Unsupervised Learning Software should implement pre-processing routines to identify whether aspects of input data are perfectly collinear. 5.5.3 Algorithms 5.5.3.1 Labelling UL3.1 Algorithms which apply sequential labels to input data (such as clustering or partitioning algorithms) should ensure that the sequence follows decreasing group sizes (so labels of “1”, “a”, or “A” describe the largest group, “2”, “b”, or “B” the second largest, and so on.) Note that the stats::cutree() function does not accord with this standard: hc &lt;- hclust(dist(USArrests)) table(cutree(hc, k = 10)) ## ## 1 2 3 4 5 6 7 8 9 10 ## 3 3 3 6 5 10 2 5 5 8 The cutree() function applies arbitrary integer labels to the groups, yet the order of labels is not related to the order of group sizes. UL3.2 Dimensionality reduction or equivalent algorithms which label dimensions should ensure that that sequences of labels follows decreasing “importance” (for example, eigenvalues or variance contributions). The stats::prcomp function accords with this standard: z &lt;- prcomp(eurodist, rank = 5) # return maximum of 5 components summary(z) ## Importance of first k=5 (out of 21) components: ## PC1 PC2 PC3 PC4 PC5 ## Standard deviation 2529.6298 2157.3434 1459.4839 551.68183 369.10901 ## Proportion of Variance 0.4591 0.3339 0.1528 0.02184 0.00977 ## Cumulative Proportion 0.4591 0.7930 0.9458 0.96764 0.97741 The proportion of variance explained by each component decreasing with increasing numeric labelling of the components. UL3.3 Unsupervised Learning Software for which input data does not generally include labels (such as array-like data with no row names) should provide an additional parameter to enable cases to be labelled. 5.5.3.2 Prediction UL3.4 Where applicable, Unsupervised Learning Software should implement routines to predict the properties (such as numerical ordinates, or cluster memberships) of additional new data without re-running the entire algorithm. While many algorithms such as Hierarchical clustering can not (readily) be used to predict memberships of new data, other algorithms can nevertheless be applied to perform this task. The following demonstrates how the output of stats::hclust can be used to predict membership of new data using the class:knn() function. (This is intended to illustrate only one of many possible approaches.) library(class) ## ## Attaching package: &#39;class&#39; ## The following object is masked from &#39;package:igraph&#39;: ## ## knn hc &lt;- hclust(dist(iris [, -5])) groups &lt;- cutree(hc, k = 3) # function to randomly select part of a data.frame and # add some randomness sample_df &lt;- function(x, n = 5) { x [sample(nrow(x), size = n), ] + runif(ncol(x) * n) } iris_new &lt;- sample_df(iris [, -5], n = 5) # use knn to predict membership of those new points: knnClust &lt;- knn(train = iris [, -5], test = iris_new, k = 1, cl = groups) knnClust ## [1] 2 2 2 2 1 ## Levels: 1 2 3 The stats::prcomp() function implements its own predict() method which conforms to this standard: res &lt;- prcomp(USArrests) arrests_new &lt;- sample_df(USArrests, n = 5) predict(res, newdata = arrests_new) ## PC1 PC2 PC3 PC4 ## Minnesota -97.95811 5.823427 0.7498986 0.3587406 ## Washington -24.57368 10.200141 5.2021127 2.2892112 ## Utah -49.26660 17.963733 2.6560388 1.3241555 ## Indiana -56.69576 3.794480 4.4823340 -2.2705452 ## Pennsylvania -64.13197 9.495397 -2.3875273 -1.9402747 5.5.3.3 Group Distributions and Associated Statistics Many unsupervised learning algorithms serve to label, categorise, or partition data. Software which performs any of these tasks will commonly output some kind of labelling or grouping schemes. The above example of principal components illustrates that the return object records the standard deviations associated with each component: res &lt;- prcomp(USArrests) print(res) ## Standard deviations (1, .., p=4): ## [1] 83.732400 14.212402 6.489426 2.482790 ## ## Rotation (n x k) = (4 x 4): ## PC1 PC2 PC3 PC4 ## Murder 0.04170432 -0.04482166 0.07989066 -0.99492173 ## Assault 0.99522128 -0.05876003 -0.06756974 0.03893830 ## UrbanPop 0.04633575 0.97685748 -0.20054629 -0.05816914 ## Rape 0.07515550 0.20071807 0.97408059 0.07232502 summary(res) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 83.7324 14.21240 6.4894 2.48279 ## Proportion of Variance 0.9655 0.02782 0.0058 0.00085 ## Cumulative Proportion 0.9655 0.99335 0.9991 1.00000 Such output accords with the following standard: UL3.5 Objects returned from Unsupervised Learning Software which labels, categorise, or partitions data into discrete groups should include quantitative information on intra-group variances or equivalent, as well as on inter-group relationships where applicable. (In lieu of directly including such information in return objects, additional methods may be developed to provide access.) The above example of principal components is one where there are no inter-group relationships, and so that standard is fulfilled by providing information on intra-group variances alone. Discrete clustering algorithms, in contrast, yield results for which inter-group relationships are meaningful, and such relationships can generally be meaningfully provided. The hclust() routine, like many clustering routines, simply returns a scheme for devising an arbitrary number of clusters, and so can not meaningfully provide variances or relationships between such. The cutree() function, however, does yield defined numbers of clusters, yet devoid of any quantitative information on variances or equivalent. res &lt;- hclust(dist(USArrests)) str(cutree(res, k = 5)) ## Named int [1:50] 1 1 1 2 1 2 3 1 4 2 ... ## - attr(*, &quot;names&quot;)= chr [1:50] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ... Compare that with the output of a largely equivalent routine, the clara() function from the cluster package. library(cluster) cl &lt;- clara(USArrests, k = 10) # direct clustering into specified number of clusters cl$clusinfo ## size max_diss av_diss isolation ## [1,] 4 24.708298 14.284874 1.4837745 ## [2,] 6 28.857755 16.759943 1.7329563 ## [3,] 6 44.640565 23.718040 0.9677229 ## [4,] 6 28.005892 17.382196 0.8442061 ## [5,] 6 15.901258 9.363471 1.1037219 ## [6,] 7 29.407822 14.817031 0.9080598 ## [7,] 4 11.764353 6.781659 0.8165753 ## [8,] 3 8.766984 5.768183 0.3547323 ## [9,] 3 18.848077 10.101505 0.7176276 ## [10,] 5 16.477257 8.468541 0.6273603 That object contains information on dissimilarities between each observation and cluster medoids, which in the context of UL3.4 is “information on intra-group variances or equivalent”. Moreover, inter-group information is also available as the “silhouette” of the clustering scheme. 5.5.4 Return Results UL4.0 Unsupervised Learning Software should return some form of “model” object, generally through using or modifying existing class structures for model objects, or creating a new class of model objects. UL4.1 Unsupervised Learning Software may enable an ability to generate a model object without actually fitting values. This may be useful for controlling batch processing of computationally intensive fitting algorithms. UL4.2 The return object from Unsupervised Learning Software should include, or otherwise enable immediate extraction of, all parameters used to control the algorithm used. 5.5.4.1 Reporting Return Results UL4.2 Model objects returned by Unsupervised Learning Software should implement or appropriately extend a default print method which provides an on-screen summary of model (input) parameters and methods used to generate results. The print method may also summarise statistical aspects of the output data or results. UL4.2a The default print method should always ensure only a restricted number of rows of any result matrices or equivalent are printed to the screen. The prcomp objects returned from the function of the same name include potential large matrices of component coordinates which are by default printed in their entirety to the screen. This is because the default print behaviour for most tabular objects in R (matrix, data.frame, and objects from the Matrix package, for example) is to print objects in their entirety (limited only by such options as getOption(\"max.print\"), which determines maximal numbers of printed objects, such as lines of data.frame objects). Such default behaviour ought be avoided, particularly in Unsupervised Learning Software which commonly returns objects containing large numbers of numeric entries. UL4.3 Unsupervised Learning Software should also implement summary methods for model objects which should summarise the primary statistics used in generating the model (such as numbers of observations, parameters of methods applied). The summary method may also provide summary statistics from the resultant model. 5.5.5 Documentation 5.5.6 Visualization UL6.0 Objects returned by Unsupervised Learning Software should have default plot methods, either through explicit implementation, extension of methods for existing model objects, through ensuring default methods work appropriately, or through explicit reference to helper packages such as factoextra and associated functions. UL6.1 Where the default plot method is NOT a generic plot method dispatched on the class of return objects (that is, through a plot.&lt;myclass&gt; function), that method dispatch should nevertheless exist in order to explicitly direct users to the appropriate function. UL6.2 Where default plot methods include labelling components of return objects (such as cluster labels), routines should ensure that labels are automatically placed to ensure readability, and/or that appropriate diagnostic messages are issued where readability is likely to be compromised (for example, through attempting to place too many labels). 5.5.7 Testing 5.6 Exploratory Data Analysis Exploration is a part of all data analyses, and Exploratory Data Analysis (EDA) is not something that is entered into and exited from at some point prior to “real” analysis. Exploratory Analyses are also not strictly limited to Data, but may extend to exploration of Models of those data. The category could thus equally be termed, “Exploratory Data and Model Analysis”, yet we opt to utilise the standard acronym of EDA in this document. EDA is nevertheless somewhat different to many other categories included within rOpenSci’s program for peer-reviewing statistical software. Primary differences include: EDA software often has a strong focus upon visualization, which is a category which we have otherwise explicitly excluded from the scope of the project at the present stage. The assessment of EDA software requires addressing more general questions than software in most other categories, notably including the important question of intended audience(s). The following standards are accordingly somewhat differently structured than equivalent standards developed to date for other categories, particularly through being more qualitative and abstract. In particular, while documentation is an important component of standards for all categories, clear and instructive documentation is of paramount importance for EDA Software, and so warrants its own sub-section within this document. 5.6.1 Documentation Standards The following refer to Primary Documentation, implying in main package README or vignette(s), and Secondary Documentation, implying function-level documentation. The Primary Documentation (README and/or vignette(s)) of EDA software should: EA1.0 Identify one or more target audiences for whom the software is intended EA1.1 Identify the kinds of data the software is capable of analysing (see Kinds of Data* below).* EA1.2 Identify the kinds of questions the software is intended to help explore; for example, are these questions: inferential? predictive? associative? causal? (or other modes of statistical enquiry?) The Secondary Documentation (within individual functions) of EDA software should: EA1.3 Identify the kinds of data each function is intended to accept as input 5.6.2 Input Data A further primary difference of EDA software from that of our other categories is that input data for statistical software may be generally presumed of one or more specific types, whereas EDA software often accepts data of more general and varied types. EDA software should aim to accept and appropriately transform as many diverse kinds of input data as possible, through addressing the following standards, considered in terms of the two cases of input data in uni- and multi-variate form. All of the general standards for kinds of input (G2.0 - G2.7) apply to input data for EDA Software. 5.6.2.1 Index Columns The following standards refer to an index column, which is understood to imply an explicitly named or identified column which can be used to provide a unique index index into any and all rows of that table. Index columns ensure the universal applicability of standard table join operations, such as those implemented via the dplyr package. EA2.0 EDA Software which accepts standard tabular data and implements or relies upon extensive table filter and join operations should utilise an index column system EA2.1 All values in an index column must be unique, and this uniqueness should be affirmed as a pre-processing step for all input data. EA2.2 Index columns should be explicitly identified, either: EA2.2a by using an appropriate class system, or EA2.2b through setting an attribute on a table, x, of attr(x, \"index\") &lt;- &lt;index_col_name&gt;. For EDA software which either implements custom classes or explicitly sets attributes specifying index columns, these attributes should be used as the basis of all table join operations, and in particular: EA2.3 Table join operations should not be based on any assumed variable or column names 5.6.2.2 Multi-tabular input EDA software designed to accept multi-tabular input should: EA2.4 Use and demand an explicit class system for such input (for example, via the DM package). EA2.5 Ensure all individual tables follow the above standards for Index Columns 5.6.2.3 Classes and Sub-Classes Classes are understood here to be the classes define single input objects, while Sub-Classes refer to the class definitions of components of input objects (for example, of columns of an input data.frame). EDA software which is intended to receive input in general vector formats (see Uni-variate Input section of General Standards) should ensure: EA2.6 Routines appropriately process vector input of custom classes, including those which do not inherit from the vector class EA2.7 Routines should appropriately process vector data regardless of additional attributes The following code illustrates some ways by which “metadata” defining classes and additional attributes associated with a standard vector object may by modified. x &lt;- 1:10 class (x) &lt;- &quot;notvector&quot; attr (x, &quot;extra_attribute&quot;) &lt;- &quot;another attribute&quot; attr (x, &quot;vector attribute&quot;) &lt;- runif (5) attributes (x) #&gt; $class #&gt; [1] &quot;notvector&quot; #&gt; #&gt; $extra_attribute #&gt; [1] &quot;another attribute&quot; #&gt; #&gt; $`vector attribute` #&gt; [1] 0.03521663 0.49418081 0.60129563 0.75804346 0.16073301 All statistical software should appropriately deal with such input data, as exemplified by the storage.mode(), length(), and sum() functions of the base package, which return the appropriate values regardless of redefinition of class or additional attributes. storage.mode (x) #&gt; [1] &quot;integer&quot; length (x) #&gt; [1] 10 sum (x) #&gt; [1] 55 storage.mode (sum (x)) #&gt; [1] &quot;integer&quot; Tabular inputs in data.frame class may contain columns which are themselves defined by custom classes, and which possess additional attributes. EDA Software which accepts tabular inputs should accordingly ensure: EA2.8 EDA routines appropriately process tabular input of custom classes, ideally by means of a single pre-processing routine which converts tabular input to some standard form subsequently passed to all analytic routines. EA2.9 EDA routines accept and appropriately process tabular input in which individual columns may be of custom sub-classes including additional attributes. 5.6.3 Analytic Algorithms (There are no specific standards for analytic algorithms in EDA Software.) 5.6.4 Return Results / Output Data EA4.0 EDA Software should ensure all return results have types which are consistent with input types. For example, sum, min, or max values applied to integer-type vectors should return integer values, while mean or var will generally return numeric types. EA4.1 EDA Software should implement parameters to enable explicit control of numeric precision EA4.2 The primary routines of EDA Software should return objects for which default print and plot methods give sensible results. Default summary methods may also be implemented. 5.6.5 Visualization and Summary Output Visualization commonly represents one of the primary functions of EDA Software, and thus visualization output is given greater consideration in this category than in other categories in which visualization may nevertheless play an important role. In particular, one component of this sub-category is Summary Output, taken to refer to all forms of screen-based output beyond conventional graphical output, including tabular and other text-based forms. Standards for visualization itself are considered in the two primary sub-categories of static and dynamic visualization, where the latter includes interactive visualization. Prior to these individual sub-categories, we consider a few standards applicable to visualization in general, whether static or dynamic. EA5.0 Graphical presentation in EDA software should be as accessible as possible or practicable. In particular, EDA software should consider accessibility in terms of: EA5.0a *Typeface sizes should default to sizes which explicitly enhance accessibility EA5.0b Default colour schemes should be carefully constructed to ensure accessibility.* EA5.1 Any explicit specifications of typefaces which override default values should consider accessibility 5.6.5.1 Summary and Screen-based Output EA5.2 Screen-based output should never rely on default print formatting of numeric types, rather should also use some version of round(., digits), formatC, sprintf, or similar functions for numeric formatting according the parameter described in EDA4.2. EA5.3 Column-based summary statistics should always indicate the storage.mode, class, or equivalent defining attribute of each column (as, for example, implemented in the default print.tibble method). 5.6.5.2 General Standards for Visualization (Static and Dynamic) EA5.4 All visualisations should include units on all axes, with sensibly rounded values (for example, as produced by the pretty() function). 5.6.5.3 Dynamic Visualization Dynamic visualization routines are commonly implemented as interfaces to javascript routines. Unless routines have been explicitly developed as an internal part of an R package, standards shall not be considered to apply to the code itself, rather only to decisions present as user-controlled parameters exposed within the R environment. That said, one standard may nevertheless be applied, with an aim to minimise EA5.5 Any packages which internally bundle libraries used for dynamic visualization and which are also bundled in other, pre-existing R packages, should explain the necessity and advantage of re-bundling that library. 5.7 Time Series Software Time series software is presumed to perform one or more of the following steps: Accept and validate input data Apply data transformation and pre-processing steps Apply one or more analytic algorithms Return the result of that algorithmic application Offer additional functionality such as printing or summarising return results This document details standards for each of these steps, each prefixed with “TS”. 5.7.1 Input data structures and validation Input validation is an important software task, and an important part of our standards. While there are many ways to approach validation, the class systems of R offer a particularly convenient and effective means. For Time Series Software in particular, a range of class systems have been developed, for which we refer to the section “Time Series Classes” in the CRAN Task view on Time Series Analysis\", and the class-conversion package tsbox. Software which uses and relies on defined classes can often validate input through affirming appropriate class(es). Software which does not use or rely on class systems will generally need specific routines to validate input data structures. In particular, because of the long history of time series software in R, and the variety of class systems for representing time series data, new time series package should accept as many different classes of input as possible by according with the following standards: TS1.0 Time Series Software should explicitly document the types and classes of input data able to be passed to each function. TS1.1 Time Series Software should accept input data in as many time series specific classes as possible. TS1.2 Time Series Software should implement validation routines to confirm that inputs are of acceptable classes (or represented in otherwise appropriate ways for software which does not use class systems). TS1.3 Time Series Software should implement a single pre-processing routine to validate input data, and to appropriately transform it to a single uniform type to be passed to all subsequent data-processing functions (the tsbox package provides one convenient approach for this). TS1.4 The pre-processing function described above should maintain all time- or date-based components or attributes of input data. For Time Series Software which relies on or implements custom classes or types for representing time-series data, the following standards should be adhered to: TS1.5 The software should ensure strict ordering of the time, frequency, or equivalent ordering index variable. TS1.6 Any violations of ordering should be caught in the pre-processing stages of all functions. 5.7.1.1 Time Intervals and Relative Time While most common packages and classes for time series data assume absolute temporal scales such as those represented in POSIX classes for dates or times, time series may also be quantified on relative scales where the temporal index variable quantifies intervals rather than absolute times or dates. Many analytic routines which accept time series inputs in absolute form are also appropriately applied to analogous data in relative form, and thus many packages should accept time series inputs both in absolute and relative forms. Software which can or should accept times series inputs in relative form should: TS1.7 Accept inputs defined via the units package for attributing SI units to R vectors. TS1.8 Where time intervals or periods may be days or months, be explicit about the system used to represent such, particularly regarding whether a calendar system is used, or whether a year is presumed to have 365 days, 365.2422 days, or some other value. 5.7.2 Pre-processing and Variable Transformation 5.7.2.1 Missing Data One critical pre-processing step for Time Series Software is the appropriate handling of missing data. It is convenient to distinguish between implicit and explicit missing data. For regular time series, explicit missing data may be represented by NA values, while for irregular time series, implicit missing data may be represented by missing rows. The difference is demonstrated in the following table. Missing Values Time value 08:43 0.71 08:44 NA 08:45 0.28 08:47 0.34 08:48 0.07 The value for 08:46 is implicitly missing, while the value for 08:44 is explicitly missing. These two forms of missingness may connote different things, and may require different forms of pre-processing. With this in mind, the following standards apply: TS2.0 Appropriate checks for missing data, and associated transformation routines, should be performed as part of initial pre-processing prior to passing data to analytic algorithms. TS2.1 Time Series Software which presumes or requires regular data should only allow explicit* missing values, and should issue appropriate diagnostic messages, potentially including errors, in response to any implicit missing values.* TS2.2 Where possible, all functions should provide options for users to specify how to handle missing data, with options minimally including: TS2.2a error on missing data. TS2.2b warn or ignore missing data, and proceed to analyse irregular data, ensuring that results from function calls with regular yet missing data return identical values to submitting equivalent irregular data with no missing values. TS2.2c replace missing data with appropriately imputed values. TS2.3 Functions should never assume non-missingness, and should never pass data with potential missing values to any base routines with default na.rm = FALSE-type parameters (such as mean(), sd() or var()). 5.7.2.2 Stationarity Time Series Software should explicitly document assumptions or requirements made with respect to the stationarity or otherwise of all input data. In particular, any (sub-)functions which assume or rely on stationarity should: TS2.4 Consider stationarity of all relevant moments - typically first (mean) and second (variance) order, or otherwise document why such consideration may be restricted to lower orders only. TS2.5 Explicitly document all assumptions and/or requirements of stationarity TS2.6 Implement appropriate checks for all relevant forms of stationarity, and either: TS2.6a issue diagnostic messages or warnings; or TS2.6b enable or advise on appropriate transformations to ensure stationarity. The two options in the last point (TS2.6b) respectively translate to enabling transformations to ensure stationarity by providing appropriate routines, generally triggered by some function parameter, or advising on appropriate transformations, for example by directing users to additional functions able to implement appropriate transformations. 5.7.2.3 Covariance Matrices Where covariance matrices are constructed or otherwise used within or as input to functions, they should: TS2.7 Incorporate a system to ensure that both row and column orders follow the same ordering as the underlying time series data. This may, for example, be done by including the index attribute of the time series data as an attribute of the covariance matrix. TS2.8 Where applicable, covariance matrices should also include specification of appropriate units. 5.7.3 Analytic Algorithms Analytic algorithms are considered here to reflect the core analytic components of Time Series Software. These may be many and varied, and we explicitly consider only a small subset here. 5.7.3.1 Forecasting Statistical software which implements forecasting routines should: TS3.0 Provide tests to demonstrate at least one case in which errors widen appropriately with forecast horizon. TS3.1 If possible, provide at least one test which violates TS3.0 TS3.2 Document the general drivers of forecast errors or horizons, as demonstrated via the particular cases of TS3.0 and TS3.1 TS3.3 Either: TS3.3a Document, preferable via an example, how to trim forecast values based on a specified error margin or equivalent; or TS3.3b Provide an explicit mechanism to trim forecast values to a specified error margin, either via an explicit post-processing function, or via an input parameter to a primary analytic function. 5.7.4 Return Results For (functions within) Time Series Software which return time series data: TS4.0 Return values should either: TS4.0a Be in same class as input data, for example by using the tsbox package to re-convert from standard internal format (see 1.4, above); or TS4.0b Be in a unique, preferably class-defined, format. TS4.1 Any units included as attributes of input data should also be included within return values. TS4.2 The type and class of all return values should be explicitly documented. For (functions within) Time Series Software which return data other than direct series: TS4.3 Return values should explicitly include all appropriate units and/or time scales 5.7.4.1 Data Transformation Time Series Software which internally implements routines for transforming data to achieve stationarity and which returns forecast values should: TS4.4 Document the effect of any such transformations on forecast data, including potential effects on both first- and second-order estimates. TS4.5 In decreasing order of preference, either: TS4.5a Provide explicit routines or options to back-transform data commensurate with original, non-stationary input data TS4.5b Demonstrate how data may be back-transformed to a form commensurate with original, non-stationary input data. TS4.5c Document associated limitations on forecast values 5.7.4.2 Forecasting Where Time Series Software implements or otherwise enables forecasting abilities, it should return one of the following three kinds of information. These are presented in decreasing order of preference, such that software should strive to return the first kind of object, failing that the second, and only the third as a last resort. TS4.6 Time Series Software which implements or otherwise enables forecasting should return either: TS4.6a A distribution object, for example via one of the many packages described in the CRAN Task View on Probability Distributions (or the new distributional package as used in the fable package for time-series forecasting). TS4.6b For each variable to be forecast, predicted values equivalent to first- and second-order moments (for example, mean and standard error values). TS4.6c Some more general indication of error involved with forecast estimates. Beyond these particular standards for return objects, Time Series Software which implements or otherwise enables forecasting should: TS4.7 Ensure that forecast (modelled) values are clearly distinguished from observed (model or input) values, either (in this case in no order of preference) by TS4.7a Returning forecast values alone TS4.7b Returning distinct list items for model and forecast values TS4.7c Combining model and forecast values into a single return object with an appropriate additional column clearly distinguishing the two kinds of data. 5.7.5 Visualization Time Series Software should: TS5.0 Implement default plot methods for any implemented class system. TS5.1 When representing results in temporal domain(s), ensure that one axis is clearly labelled “time” (or equivalent), with continuous units. TS5.2 Default to placing the “time” (or equivalent) variable on the horizontal axis. TS5.3 Ensure that units of the time, frequency, or index variable are printed by default on the axis. TS5.4 For frequency visualization, abscissa spanning \\([-\\pi, \\pi]\\) should be avoided in favour positive units of \\([0, 2\\pi]\\) or \\([0, 0.5]\\), in all cases with appropriate additional explanation of units. TS5.5 Provide options to determine whether plots of data with missing values should generate continuous or broken lines. For the results of forecast operations, Time Series Software should TS5.6 By default indicate distributional limits of forecast on plot TS5.7 By default include model (input) values in plot, as well as forecast (output) values TS5.8 By default provide clear visual distinction between model (input) values and forecast (output) values. 5.8 Machine Learning Software R has an extensive and diverse ecosystem of Machine Learning (ML) software which is very well described in the corresponding CRAN Task View. Unlike most other categories of statistical software considered here, the primary distinguishing feature of ML software is not (necessarily or directly) algorithmic, rather pertains to a workflow typical of machine learning tasks. In particular, we consider ML software to approach data analysis via the two primary steps of: Passing a set of training data to an algorithm in order to generate a candidate mapping between that data and some form of pre-specified output or response variable. Such mappings will be referred to here as “models”, with a single analysis of a single set of training data generating one model. Passing a set of test data to the model(s) generated by the first step in order to derive some measure of predictive accuracy for that model. A single ML task generally yields two distinct outputs: The model derived in the first of the previous steps; and Associated statistics of model performance (as evaluated within the context of the test data used to assess that performance). A Machine Learning Workflow Given those initial considerations, we now attempt the difficult task of envisioning a typical standard workflow for inherently diverse ML software. The following workflow ought to be considered an “extensive” workflow, with shorter versions, and correspondingly more restricted sets of standards, possible dependent upon envisioned areas of application. For example, the workflow presumes input data to be too large to be stored as a single entity in local memory. Adaptation to situations in which all training data can be loaded into memory may mean that some of the following workflow stages, and therefore corresponding standards, may not apply. Just as typical workflows are potentially very diverse, so are outputs of ML software, which depend on areas of application and intended purpose of software. The following refers to the “desired output” of ML software, a phrase which is intentionally left non-specific, but which it intended to connote any and all forms of “response variable” and other “pre-specified outputs” such as categorical labels or validation data, along with outputs which may not necessarily be able to be pre-specified in simple uni- or multi-variate form, such as measures of distance between sets of training and validation data. Such “desired outputs” are presumed to be quantified in terms of a “loss” or “cost” function (hereafter, simply “loss function”) quantifying some measure of distance between a model estimate (resulting from applying the model to one or more components of a training data set) and a pre-defined “valid” output (during training), or a test data set (following training). Given the foregoing considerations, we consider a typical ML workflow to progress through (at least some of) the following steps: Input Data Specification Obtain a local copy of input data, often as multiple objects (either on-disk or in memory) in some suitably structured form such as in a series of sub-directories or accompanied by additional data defining the structural properties of input objects. Regardless of form, multiple objects are commonly given generic labels which distinguish between training and test data, along with optional additional categories and labels such as validation data used, for example, to determine accuracy of models applied to training data yet prior to testing. Pre-Processing Define transformations of input data, including but not restricted to, broadcasting dimensions (as defined below) and standardising data ranges (typically to defined values of mean and standard deviation). Model and Algorithm Specification Specify the model and associated processes which will be applied to map the input data on to the desired output. This step minimally includes the following distinct stages (generally in no particular order): Specify the kind of model which will be applied to the training data. ML software often allows the use of pre-trained models, in which case this this step includes downloading or otherwise obtaining a pre-trained model, along with specification of which aspects of those models are to be modified through application to a particular set of training and validation data. Specify the kind of algorithm which will be used to explore the search space (for example some kind of gradient descent algorithm), along with parameters controlling how that algorithm will be applied (for example a learning rate, as defined above). Specify the kind of loss function will be used to quantify distance between model estimates and desired output. Model Training Apply the specified model to the training data to generate a series of estimates from the specified loss function. This stage may also include specifying parameters such as stopping or exit criteria, and parameters controlling batch processing of input data. Moreover, this stage may involve retaining some of the following additional data: Potential “pre-processing” stages such as initial estimates of optimal learning rates (see above). Details of summaries of actual paths taken through the search space towards convergence on local or global minimum. Model Output and Performance Measure the performance of the trained model when applied to the test data set, generally requiring the specification of a metric of model performance or accuracy. Importantly, ML workflows may be partly iterative. This may in turn potentially confound distinctions between training and test data, and accordingly confound expectations commonly placed upon statistical analyses of statistical independence of response variables. ML routines such as cross-validation repeatedly (re-)partition data between training and test sets. Resultant models can then not be considered to have been developed through application to any single set of truly “independent” data. In the context of the standards that follow, these considerations admit a potential lack of clarity in any notional categorical distinction between training and test data, and between model specification and training. The preceding workflow mentioned a couple of concepts the definitions of which may be seen by clicking on the corresponding items below. Following that, we proceed to standards for ML software, enumerated and developed with reference to the preceding workflow steps. As described above, these steps may not be applicable to all ML software, and so all of the following standards should be considered to be conditioned on “where applicable.” In order that the following standards initially adhere to the enumeration of workflow steps given above, more general standards pertaining to aspects such as documentation and testing are given following the initial five “workflow” standards. Click for a definition of broadcasting, referred to in Step 2, above. The following definition comes from a vignette for the rray package named Broadcasting. Broadcasting is, “repeating the dimensions of one object to match the dimensions of another.” This concept runs counter to aspects of standards in other categories, which often suggest that functions should error when passed input objects which do not have commensurate dimensions. Broadcasting is a pre-processing step which enables objects with incommensurate dimensions to be dimensionally reconciled. The following demonstration is taken directly from the rray package (which is not currently on CRAN). library (rray) a &lt;- array(c(1, 2), dim = c(2, 1)) b &lt;- array(c(3, 4), dim = c(1, 2)) # rbind (a, b) # error! rray_bind (a, b, .axis = 1) #&gt; [,1] [,2] #&gt; [1,] 1 1 #&gt; [2,] 2 2 #&gt; [3,] 3 4 rray_bind (a, b, .axis = 2) #&gt; [,1] [,2] [,3] #&gt; [1,] 1 3 4 #&gt; [2,] 2 3 4 Broadcasting is commonly employed in ML software because it enables ML operations to be implemented on objects with incommensurate dimensions. One example is image analysis, in which training data may all be dimensionally commensurate, yet test images may have different dimensions. Broadcasting allows data to be submitted to ML routines regardless of potentially incommensurate dimensions. Click for a definition of learning rate, referred to in Step 5, above. Learning Rate (generally) determines the step size used to search for local optima as a fraction of the local gradient. This parameter is particularly important for training ML algorithms like neural networks, the results of which can be very sensitive to variations in learning rates. A useful overview of the importance of learning rates, and a useful approach to automatically determining appropriate values, is given in this blog post. Partly because of widespread and current relevance, the category of Machine Learning software is one for which there have been other notable attempts to develop standards. A particularly useful reference is the MLPerf organization which, among other activities, hosts several github repositories providing reference datasets and benchmark conditions for comparing performance aspects of ML software. While such reference or benchmark standards are not explicitly referred to in the current version of the following standards, we expect them to be gradually adapted and incorporated as we start to apply and refine our standards in application to software submitted to our review system. 5.8.1 Input Data Specification Many of the following standards refer to the labelling of input data as “testing” or “training” data, along with potentially additional labels such as “validation” data. In regard to such labelling, the following two standards apply, ML1.0 Documentation should make a clear conceptual distinction between training and test data (even where such may ultimately be confounded as described above.) ML1.0a Where these terms are ultimately eschewed, these should nevertheless be used in initial documentation, along with clear explanation of, and justification for, alternative terminology. ML1.1 Absent clear justification for alternative design decisions, input data should be expected to be labelled “test”, “training”, and, where applicable, “validation” data. ML1.1a The presence and use of these labels should be explicitly confirmed via pre-processing steps (and tested in accordance with ML7.0, below). ML1.1b Matches to expected labels should be case-insensitive and based on partial matching such that, for example, “Test”, “test”, or “testing” should all suffice. The following three standards (ML1.2–ML1.4) represent three possible design intentions for ML software. Only one of these three will generally be applicable to any one piece of software, although it is nevertheless possible that more than one of these standards may apply. The first of these three standards applies to ML software which is intended to process, or capable of processing, input data as a single (generally tabular) object. ML1.2 Training and test data sets for ML software should be able to be input as a single, generally tabular, data object, with the training and test data distinguished either by A specified variable containing, for example, TRUE/FALSE or 0/1 values, or which uses some other system such as missing (NA) values to denote test data); and/or An additional parameter designating case or row numbers, or labels of test data. The second of these three standards applies to ML software which is intended to process, or capable of processing, input data represented as multiple objects which exist in local memory. ML1.3 Input data should be clearly partitioned between training and test data (for example, through having each passed as a distinct list item), or should enable an additional means of categorically distinguishing training from test data (such as via an additional parameter which provides explicit labels). Where applicable, distinction of validation and any other data should also accord with this standard. The third of these three standards for data input applies to ML software for which data are expected to be input as references to multiple external objects, generally expected to be read from either local or remote connections. ML1.4 Training and test data sets, along with other necessary components such as validation data sets, should be stored in their own distinctly labelled sub-directories (for distinct files), or according to an explicit and distinct labelling scheme (for example, for database connections). Labelling should in all cases adhere to ML1.1, above. The following standard applies to all ML software regardless of the applicability or otherwise of the preceding three standards. ML1.5 ML software should implement a single function which summarises the contents of test and training (and other) data sets, minimally including counts of numbers of cases, records, or files, and potentially extending to tables or summaries of file or data types, sizes, and other information (such as unique hashes for each component). 5.8.1.1 Missing Values Missing data are handled differently by different ML routines, and it is also difficult to suggest generally applicable standards for pre-processing missing values in ML software. The following standards attempt to cover a practical range of typical approaches and applications. ML1.6 ML software which does not admit missing values, and which expects no missing values, should implement explicit pre-processing routines to identify whether data has any missing values, and should generally error appropriately and informatively when passed data with missing values. In addition, ML software which does not admit missing values should: ML1.6a Explain why missing values are not admitted. ML1.6b Provide explicit examples (in function documentation, vignettes, or both) for how missing values may be imputed, rather than simply discarded. ML1.7 ML software which admits missing values should clearly document how such values are processed. ML1.7a Where missing values are imputed, software should offer multiple user-defined ways to impute missing data. ML1.7b Where missing values are imputed, the precise imputation steps should also be explicitly documented, either in tests (see ML7.2 below), function documentation, or vignettes. ML1.8 ML software should enable equal treatment of missing values for both training and test data, with optional user ability to control application to either one or both. 5.8.2 Pre-processing As reflected in the workflow envisioned at the outset, ML software operates somewhat differently to statistical software in many other categories. In particular, ML software often requires explicit specification of a workflow, including specification of input data (as per the standards of the preceding sub-section), and of both transformations and statistical models to be applied to those data. This section of standards refers exclusively to the transformation of input data as a pre-processing step prior to any specification of, or submission to, actual models. ML2.0 A dedicated function should enable pre-processing steps to be defined and parametrized. ML2.0a That function should return an object which can be directly submitted to a specified model (see section 3, below). ML2.0b Absent explicit justification otherwise, that return object should have a defined class minimally intended to implement a default print method which summarizes the input data set (as per ML1.5 above) and associated transformations (see the following standard). Standards for most other categories of statistical software suggest that pre-processing routines should ensure that input data sets are commensurate, for example, through having equal numbers of cases or rows. In contrast, ML software is commonly intended to accept input data which can not be guaranteed to be dimensionally commensurate, such as software intended to process rectangular image files which may be of different sizes. ML2.1 ML software which uses broadcasting to reconcile dimensionally incommensurate input data should offer an ability to at least optionally record transformations applied to each input file. Beyond broadcasting and dimensional transformations, the following standards apply to the pre-processing stages of ML software. ML2.2 ML software which requires or relies upon numeric transformations of input data (such as change in mean values or variances) should allow optimal explicit specification of target values, rather than restricting transformations to default generic values only (such as transformations to z-scores). ML2.2a Where the parameters have default values, reasons for those particular defaults should be explicitly described. ML2.2b Any extended documentation (such as vignettes) which demonstrates the use of explicit values for numeric transformations should explicitly describe why particular values are used. For all transformations applied to input data, whether of dimension (ML2.1) or scale (ML2.2), ML2.3 The values associated with all transformations should be recorded in the object returned by the function described in the preceding standard (ML2.0). ML2.4 Default values of all transformations should be explicitly documented, both in documentation of parameters where appropriate (such as for numeric transformations), and in extended documentation such as vignettes. ML2.5 ML software should provide options to bypass or otherwise switch off all default transformations. ML2.6 Where transformations are implemented via distinct functions, these should be exported to a package’s namespace so they can be applied in other contexts. ML2.7 Where possible, documentation should be provided for how transformations may be reversed. For example, documentation may demonstrate how the values retained via ML2.3, above, can be used along with transformations either exported via ML2.6 or otherwise exemplified in demonstration code to independently transform data, and then to reverse those transformations. 5.8.3 Model and Algorithm Specification A “model” in the context of ML software is understood to be a means of specifying a mapping between input and output data, generally applied to training and validation data. Model specification is the step of specifying how such a mapping is to be constructed. The specification of what the values of such a model actually are occurs through training the model, and is described in the following sub-section. These standards also refer to control parameters which specify how models are trained. These parameters commonly include values specifying numbers of iterations, training rates, and parameters controlling algorithmic processes such as re-sampling or cross-validation. ML3.0 Model specification should be implemented as a distinct stage subsequent to specification of pre-processing routines (see Section 2, above) and prior to actual model fitting or training (see Section 4, below). In particular, ML3.0a A dedicated function should enable models to be specified without actually fitting or training them, or if this (ML3) and the following (ML4) stages are controlled by a single function, that function should have a parameter enabling models to be specified yet not fitted (for example, nofit = FALSE). ML3.0b That function should accept as input the objects produced by the previous Input Data Specification stage, and defined according to ML2.0, above. ML3.0c The function described above (ML3.0a) should return an object which can be directly trained as described in the following sub-section (ML4). ML3.0d That return object should have a defined class minimally intended to implement a default print method which summarises the model specification, including values of all relevant parameters. ML3.1 ML software should allow the use of both untrained models, specified through model parameters only, as well as pre-trained models. Use of the latter commonly entails an ability to submit a previously-trained model object to the function defined according to ML3.0a, above. ML3.2 ML software should enable different models to be applied to the object specifying data inputs and transformations (see sub-sections 1–2, above) without needing to re-define those preceding steps. A function fulfilling ML3.0–3.2 might, for example, permit the following arguments: data: Input data specification constructed according to ML1 model: An optional previously-trained model control: A list of parameters controlling how the model algorithm is to be applied during the subsequent training phase (ML4). A function with the arguments defined above would fulfil the preceding three standards, because the data stage would represent the output of ML1, while the model stage would allow for different pre-trained models to be submitted using the same data and associated specifications (ML3.1). The provision of a separate .data argument would fulfil ML3.2 by allowing one or both model or control parameters to be re-defined while submitting the same data object. ML3.3 Where ML software implements its own distinct classes of model objects, the properties and behaviours of those specific classes of objects should be explicitly compared with objects produced by other ML software. In particular, where possible, ML software should provide extended documentation (as vignettes or equivalent) comparing model objects with those from other ML software, noting both unique abilities and restrictions of any implemented classes. ML3.4 Where training rates are used, ML software should provide explicit documentation both in all functions which use training rates, and in extended form such as vignettes, of the importance of, and/or sensitivity to, different values of training rates. In particular, ML3.4a Unless explicitly justified otherwise, ML software should offer abilities to automatically determine appropriate or optimal training rates, either as distinct pre-processing stages, or as implicit stages of model training. ML3.4b ML software which provides default values for training rates should clearly document anticipated restrictions of validity of those default values; for example through clear suggestions that user-determined and -specified values may generally be necessary or preferable. 5.8.3.1 Control Parameters Control parameters are considered here to specify how a model is to be applied to a set of training data. These are generally distinct from parameters specifying the actual model (such as model architecture). While we recommend that control parameters be submitted as items of a single named list, this is neither a firm expectation nor an explicit part of the current standards. ML3.5 Parameters controlling optimization algorithms should minimally include: ML3.5a Specification of the type of algorithm used to explore the search space (commonly, for example, some kind of gradient descent algorithm) ML3.5b The kind of loss function used to assess distance between model estimates and desired output. ML3.6 Unless explicitly justified otherwise (for example because ML software under consideration is an implementation of one specific algorithm), ML software should: ML3.6a Implement or otherwise permit usage of multiple ways of exploring search space ML3.6b Implement or otherwise permit usage of multiple loss functions. 5.8.3.2 CPU and GPU processing ML software often involves manipulation of large numbers of rectangular arrays for which graphics processing units (GPUs) are often more efficient than central processing units (CPUs). ML software thus commonly offers options to train models using either CPUs or GPUs. While these standards do not currently suggest any particular design choice in this regard, we do note the following: ML3.7 For ML software in which algorithms are coded in C++, user-controlled use of either CPUs or GPUs (on NVIDIA processors at least) should be implemented through direct use of libcudacxx. This library can be “switched on” through activating a single C++ header file to switch from CPU to GPU. 5.8.4 Model Training Model training is the stage of the ML workflow envisioned here in which the actual computation is performed by applying a model specified according to ML3 to data specified according to ML1 and ML2. ML4.0 ML software should generally implement a unified single-function interface to model training, able to receive as input a model specified according to all preceding standards. In particular, models with categorically different specifications, such as different model architectures or optimization algorithms, should be able to be submitted to the same model training function. ML4.1 ML software should at least optionally retain explicit information on paths taken as an optimizer advances towards minimal loss. Such information should minimally include: ML4.1a Specification of all model-internal parameters, or equivalent hashed representation. ML4.1b The value of the loss function at each point ML4.1c Information used to advance to next point, for example quantification of local gradient. ML4.2 The subsequent extraction of information retained according to the preceding standard should be explicitly documented, including through example code. 5.8.4.1 Batch Processing The following standards apply to ML software which implements batch processing, commonly to train models on data sets too large to be loaded in their entirety into memory. ML4.3 All parameters controlling batch processing and associated terminology should be explicitly documented, and it should not, for example, be presumed that users will understand the definition of “epoch” as implemented in any particular ML software. According to that standard, it would for example be inappropriate to have a parameter, nepochs, described as “Number of epochs used in model training”. Rather, the definition and particular implementation of “epoch” must be explicitly defined. ML4.4 Explicit guidance should be provided on selection of appropriate values for parameter controlling batch processing, for example, on trade-offs between batch sizes and numbers of epochs (with both terms provided as Control Parameters in accordance with the preceding standard, ML3). ML4.5 ML software may optionally include a function to estimate likely time to train a specified model, through estimating initial timings from a small sample of the full batch. ML4.6 ML software should by default provide explicit information on the progress of batch jobs (even where those jobs may be implemented in parallel on GPUs). That information may be optionally suppressed through additional parameters. 5.8.4.2 Re-sampling As described at the outset, ML software does not always rely on pre-specified and categorical distinctions between training and test data. For example, models may be fit to what is effectively one single data set in which specified cases or rows are used as training data, and the remainder as test data. Re-sampling generally refers to the practice of re-defining categorical distinctions between training and test data. One training run accordingly connotes training a model on one particular set of training data and then applying that model to the specified set of test data. Re-sampling starts that process anew, through constructing an alternative categorical partition between test and training data. Even where test and training data are distinguished by more than a simple data-internal category (such as a labelling column), for example, by being stored in distinctly-named sub-directories, re-sampling may be implemented by effectively shuffling data between training and test sub-directories. ML4.7 ML software should provide an ability to combine results from multiple re-sampling iterations using a single parameter specifying numbers of iterations. ML4.8 Absent any additional specification, re-sampling algorithms should by default partition data according to proportions of original test and training data. ML4.8a Re-sampling routines of ML software should nevertheless offer an ability to explicitly control or override such default proportions of test and training data. 5.8.5 Model Output and Performance Model output is considered here as a stage distinct from model performance. Model output refers to the end result of model training (ML4), while model performance involves the assessment of a trained model against a test data set. The present section first describes standards for model output, which are standards guiding the form of a model trained according to the preceding standards (ML4). Model Performance is then considered as a separate stage. 5.8.5.1 Model Output ML5.0 The result of applying the training processes described above should be contained within a single model object returned by the function defined according to ML4.0, above. Even where the output reflects application to a test data set, the resultant object need not include any information on model performance (see ML5.3–ML5.4, below). ML5.0a That object should either have its own class, or extend some previously-defined class. ML5.0b That class should have a defined print method which summarises important aspects of the model object, including but not limited to summaries of input data and algorithmic control parameters. ML5.1 As for the untrained model objects produced according to the above standards, and in particular as a direct extension of ML3.3, the properties and behaviours of trained models produced by ML software should be explicitly compared with equivalent objects produced by other ML software. (Such comparison will generally be done in terms of comparing model performance, as described in the following standard ML5.3–ML5.4). ML5.2 The structure and functionality of objects representing trained ML models should be thoroughly documented. In particular, ML5.2a Either all functionality extending from the class of model object should be explicitly documented, or a method for listing or otherwise accessing all associated functionality explicitly documented and demonstrated in example code. ML5.2b Documentation should include examples of how to save and re-load trained model objects for their re-use in accordance with ML3.1, above. ML5.2c Where general functions for saving or serializing objects, such as saveRDS are not appropriate for storing local copies of trained models, an explicit function should be provided for that purpose, and should be demonstrated with example code. The R6 system for representing classes in R is an example of a system with explicit functionality, all components of which are accessible by a simple ls() call. Adherence to ML5.2a would nevertheless require explicit description of the ability of ls() to supply a list of all functions associated with an object. The mlr package, for example, uses R6 classes, yet neither explicitly describes the use of ls() to list all associated functions, nor explicitly lists those functions. 5.8.5.2 Model Performance Model performance refers to the quantitative assessment of a trained model when applied to a set of test data. ML5.3 Assessment of model performance should be implemented as one or more functions distinct from model training. ML5.4 Model performance should be able to be assessed according to a variety of metrics. ML5.4a All model performance metrics represented by functions internal to a package must be clearly and distinctly documented. ML5.4b It should be possible to submit custom metrics to a model assessment function, and the ability to do so should be clearly documented including through example code. The remaining sub-sections specify general standards beyond the preceding workflow-specific ones. 5.8.6 Documentation ML6.0 Descriptions of ML software should make explicit reference to a workflow which separates training and testing stages, and which clearly indicates a need for distinct training and test data sets. The following standard applies to packages which are intended or other able to only encompass a restricted subset of the six primary workflow steps enumerated at the outset. Envisioned here are packages explicitly intended to aid one particular aspect of the general workflow envisioned here, such as implementations of ML optimization functions, or specific loss measures. ML6.1 ML software intentionally designed to address only a restricted subset of the workflow described here should clearly document how it can be embedded within a typical full ML workflow in the sense considered here. ML6.1 Such demonstrations should include and contrast embedding within a full workflow using at least two other packages to implement that workflow. 5.8.7 Testing 5.8.7.1 Input Data ML7.0 Test should explicitly confirm partial and case-insensitive matching of “test”, “train”, and, where applicable, “validation” data. ML7.1 Tests should demonstrate effects of different numeric scaling of input data (see ML2.2). ML7.2 For software which imputes missing data, tests should compare internal imputation with explicit code which directly implements imputation steps (even where such imputation is a single-step implemented via some external package). These tests serve as an explicit reference for how imputation is performed. 5.8.7.2 Model Classes The following standard applies to models in both untrained and trained forms, considered to be the respective outputs of the preceding standards ML3 and ML4. ML7.3 Where model objects are implemented as distinct classes, tests should explicitly compare the functionality of these classes with functionality of equivalent classes for ML model objects from other packages. ML7.3a These tests should explicitly identify restrictions on the functionality of model objects in comparison with those of other packages. ML7.3b These tests should explicitly identify functional advantages and unique abilities of the model objects in comparison with those of other packages. 5.8.7.3 Model Training ML7.4 ML software should explicit document the effects of different training rates, and in particular should demonstrate divergence from optima with inappropriate training rates. ML7.5 ML software which implements routines to determine optimal training rates (see ML3.4, above) should implement tests to confirm the optimality of resultant values. ML7.6 ML software which implement independent training “epochs” should demonstrate in tests the effects of lesser versus greater numbers of epochs. ML7.7 ML software should explicitly test different optimization algorithms, even where software is intended to implement one specific algorithm. ML7.8 ML software should explicitly test different loss functions, even where software is intended to implement one specific measure of loss. ML7.9 Tests should explicitly compare all possible combinations in categorical differences in model architecture, such as different model architectures with same optimization algorithms, same model architectures with different optimization algorithms, and differences in both. ML7.9a Such combinations will generally be formed from multiple categorical factors, for which explicit use of functions such as expand.grid() is recommended. The following example illustrates: architechture &lt;- c (&quot;archA&quot;, &quot;archB&quot;) optimizers &lt;- c (&quot;optA&quot;, &quot;optB&quot;, &quot;optC&quot;) cost_fns &lt;- c (&quot;costA&quot;, &quot;costB&quot;, &quot;costC&quot;) expand.grid (architechture, optimizers, cost_fns) ## Var1 Var2 Var3 ## 1 archA optA costA ## 2 archB optA costA ## 3 archA optB costA ## 4 archB optB costA ## 5 archA optC costA ## 6 archB optC costA ## 7 archA optA costB ## 8 archB optA costB ## 9 archA optB costB ## 10 archB optB costB ## 11 archA optC costB ## 12 archB optC costB ## 13 archA optA costC ## 14 archB optA costC ## 15 archA optB costC ## 16 archB optB costC ## 17 archA optC costC ## 18 archB optC costC All possible combinations of these categorical parameters could then be tested by iterating over the rows of that output. ML7.10 The successful extraction of information on paths taken by optimizers (see ML5.1, above), should be tested, including testing the general properties, but not necessarily actual values of, such data. 5.8.7.4 Model Performance ML7.11 All performance metrics available for a given class of trained model should be thoroughly tested and compared. ML7.11a Tests which compare metrics should do so over a range of inputs (generally implying differently trained models) to demonstrate relative advantages and disadvantages of different metrics. "],["assessment.html", "Chapter 6 Assessment 6.1 General Software Metrics 6.2 Metrics specific to statistical software 6.3 Diagnostics and Reporting 6.4 Proposals and Aims", " Chapter 6 Assessment The preceding Standards section primarily served (in its current form) to illustrate one possible approach to standards used to prospectively guide software during development. In contrast, the present section lists aspects of software, both general and specific to statistical software, which may usefully assessed in order to give insight into structure, function, and other aspects. The following list of assessments is ultimately intended to inform our development of a stand-alone tool, and potentially also a publicly available service, which can be used by any developers to assess their own software. We accordingly intend the following standards to be used both for retrospective purposes of peer review, and for prospective use in developing software both in general, and in preparation for peer-review. It is important to consider the applicability of each metric to different categories of statistical software, as well as the extent to which the following aspects may be more or less applicable or relevant at different phases of a software life cycle, or how expected values for, or results of applying, metrics may vary throughout a software life cycle. 6.1 General Software Metrics The following is an incomplete list of the kinds of metrics commonly used to evaluate software in general, and which might provide useful for assessing statistical software in the present project. Code structure Cyclomatic complexity Codebase size Function size / number Numbers of external calls within functions Numbers and proportions of Exported / non exported functions Code consistency Dynamic metrics derived from function call networks or similar Network-based metrics both for entire packages, for individual functions, and derived from analyses of test coverage Functional overlap with other packages Documentation metrics: Numbers of documentation lines per function Proportion of documentation to code lines Presence of examples Vignettes Data documentation metrics Intended and/or permitted kinds of input data Nature of output data Description of data used in tests Meta structure Dependencies Reverse dependencies Meta metrics License (type, availability, compatibility) Version control? Availability of website Availability of source code (beyond CRAN or similar) Community: Software downloads and usage statistics Numbers of active contributors Numbers or rates of issues reported Maintenance: Rate/Numbers of releases Rate of response to reported issues Last commit Commit rate stars (for github, gitlab, or equivalent for other platforms) forks Extent of testing Code coverage Examples and their coverage Range of inputs tested Nature of testing Testing beyond R CMD check? Testing beyond concrete testing? 6.2 Metrics specific to statistical software Metrics specific to statistical software will depend on, and vary in applicability or relevance with, the system for categorizing statistical software expected to emerge from the initial phase of this project. Details of this sub-section will be largely deferred until we have a clearer view of what categories might best be considered, which we are hopeful will emerge following the first committee meeting, and in response to ensuing feedback. In the meantime, metrics can be anticipated by referring to the preceding examples for categories of statistical software (numerical standards, method validity, software scope, and reference standards). We anticipate having a number of such categories, along with a number of corresponding metrics for assessing software in regard to each category. As mentioned at the outset, software will generally be expected to fit within multiple categories, and specific metrics will need to be developed to ensure validity for software encompassing any potential combination of categories. 6.3 Diagnostics and Reporting While the preceding sub-sections considered what might be assessed in relation to statistical software, the project will also need to explicitly consider how any resultant assessment might best be presented and reported upon. Indeed, a key output of the project is expected to be a suite of tools which can be used both in this and other projects to construct, curate, and report upon a suite of peer-reviewed software. Moreover, we will aim to develop these tools partly to provide or enhance the automation of associated processes, aiming both to enhance adaptability and transferability, and to ensure the scalability of our own project. It is useful in this context to distinguish between collective tools useful for, of applicable to, collections of software, of individuals, or of processes pertaining to either (here, primarily peer review), and singular tools of direct applicability to individual pieces of software. We envision needing to address the (likely relative) importance of some of the following kinds of diagnostic and reporting tools which may be usefully developed. Collective Tools Qualitative tools useful in assessing or formalizing categories of software Quantitative tools to retrospectively assess such aspects as: Collective “quality” of software Community engagement Effectiveness (or other metrics) of review Singular Tools Quantitative tools that can be prospectively used to Improve or assure software quality Document aspects of software quality Aid modularity or transferability either of software, or of the tools themselves Tools to formalize structural aspects of software such as tests (for example, through implementing new frameworks or grammars) Extensions of extant packages such as lintr, covr, goodpractice Comparisons of package metrics to distributions for other packages or systems (such as the CRAN archive directories) Diagnostic and report aggregation, design, or automatic creation at any stage before, during, or after peer review. The one question of abiding importance is the extent to which any such tools, and/or the automation of processes which they may enable, might enhance any of the following aspects: Software development Peer review of software Wider communities of users or developers The adaptation of our system to other domains 6.4 Proposals and Aims We develop a system to automatically assess aspects of software detailed in the above list (and more).AIM: To assess software in as much detail as possible, while reducing the burden on developers of manual assessment. The development of this system extend from the riskmetric package developed by the PharmaR group.AIM: To efficiently re-use work, prevent duplication, and develop our system as quickly as possible. We provide this assessment both as a stand-alone R package, and a publicly available service which receives text links to publicly available repositories (and does not enable upload of binary software).AIM: To extend engagement with the present project beyond just those directly interested in submitting software for review, and so to enhance broader community engagement with both this project, and rOpenSci in general. Considerable effort be devoted to developing a coherent and adaptable system to summarise and report on software assessment.AIM: To devise a reporting system which imposes a low cognitive burden on both developers and reviewers needing to assess packages. We simultaneously develop a system for assessing historical development of all available metrics, through applying our assessment system to all packages in the CRAN and bioconductor archives, and we use comparisons with these historical patterns to inform and guide our reporting system.AIM: To ensure standards and assessments reflect enacted practices, rather than just theoretical assumptions, and to ensure that standards and assessments keep apace with ongoing developments of practices. "],["lifeycle.html", "Chapter 7 Software Review and Life Cycle Models [Seeking Feedback] 7.1 Other systems for software and peer review 7.2 Software Life Cycle Considerations", " Chapter 7 Software Review and Life Cycle Models [Seeking Feedback] Prior to describing the review system we intend to develop, we briefly digress to describe analogous review systems, aiming to emphasise aspects which may be useful to adopt within our system. This chapter concludes with succinct proposals of aspects of these prior systems which we intend to adopt within our system, and for which we are explicitly seeking feedback. There are notable differences between the systems described here, with contrasts between them often providing convenient reference points in considering many of the subsequent review phases we envision developing. The present chapter then concludes with brief consideration of what a model of the life cycle of open source software might look like. This is important in guiding the structure of the proposed review process. 7.1 Other systems for software and peer review 7.1.1 rOpenSci rOpenSci’s current software peer-review process, detailed in our developer guide, is based on a blend of practices from peer review of academic practices and code review in open-source projects. Review takes place via an issue thread in our “software-review” repository on GitHub. The review process is entirely open, with each issue thread used to manage the entire process, coordinated by rOpenSci’s editors. After initial screening for scope and minimal qualification by editors, two reviewers provide comments and feedback on software packages. After one or more rounds of revisions, packages reach a point of approval, at which point they are “accepted” by rOpenSci, symbolized both through a badge system, and (generally) through transferring the software from the authors’ private domain to the github.com/ropensci domain. 7.1.2 The Journal of Open Source Software The Journal of Open Source Software (JOSS) was based on rOpenSci and follows a similar approach, with greater automation and broader scope. The Journal of Statistical Software conducts a closed review of both manuscript and software, with fewer prescriptive standards. In reviewing packages for acceptance into its repository, BioConductor conducts an open review primarily aimed at maintaining minimum standards and inter-compatibility. 7.1.3 Academic Journal Reviews One ubiquitous model for processes of peer review is that of “standard” academic journals, for which we now highlight two relevant aspects. 7.1.3.1 Primary and Secondary Editors Academic journals commonly have a board of (primary) editors, and a field of (secondary) subject or specialist editors. The initial and terminal points of review processes are commonly handled by the primary editors, who delegate subject editors to both solicit appropriate reviewers, and to manage the review process. Upon completion, the primary editor generally signifies ultimate acceptance. Such a model would also likely be beneficial for the present project, in spite of potential difficulties we may face in attracting sufficient numbers of subject editors. Division of labour between primary and secondary editors would offer distinct advantages, foremost among which would be an ability to appoint a reasonably large number of “subject editors” (or equivalent), for whom such an official designation would be able to be used to boost their own careers. As a contrast, rOpenSci’s editorial processes are handled by a single cohort of eight editors, of whom four are staff members, while JOSS currently has six primary editors and 31 “topic” editors. The preceding consideration of categories suggests we may end up with around a dozen categories, and so potentially be able to offer around this number (or more) of honorary subject editor positions, along with a concomitant reduction in workload for each of these. Engaging such a range of subject editors would also lessen the burden on primary editors, perhaps enabling the system to be initially trialled with the two or three people primarily engaged in its current developmental phase. The engagement of a wider range of subject editors would also enlarge the network of people directly engaged with the project, as well as extending its sphere of influence to encompass the professional networks of all those involved. 7.1.3.2 Invited and Mentored Submissions Many journals enable editors to personally invite selected authors to submit manuscripts on some particular topic, often compiled within single “special issues”. While special issues may not be relevant here, the notion of invited submissions may prove particularly useful in fostering integration between software packages. One likely defining distinction between rOpenSci and RStudio may be the ability of the latter organisation to strategically plan the development of software that links pre-existing software into more coherent or thematically-aligned “suites” of software (the tidyverse likely being the prime example). In contrast, rOpenSci’s software profile is very largely dependent on the whims of largely independent developers, and the software they autonomously elect to submit. (rOpenSci staff may themselves also develop software, and so strive to create more focussed suites of related packages, but this is then by definition more an individual than community effort.) The ability to solicit software within particular categories, or fulfilling particular functionality, may greatly aid an ability for this project to develop a coherent and singularly identifiable suite of packages for use in statistical analyses. One potential ways by which submissions could be invited would be through all regular meetings of the editors and board having a fixed discussion point on potential categories in which submissions may be desired. Agreement on the importance or usefulness of particular categories or themes may be relatively rare, but having this as a fixed item would allow progressive contemplation ultimately leading to sporadic consensus. Following meetings during which such consensus emerges, a general call for themed submissions may be issued, and/or specific potential package authors may be individually approached. The solicitation of themed submissions may also involve editors, members of the board, or other community members, offering their services as mentors or advisors throughout processes of package development. Invited submissions would then also serve as an opportunity for dissemination of the knowledge and expertise built up and held by individuals prior to and throughout the life of this project. Extending on from that idea, it may be worthwhile examining a “mentorship” system, whereby people who might feel they lack the skills necessary to develop a package of suitable standards might apply via an alternative form of pre-submission enquiry (in this case something more like a “pre-development enquiry”) as to whether anybody might be willing to mentor the development of a particular package idea. Such a system would of course require individuals to be willing to volunteer their services as mentors, but would have potentially significant advantages in expanding the entire system well beyond the boundaries of the limited few who have sufficient confidence in their abilities to develop packages. Proposal We adopt a model of primary and secondary editors, through having the rOpenSci staff directly involved in the development of this project serve as primary editors, and we seek to find and nominate subject editors as soon as we have reached agreement on categories of statistical software. Members of the board may also offer their services in either as primary or secondary editorial capacity. Once the system has started, we implement a fixed discussion point of every meeting on potential themes for invited submissions, and sporadically issue open (and directed) invitations for submissions of category-specific software. We offer a separate stream of “pre-development enquiry” as a kind of “ideas lab” to which people may submit and discuss ideas, with the system explicitly designed to connect ideas to potential mentors who may guide development towards full packages. 7.1.4 The Debian System The development of software for the open-source Debian Operating System is guided by Debian Developers and Debian Maintainers. Expressed roughly, maintainers are individuals responsible for the maintenance of particular pieces of software, while developers engage with activities supporting the development of the operating system as a whole. The submission and review process for Debian is almost entirely automated, based on tools such as their own software checker, lintian. Debian differs fundamentally from the system proposed here in being centred around the trust and verification of people rather than software. Submission of software to Debian is largely automatic, and bug-free software may often progress automatically through various stages towards acceptance. Software may, however, only be submitted by official Debian Maintainers or Developers. People can only become developers or maintainers through being sponsored by existing members, and are then subject to review of the potential contribution they may be able to make to the broader Debian community. (Details can be seen in this chapter of the Debian handbook.) While the general process for software submission and acceptance in Debian may not be of direct relevance, their versioning policy may provide a useful mode. The ongoing development of both the Debian system and all associated packages proceeds in accordance with a versioned policy manual. All new packages must comply to the current standards at the time of submission, and are labelled with the latest version of the standards to which they comply, noting that, For a package to have an old Standards-Version value is not itself a bug … It just means that no-one has yet reviewed the package with changes to the standards in mind. Each new version of the standards is accompanied by a simple checklist of differences, explicitly indicating differences with and divergences from previous versions. As long as software continues to pass all tests, upgrading to current standards remains optional. Failing tests in response to any upgrading of standards serve as a trigger for review of software. The nominated standards version may only be updated once review has confirmed compliance with current standards. We propose to adapt some of these aspects of the Debian system in the present project, as described below. 7.1.5 Other Potential Models The Linux Core Infrastructure Initiative provides badges to projects meeting development best practices. Badges are graded (passing/silver/gold), and awarded by package authors self-certifying that they have implemented items on a checklist. 7.2 Software Life Cycle Considerations The importance of considering Software “life cycles” has long been recognized for closed-source proprietary software, yet life cycles have only been given scant consideration in contexts of open source software (exceptions include Stokes 2012; Lenhardt et al. 2014). A long history and tradition in both practice and published literature on software review (for example, Mili 2015; Ammann and Offutt 2017) generally concludes that software review is most effective when it is an ongoing process that is structurally embedded within a software life cycle, and when review occurs as frequently as possible. Such practices contrast strongly with the singular nature of review as currently implemented by rOpenSci. An effective system for peer review of statistical software is thus may lie somewhere between the current “one-off” practices of rOpenSci and the JOSS, and frequent, ongoing review typical of software development in active teams. An analysis of the effects of rOpenSci’s review process on a few metrics of software development activity revealed that software development tends to stagnate following review. This may be interpreted to reflect software having reached a sufficiently stable state requiring relatively little ongoing maintenance. However, we note that metrics of community engagement with software are generally positively related to the metrics of development activity considered there. Slowing of software development following review may also accordingly reflect or result in decreases in community engagement. Potential systems to enhance review of the kind current practiced by rOpenSci, and particularly to encourage and enable more ongoing review on smaller scales and shorter time frames—and ultimately to encourage the ongoing and active development of software following review—include pull-request reviews, and systems for providing inline code reviews (such as watson-ruby). In addition, ongoing “review” may be explicit in considering the role of user feedback, for instance, in defining and updating the scope of statistical routines (see “Standards for Statistical Software” below). References "],["process.html", "Chapter 8 The Review Process [SEEKING FEEDBACK] 8.1 Self-Evaluation of Software Prior to Submission 8.2 Pre-Submission Communication 8.3 Reviewers / Selection 8.4 Submission 8.5 Initial Screening 8.6 Review Process 8.7 Acceptance / Scoring / Badging 8.8 Post-acceptance Dissemination, Publication, etc. 8.9 Ongoing Maintenance 8.10 Structured Review beyond Acceptance", " Chapter 8 The Review Process [SEEKING FEEDBACK] This section attempts to briefly describe the entire workflow envisioned to emerge from this project, from tools to enable authors to self-assess packages prior to submission, to tools for identifying and assigning reviewers, to methods and tools for structured review interventions after software has been officially accepted. Although there are several potential general models we could adapt for our proposed system, we anticipate the general workflow being based on github, for which two of the most prominent current models are rOpenSci’s own submission process, and that of the Journal of Open Source Software (JOSS). Both of these systems treat submissions as issues within dedicated review repositories, an approach we intend to adopt. 8.1 Self-Evaluation of Software Prior to Submission A strong focus of this project will be the development of tools to assess software, both generally and for statistical software specifically. One important aim is to develop tools able to be used by software authors to assess their own software. Such self-assessment, along with associated standardised reporting of results, will ease pre-submission enquiries both on the part of submitting authors, and editors responsible for assessing such enquiries. Standardised reporting is considered in the submission phase, while the remainder of the present sub-section considers tools for self-assessment. Current rOpenSci practices expect authors to assess their software using our package standards, then editors perform automated assessment using goodpractice, a package which runs R CMD check as well as other tests including calculating test coverage, linting, and checking for some common coding anti-patterns. The PharmaR project’s riskmetric package performs similar functions as well as providing more development-based metrics and providing a more extensible framework. While authors commonly use the goodpractice assessment tool, demonstrated self-assessment is not currently required at submission. We anticipate developing a system for self-evaluation of software, both in generic form able to be widely applied to software regardless of category, as well as specific tools for statistical software. Many of these generic assessments have been listed at the end of the preceding Framework, while tools specific to statistical software have been considered in the preceding Scope chapter. Key Considerations The primary consideration here is actually one of the primary considerations of the entire project, which is what sort of tools might best be developed? It will be possible to develop extremely sophisticated tools, but at the expense of compromising progress in other important aspects of the project. Perhaps more than any other aspect of this project, answering this question will require maintaining a keen awareness of the compromises necessary to successfully deliver all desired project outcomes. Proposal Authors will be expected to run automated self-assessments prior to submission. We develop a tool for general assessment of software and reporting of analytics, with several modules extending to specific assessment of statistical software. We simultaneously develop a lightweight infrastructure to enable such assessment and reporting to be provided as an online service, so that authors can run assessment in the same environment as it will be run at submission. 8.2 Pre-Submission Communication Pre-Submission Communication is the stage currently practised by both rOpenSci and JOSS whereby authors can enquire as to whether a potential submission is likely to be considered within scope prior to full submission. Full submissions can be potentially onerous, and the pre-submission phase represents a considerable easing of the burden on authors through enabling them to ascertain the potential suitability of a submission prior to completing a full submission. For this reason, we intend to adopt and adapt this phase as part of the new peer-review system. rOpenSci has github issue templates both for pre-submissions and submissions, whereas the both pre-submission and submission enquiries to JOSS are initiated through an external (non-github) website which automatically opens an issue on github with initial details provided by the submitting author. These two systems have two major differences: rOpenSci’s pre-submission enquiries are entirely optional, whereas initial submissions to JOSS are by default pre-submission enquiries (unless originating elsewhere, such as from a completed rOpenSci review). Pre-submission enquiries to rOpenSci serve the singular purposes of determining suitability of a potential full submission, whereas those to JOSS serve the additional purpose of seeking and assigning reviewers. Having found both editors and reviewers, a simple bot command of @whedon start review suffices to automatically transform the pre-submission to a full submission (as a new issue). JOSS is also trialling the automatic generation and reporting of initial software metrics in response to pre-submission enquiries, as in this example. The generation is triggered by the command @whedon check repository, and currently generates a CLOC (Count Lines of Code) summary of lines devoted to various computer languages, and a contribution chart with commits, additions, and deletions from each contributor to the repository. The CLOC output is used to automatically add labels to the issue identifying the primary computer languages. Determining whether or not a potential submission lies within or beyond scope requires aligning software with the statistical categories described above. The processing of pre-submission enquiries is accordingly also expected to entail the categorisation of software. While it may be possible to automate some aspects of software categorisation, we do not currently envision such automated tools being developed during the initial stages of this project. Proposals All submissions be initiated as mandatory pre-submission enquiries which may be automatically transitioned to full submissions upon successful nomination of editors and reviewers, as for JOSS. This has the distinct advantage of separating the search for reviewers from the actual review process itself, leaving resultant review issues notably cleaner and more focussed. Mandatory pre-submission enquiries also add clarity through removing potential ambiguity in deciding between two distinct ways to commence submission. The process of pre-submission be partially automated in a similar manner to the current system of JOSS, with metrics extended and adapted to the unique needs of our own project. Only cursory metrics pertinent to the pre-submission stage will be generated, as exemplified by the JOSS system of using CLOC output to assign labels identifying primary computer languages. Submitting authors be requested to identify potential categories describing their software as part of a pre-submission enquiry, with final categorisation being determined through mutual agreement between editors and submitting authors. Automation procedures may perhaps be extended by some form of automated identification or suggestion of appropriate reviewers, with some aspects of the processes described in the following section potentially automatically triggered by a pre-submission enquiry. Questions Which software metrics might aid the pre-submission process? 8.3 Reviewers / Selection The solicitation of reviewers is one of the most difficult tasks facing any peer review system, including both rOpenSci and JOSS. rOpenSci has built up an extensive network of users, participants, and developers, many of whom are members of the organisation’s slack group. In contrast to traditional academic journals, submitting authors are not requested to recommend potential reviewers. JOSS explicitly states, if you have any suggestions for potential reviewers then please mention them here in this thread, and also points authors of pre-submission enquiries to a curated list of potential reviewers (as a google document), further requesting that authors suggest any potentially appropriate reviewers from that list. Being a google document, it is simply progressively extended line-by-line, currently amounting to 1,163 names, likely making it not particularly easy for authors to find appropriate reviewers. We will of course extend upon our existing pool of reviewers, and also intend to cultivate and extend a network of reviewers with expertise in statistical software throughout the duration of this project. We would also very much like to develop and utilise tools which may aid the process of finding and soliciting reviewers, with the remainder of this sub-section exploring a few options. 8.3.1 Database of Potential Reviewers As described, JOSS maintains a database of potential reviewers within a publicly accessible google document, while rOpenSci maintains theirs in an private airtable. The debian system also maintains a comprehensive database of developers, maintainers, and other stakeholders, publicly accessible for search and via password for restricted access. Private directories have the advantage of allowing for including notes such as review quality and timeliness that may not, but also thus need to be more aggressively managed under standards such as GPDR. Different database platforms also have different privacy advantages. 8.3.2 Automating the Identification of Potential Reviewers It may be worthwhile developing automatic tools to aid identifying appropriate reviewers. One possibility may be analyses of all openly-available code from potential reviewers, to somehow measure degree of similarity with a given submission. While this would be almost impossible to do between different computer languages, it may be possible within R code alone, through processing output from the utils::getParseData function to identify frequencies of usage of various function calls. Such an approach may have important advantages, notably in highlighting reviewers for reasons other than mere prominence within some form of public arena. Appropriate development of such a tool should ultimately enable and empower a more equitable system which is actively designed to avoid any tendency of submitting authors suggesting similar names of centrally prominent developers. 8.4 Submission Submission is envisioned to mirror rOpenSci’s current submission process to a certain degree, although we anticipate a more extensive and structured checklist (or equivalent) system, along with the development of automated tools triggered in response to submission. For example, the current rOpenSci system requires editors run diagnostics locally and to paste the goodpractice output after submission. Such a process is readily automated (as exemplified by JOSS’s current experimental system), and we expect to extend and refine both the automated checking described above, and to collate results within some kind of reporting system. The self-identification of appropriate categories may also trigger automated checking using software specific to various categories of statistical software, with associated output also being automatically inserted into an issue. For both JOSS and rOpenSci’s, current submissions occur via GitHub Issue template, which is primarily a checklist of broad or general attributes with which both software and associated online repositories are expected to comply. Submissions to JOSS have a more extensive and detailed template, which is filled out after initial submission form. We may explore submission via a other mechanisms, forms that automatically generate templates, or an R-based workflows similar to devtools::release(). Key Considerations Presuming the primary entry point to be via pre-submission enquiries as described above means that considerably more information will be available upon transitioning to actual submissions, and that the information will accordingly be able to be used in a more structured way that better lends itself to automation. The tools used to generate such structured information will be largely those considered in the first of the above points, as tools able to be used for self-evaluation of software. As mentioned above, the actual Submission phase is to be entered in to only following successful assignment of willing reviewers (notwithstanding potential alternative paths, exemplified by current path from rOpenSci review to direct JOSS submission). Proposal Progression from pre-submission to submission be automated as for JOSS. A checklist be automatically generated as part of the opening issue, yet more reflecting current rOpenSci practices of affirming compliant aspects of a submission, rather than JOSS practices of affirming ultimate reviewer judgements. 8.5 Initial Screening The development and provision of automated tools for initial software assessment will enable considerably more structured information to be provided in direct (automated) response to the opening of a submission issue that with the current rOpenSci system. The ready provision of such structured information will aid all of the preceding steps, and will also greatly ease the burden of initial screening of submissions. Software will already have been ascertained to be within scope, willing reviewers will already have been assigned, and an extensive report will have been automatically generated summarising a variety of aspects of software structure, function, and other aspects pertinent to review. The primary purpose of the initial screening step will accordingly be for editors to judge whether or not the totality of submitted data suffices for the review process to officially start. An additional purpose could be the assignment of due dates for submission of reviews. JOSS imposes a generic review period of two weeks, whereas rOpenSci provides opportunity to discuss appropriate due dates with reviewers. Proposal Initial screening involve the two tasks of editors agreeing on submission dates for reviews, and officially approving a submission. The agreement of submission dates be integrated within the official submission issue, rather than the pre-submission issue, so that explicit information on review dates remains within the review issue itself. Submission dates be negotiated around an initial suggested duration of one month. An automated command be implemented for the review process to be “officially” started, which will announce the agreed-upon dates and provide any extra information for reviewers. Note that JOSS currently implements @whedon start review to transition a pre-submission to a full submission. The above suggestions effectively translate to breaking this into the two commands of start submission to transition to a full submission and start review to commence the actual review process once approved by editors. 8.6 Review Process The review processes of rOpenSci and JOSS are qualitatively different, with JOSS submissions guided by extensive automation, and so being strongly determined by their checklist, whereas rOpenSci reviews are commenced only after authors complete the checklist (or otherwise explain any anomalies). Reviewers of submissions to rOpenSci are solicited privately, and privately informed both to read the Guide for Reviewers chapter in the Development, Maintenance, and Peer Review Guide, and that their review must be submitted with the Review Template. This template serves the same purpose as the automatically-generated JOSS template, but is to be pasted by authors themselves into their own comments in the review issue, whereas the JOSS checklist is to be filled out by reviewers editing the opening comment of the review issue. In short, the rOpenSci checklist is an official starting point, with reviews submitted at the end with the help of a template; the JOSS checklist is an official endpoint, empty at first and progressively completed by each reviewer as they progress through the review process. We envision a system primarily derived from rOpenSci’s current system, with reviews completed through the use of a template and pasted as comments at the bottom of a github issue. This approach will face one immediate difficulty in that templates will likely differ through software being described by different combinations of the categories described above. It may suffice to combine a generic “master” template with category-specific items to be appended according to the description of submitted software within our list of categories, although it is important to note that this may exclude review criteria reflecting unique combinations of categories (for example, a checklist item appropriate for the visualisation of results from ML algorithms). The preceding consideration exemplifies the extent to which processes developed and employed to review statistical software are likely to be strongly influenced by the kinds of automated tools we develop, both for automated and self assessment along with associated reporting systems, as well as potentially for more comprehensive assessments and reporting systems or standards not otherwise amenable to automation. In the current initial phase of this project prior to the concrete development of any of these kind of tools, the present considerations of the review process are accordingly and necessarily generic in nature. We anticipate this current sub-section becoming particularly more detailed as the project progress and as we develop project-specific tools for software assessment. 8.6.1 Review Templates As described above, the JOSS checklist is pre-generated with the opening of each review issue, whereas the rOpenSci template is to be completed and pasted in to the issue by reviewers. The two templates are nevertheless broadly similar, both including the following checklist items: The reviewer has no conflict of interests Documentation The software has: A clear statement of need Installation instructions Function documentation Examples Community guidelines for how to contribute Functionality The software should: Install as documented Meet its own functional claims Meet its own performance claims Have automated tests (considered by JOSS as part of “Documentation”) In addition, JOSS requires reviewers: To agree to abide by a reviewer code of conduct To confirm that the source code is in the nominated repository To confirm that the software has an appropriate license. To confirm that the submitting author has made major contributions, and that the provided list of authors seems appropriate and complete. rOpenSci insists in turn on the two additional aspects, that software Has a vignette demonstrating major functionality; and Conforms to the rOpenSci packaging guidelines Perhaps the most influential difference between the two systems is that the rOpenSci template concludes with the following lines: 8.6.2 Review Comments The section break and sub-section heading act in combination as a prompt for reviewers to add their own discursive comments, whereas the JOSS template has no such field. Accordingly, the majority1 of JOSS reviews merely consist of a completed checklist, whereas all rOpenSci reviews are extensively discursive, with reviewers frequently offering very extensive comments and analyses of submitted code. These differences may plausibly be interpreted to reflect general differences in the cultural practices of the two review systems, with rOpenSci having particularly nurtured the cultural practice of extensively discursive reviews, notably through suggesting that prior reviews ought be perused for good guidelines on review practices. We intend to continue to foster and encourage such cultural practices, while at the same time aiming to develop a system for more structured yet discursive input, in order both to provide more focussed software reviews, and to lessen the burden on reviewers. We anticipate commencing the development of such structure in subsequent iterations of the present document. 8.6.3 Category-Specific Aspects of Reviews We defer consideration of category-specific aspects of review until we have concluded a first round of consultation on the preceding categorical definitions. 8.6.4 Reviewer Recommendations Both rOpenSci and JOSS currently work with a binary recommendation scheme of rejection or acceptance. In both cases, rejection is primarily decided in response to a pre-preview (JOSS) or pre-submission enquiry (rOpenSci), and usually for the reason of being out-of-scope (in rOpenSci’s case because software does not fit within the defined categories; and in JOSS’s case because the software does not have a specific research focus). Having obtained approval to proceed from pre-review to full review, both systems generally work with package authors to strive for ultimate acceptance. Rejections during this phase generally only happen when authors stall or abandon ongoing or requested package development. As long as authors continue to be engaged, reviews very generally proceed until a submission is accepted. Proposal While a variety of potential outcomes of the review process are considered immediately below, reviewers will only be requested to ultimately check a single box indicating their approval for software to be accepted. An approved submission may then receive a variety of labels in response to binary acceptance, as described below. Proposal We adopt the current rOpenSci approach of having reviews based on a pre-defined template to be completed by reviewers and pasted as the latest comment in a review issue, rather than the JOSS model of having reviewers edit the initial, opening comment of a review issue. We adopt the current rOpenSci approach of having reviewers testify the time spent on their review. We either then: Do not provide any information on typical times devoted by other reviewers; or Provide summary information including estimates of variation and proviso that such information is only intended to avoid reviewers otherwise feeling obliged to devote unnecessarily long times to reviews. We adopt and adapt the general review templates currently used by rOpenSci and JOSS, extending both in order to provides as much structured discursive feedback as possible. We develop at least examples of category-specific template items to be added to the general review template. 8.7 Acceptance / Scoring / Badging Software being recommended for acceptance by reviews need not be reflected in a simple “accepted” label. Particularly in the early stages of our system for peer-reviewing statistical software, we may have some kind of checklist from which we require authors to ultimately comply with some recommended limited number of items, yet not all. It may then be worthwhile have a review outcome that flags this compliance level, and indicates that software will be expected to retain compliance as our system develops. Another example may be outcomes which consider the kinds of life cycle models considered above, in which context it may be useful to have an outcome that labels software as having passed initial or primary review, yet which will still be subject to subsequent review some agreed-upon time later. Such systems of re-assessment will nevertheless not necessarily be (equally) applicable to all submissions, and so such “progressive labelling” will likely only ever be optional, and applicable where appropriate. Proposal We implement a recommendation system which explicitly flags the version of our system’s standards with which reviewed software complies. 8.8 Post-acceptance Dissemination, Publication, etc. 8.9 Ongoing Maintenance 8.10 Structured Review beyond Acceptance That claim has not been substantiated.↩︎ "],["appendices.html", "A Appendices A.1 Notes on Scope and the Python Statistical Ecosystem A.2 Analysis of statistical software keywords A.3 Other Software Standards A.4 Bibliography", " A Appendices A.1 Notes on Scope and the Python Statistical Ecosystem Two factors may be usefully noted in this regard: The potential number of python packages for statistical analyses is likely to be relatively more restricted than relative numbers of R packages. Taking as indicative presentations at the previous three Joint Statistical Meetings (JSMs; 2018-2020), no python packages were referred to in any abstract, while 32 R packages were presented, along with two meta-platforms for R packages. Presentations at the Symposium of Data Science and Statistics (SDSS) for 2018-19 similarly including numerous presentations of R packages, along with presentation of three python packages. It may accordingly be expected that potential expansion to include python packages will demand relatively very little time or effort compared with that devoted to R packages as the primary software scope. In spite of the above, the community of python users is enormously greater, reflected in the currently 266,576 packages compared with 16,379 packages on CRAN, or over 16 times as many python packages. Similarly, 41.7% of all respondents to the 2019 stackoverflow developer survey nominated python as their most popular language, compared with only 5.8% who nominated R. The relative importance of python is powerfully reflected in temporal trends from the stackoverflow developer survey from the previous three years, with results shown in the following graphic. Python is not only more used and more loved than R, but both statistics for python have consistently grown at a faster rate over the past three years as have equivalent statistics for R. Both languages nevertheless have relative well-defined standards for software packaging, python via the Python Package Index (pypi), and R via CRAN. In contrast to CRAN, which runs its own checks on all packages on a daily basis, there are no automatic checks for pypi packages, and almost any form of package that minimally conforms to the standards may be submitted. This much lower effective barrier to entry likely partially contributes to the far greater numbers of pypi (266,576) than CRAN (16,379) packages. A.2 Analysis of statistical software keywords The JOSS conducts its own peer review process, and publishes textual descriptions of accepted software. Each piece of software then has its own web page on the journal’s site, on which the text is presented as a compiled .pdf-format document, along with links to the open review, as well as to the software repository. The published document must be included within the software repository in a file named paper.md, which enables automatic extraction and analysis of these text descriptions of software. Rather than attempt a comprehensive, and unavoidably subjective, categorization of software, these textual descriptions were used to identify key words or phrases (hereafter, “keywords”) which encapsulated the purpose, function, or other general descriptive elements of each piece of software. Each paper generally yielded multiple keywords. Extracting these from all papers judged to be potentially in scope allowed for the construction of a network of topics, in which the nodes were the key words and phrases, and the connections between any pair of nodes reflected the number of times those two keywords co-occurred across all papers. We extracted all papers accepted and published by JOSS (217 at the time of writing in early 2020), and manually determined which of these were broadly statistical, reducing the total to 92. We then read through the contents of each of these, and recorded as many keywords as possible for each paper. The resultant network is shown in the following interactive graphic, in which nodes are scaled by numbers of occurrences, and edges by numbers of co-occurrences. (Or click here for full-screen version with link to code.) Such a network visualization enables immediate identification of more and less central concepts including, in our case, several that we may not otherwise have conceived of as having been potentially in scope. We then used this network to define our set of key “in scope” concepts. This figure also reveals that many of these keywords are somewhat “lower level” than the kinds of concepts we might otherwise have used to define scoping categories. For example, keywords such as “likelihood” or “probability” are not likely to be useful in defining actual categories of statistical software, yet they turned out to lie at the centres of relatively well-defined groups of related keywords. We also examined the forms of both input and output data for each of the 92 pieces of software described in these JOSS papers, and constructed an additional graph directionally relating these different data formats. ## `summarise()` regrouping output by &#39;from&#39; (override with `.groups` argument) A.3 Other Software Standards The following list represents the standards developed by the Software Sustainability Institute: Usability 1.1 Understandability High level description of what/who the software is for High level description of what the software does High level description of how the software works Design rationale - why the software does things the way it does Architectural overview with diagrams Descriptions of intended use cases Case studies of use 1.2 Documentation Provides a high level overview of the software Partitioned into sections for users, user-developers, and developers (depending on the software) Lists resources for further information Is task-oriented Consists of clear, step-by-step instructions Gives examples of what the user can see at each step For problems and error messages, the symptoms and step-by-step solutions are provided Does not use terms like “intuitive”, “user-friendly”, “easy to use”, “simple”, or “obviously” (other than in quotes from satisfied users). States command names, syntax, parameters, error messages exactly as they appear or should be typed. Uses \\({\\tt teletype-style fonts}\\) for command line inputs and outputs, source code fragments, function names, class names, etc. English language descriptions of commands or errors are provided. Plain text files (e.g. READMEs) use indentation and underlining to structure the text. Plain text files do not use TAB characters to indent the text. Documentation is complete (includes configuration requirements or properties). Is held under version control alongside the code Is on the project web site Documentation on web site makes it clear what version of software the documentation applies to. 1.3 Buildability Straightforward to meet build pre-requisites Straightforward to build the software Web site has build instructions Source distributions have build instructions Web site lists all third-party dependencies that are not bundled Source distribution lists all third-party dependencies that are not bundled All mandatory third-party dependencies are currently available All optional third-party dependencies are currently available 1.4 Installability Web site has installation instructions Binary distributions have installation instructions Web site lists all third-party dependencies that are not bundled 1.5 Learnability A getting started guide is provided with a basic example Instructions are provided for many basic use cases Reference guides are provided for all options Documentation is provided for user-developers and developers 1.5 Performance Sustainability &amp; Maintainability 2.1 Identity Identity of project is clear and unique both within domain of application and generally. Project/software has own domain name Project/software has distinct name within application area (appears within first page of search results when entered with domain keywords). Project/software has distinct name regardless of application area Project/software does not throw up embarrassing “Did you mean …” suggestions on search engines. Project/software name does not violate a trademark Project/software name is trademarked 2.2 Copyright Web site states copyright Web sites states developers and funders If multiple web sites, then all state exactly same copyright, license, authorship Each source code file has copyright statement If supported by language, each source file has copyright statement embedded within a constant - Each source code file has a license header 2.3 Licencing Appropriate licence Web site states licence Software has a licence Software has an open source licence Software has an Open Software Institute recognised licence 2.4 Governance Management is transparent Project has a defined governance policy Governance policy is publicly available 2.5 Community To what extent does an active user community exist? Web site has statement of numbers of users/developers/members Web site has quotes from satisfied users Web site lists most important partners or collaborators Web site has list of project publications Web site lists third-party publications that use the software Web site lists software that uses/bundles this software Users are required to cite software if publishing results derived from its use Users exists who are not members of the project Developers exists who are not members of the project 2.6 Accessibility To what extent is software accessible? Binary distributions are available Binary distributions are available without need for registration or authorisation Source distributions are available Source distributions are available without need for registration or authorisation Access to source code repository is available (whether for free, payment, registration) Anonymous read-only access to source code repository Ability to browse source code repository online Repository hosted in sustainable third-party site which will live beyond lifetime of any current funding Downloads page shows evidence of regular releases 2.7 Testability Straightforward to test software to verify modifications Project has unit tests Project has integration tests Project has scripts for testing non-automated scenarios (e.g. GUIs) Project recommends tools to check conformance to coding standards Project has automated tests to check conformance to coding standards Project recommends tools to check test coverage Project has automated tests to check test coverage A minimum test coverage level has been defined There is an automated test for this minimum level Tests are automatically run nightly Continuous integration is supported Test results are visible to all developers/members Test results are visible publicly Project specifies how to set up external resources (FTP servers, databases, etc.) for tests Tests create their own files, database tables, etc. 2.8 Portability To what extent can software be used on other platforms? (Checkboxes for various platforms.) 2.9 Supportability To what extent will software be supported currently and in the future? Web site has page describing how to get support User doc has page describing how to get support Software describes how to get support (in README) Project has an e-mail address Project e-mail address has domain name E-mails are read by more than one person E-mails are archived E-mails archives are publicly readable E-mail archives are searchable Project has a ticketing system Ticketing system is publicly available Ticketing system is searchable Web site has a site map or index Web site has a search facility Project resources are hosted externally in a sustainable third-part repository which will live beyond lifetime of current project E-mail archives or ticketing system shows that queries are resounded to (not necessarily fixed) within a week. If there is a blog, it is regularly used E-mail lists of forums, if present, have regular posts 2.10 Analysability Source code is structured into modules or packages Source code structure relates clearly to the architecture or design. Source code repository is in a version control system Structure of source code repository and how this maps to software’s components is documented Source releases are snapshots of the repository Source code is commented Source code comments are written in a document generation mark-up language Source code is laid out and indented well Source code uses sensible class, package, and variable names There are no old or obsolete source code files that should be handled by version control There is no commented out code There are not TODOs in the code Auto-generated source code is in separate directories from other source code Regeneration of auto-generated source code is documented Coding standards are recommended by the project Coding standards are required to be observed Project-specific coding standards are consistent with community standards 2.11 Changeability Project has a defined contributions policy Contributions policy is publicly available Contributors retain copyright/IP of their contributions Users, developer members, and developers who are not members can contribute Project has a defined and stable deprecation policy Stability/deprecation policy is publicly available Releases document deprecated components within release Releases document removed or changed components within release 2.12 Evolvability Web site describes project roadmap, plans, or milestones Web site describes how project is funded or sustained Web site describes end date of current funding lines 2.13 Interoperability Uses open standards Uses mature, ratified, non-draft standards Provides tests demonstrating compliance with standards In contrast to these qualitative aspects, Mili (2015) identifies the following attributes of Software Quality: Functional Attributes 1.1 Correctness 1.2 Robustness Useability Attributes 2.1 Ease of Use 2.2 Ease of Learning 2.3 Customizability 2.4 Calibrability 2.5 Interoperability Structural Attributes 3.1 Design Integrity 3.2 Modularity (including “cohesion” and “coupling”) 3.3 Testability 3.4 Adaptability Other aspects derived from other forms of standards include: Must use https Must be in English Unique version numbering Semantic versioning Release notes Static code analysis (with links to lots of language-specific tools) Dynamic code analysis (with links to lots of language-specific tools) Use of memory check tools, or memory-safe languages Functions that are type stable unless explicitly indicated and explained otherwise. (See also the section on type stability in the tidyverse design guide.) A.4 Bibliography "]]
